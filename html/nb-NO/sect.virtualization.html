<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html
    xmlns="http://www.w3.org/1999/xhtml"><head><meta
        http-equiv="Content-Type"
        content="text/html; charset=UTF-8" /><title
        xmlns:d="http://docbook.org/ns/docbook">12.2. virtualisering</title><link
        rel="stylesheet"
        type="text/css"
        href="Common_Content/css/default.css" /><link
        rel="stylesheet"
        media="print"
        href="Common_Content/css/print.css"
        type="text/css" /><meta
        xmlns:d="http://docbook.org/ns/docbook"
        name="generator"
        content="publican v4.3.2" /><meta
        xmlns:d="http://docbook.org/ns/docbook"
        name="package"
        content="Debian-debian-handbook-9-nb-NO-1.0-1" /><meta
        name="keywords"
        content="RAID, LVM, FAI, Forhåndsutfylling (preseed.cfg), Overvåking, virtualisering, Xen, LXC" /><link
        rel="home"
        href="index.html"
        title="Håndbok for Debian-administratoren" /><link
        rel="up"
        href="advanced-administration.html"
        title="Kapittel 12. Avansert administrasjon" /><link
        rel="prev"
        href="advanced-administration.html"
        title="Kapittel 12. Avansert administrasjon" /><link
        rel="next"
        href="sect.automated-installation.html"
        title="12.3. Automatisert installasjon" /><meta
        name="viewport"
        content="width=device-width, initial-scale=1" /><meta
        name="flattr:id"
        content="4pz9jq" /><link
        rel="canonical"
        href="http://l.github.io/debian-handbook/html/nb-NO/sect.virtualization.html" /></head><body><noscript><iframe
          src="//www.googletagmanager.com/ns.html?id=GTM-5H35QX"
          height="0"
          width="0"
          style="display:none;visibility:hidden"></iframe></noscript><script
        type="text/javascript">(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&amp;l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-5H35QX');</script><div
        id="banner"><a
          href="../../"><span
            class="text">Download the ebook</span></a></div><p
        id="title"><a
          class="left"
          href="http://www.debian.org"><img
            alt="Product Site"
            src="Common_Content/images//image_left.png" /></a><a
          class="right"
          href="index.html"><img
            alt="Documentation Site"
            src="Common_Content/images//image_right.png" /></a></p><ul
        class="docnav top"><li
          class="previous"><a
            accesskey="p"
            href="advanced-administration.html"><strong>Forrige</strong></a></li><li
          class="home">Håndbok for Debian-administratoren</li><li
          class="next"><a
            accesskey="n"
            href="sect.automated-installation.html"><strong>Neste</strong></a></li></ul><div
        class="section"><div
          class="titlepage"><div><div><h2
                class="title"><a
                  id="sect.virtualization"></a>12.2. virtualisering</h2></div></div></div><a
          id="id-1.15.5.2"
          class="indexterm"></a><div
          class="para">
			Virtualisering er et av de viktigste fremskritt i de seneste årenes datautvikling. Begrepet omfatter ulike abstraksjoner og teknikker som simulerer virtuelle datamaskiner med varierende grad av uavhengighet på selve maskinvaren. En fysisk tjener kan så være vert for flere systemer som arbeider samtidig, og i isolasjon. Bruksområdene er mange, og utledes ofte fra denne isolasjon: for eksempel testmiljøer med varierende konfigurasjoner, eller separasjon av vertsbaserte tjenester mellom ulike virtuelle maskiner for sikkerheten.
		</div><div
          class="para">
			Det er flere virtualiseringsløsninger, hver med sine egne fordeler og ulemper. Denne boken vil fokusere på Xen, LXC, og KVM, mens andre viktige implementeringer omfatter de følgende:
		</div><a
          id="id-1.15.5.5"
          class="indexterm"></a><a
          id="id-1.15.5.6"
          class="indexterm"></a><a
          id="id-1.15.5.7"
          class="indexterm"></a><a
          id="id-1.15.5.8"
          class="indexterm"></a><a
          id="id-1.15.5.9"
          class="indexterm"></a><a
          id="id-1.15.5.10"
          class="indexterm"></a><div
          xmlns:d="http://docbook.org/ns/docbook"
          class="itemizedlist"><ul><li
              class="listitem"><div
                class="para">
					QEMU er en programvare-emulator for en fullverdig datamaskin. Prestasjonen er langt fra den hastigheten man kunne oppnå ved å kjøre den opprinnelige, men den tillater å kjøre umodifiserte eller eksperimentelle operativsystemer på den emulerte maskinvaren. Den tillater også å emulere en annen maskinvarearkitektur: For eksempel, kan et <span
                  class="emphasis"><em>AMD64</em></span>-system emulere en <span
                  class="emphasis"><em>arm</em></span>-datamaskin. QEMU er fri programvare. <div
                  class="url">→ <a
                    href="http://www.qemu.org/">http://www.qemu.org/</a></div>
				</div></li><li
              class="listitem"><div
                class="para">
					Bochs er en annen gratis virtuell maskin, men den emulerer bare x86-arkitekturene (i386 eller AMD64).
				</div></li><li
              class="listitem"><div
                class="para">
					VMWare er en proprietær virtuell maskin; og som en av de eldste der ute, er den også en av de mest kjente. Den fungerer på prinsipper som ligner på QEMU. VMWare foreslår avanserte funksjoner som å ta øyeblikksbilder av en kjørende virtuell maskin. <div
                  class="url">→ <a
                    href="http://www.vmware.com/">http://www.vmware.com/</a></div>
				</div></li><li
              class="listitem"><div
                class="para">
					VirtualBox er en virtuell maskin som stort sett består av fri programvare (noen ekstra komponenter er tilgjengelige under en proprietær lisens). Dessverre er det i Debians «contrib»-del fordi den inneholder noen ferdigbygde filer som ikke kan bygges opp igjen uten en proprietær kompilator. Mens VirtualBox er yngre enn VMWare, og begrenset til i386 eller AMD64-arkitekturer, inneholder den fortsatt muligheten til å ta noen øyeblikksbilder og andre interessante funksjoner. <div
                  class="url">→ <a
                    href="http://www.virtualbox.org/">http://www.virtualbox.org/</a></div>
				</div></li></ul></div><div
          class="section"><div
            class="titlepage"><div><div><h3
                  class="title"><a
                    id="sect.xen"></a>12.2.1. Xen</h3></div></div></div><div
            class="para">
				Xen <a
              id="id-1.15.5.12.2.1"
              class="indexterm"></a> er en «paravirtualiserings»-løsning. Den introduserer et tynt abstraksjonslag, kalt en «hypervisor», mellom maskinvaren og de øvre systemer; dette fungerer som en dommer som kontrollerer tilgangen til maskinvaren fra de virtuelle maskinene, men den håndterer bare noen av instruksjonene, resten kjøres direkte av maskinvaren på vegne av systemene. Den største fordelen er at prestasjonen ikke blir dårligere, og systemer kjører med nær sin opprinnelige hastighet; ulempen er at operativsystemkjernene man ønsker å bruke på en Xen-hypervisor, trenger tilpasning for å kjøre på Xen.
			</div><div
            class="para">
				La oss bruke litt tid på vilkår. Hypervisoren er det nederste laget, som kjører direkte på maskinvaren, selv under kjernen. Denne hypervisor kan dele resten av programvaren over flere <span
              class="emphasis"><em>domener</em></span>, som kan sees på som så mange virtuelle maskiner. En av disse domenene (den første som blir startet) er kjent som <span
              class="emphasis"><em>dom0</em></span>, og har en spesiell rolle, siden bare dette domenet kan kontrollere hypervisor, og kjøring av andre domener. Disse andre domener er kjent som<span
              class="emphasis"><em>domU</em></span>. Med andre ord, og fra et brukersynspunkt, samsvarer <span
              class="emphasis"><em>dom0</em></span>-et med «verten» i andre visualiseringssystemer, mens en <span
              class="emphasis"><em>domU</em></span> kan bli sett på som en «gjest».
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>KULTUR</em></span> Xen og de ulike versjonene av Linux</strong></p></div></div></div><div
              class="para">
				Xen ble opprinnelig utviklet som et sett av oppdateringer ut fra det offisielle treet, men uten å bli integrert i Linux-kjernen. Samtidig krevde flere kommende virtualiseringssystemer (inkludert KVM) noen generiske virtualiseringsrelaterte funksjoner for å lette integrering sin, og Linux-kjernen fikk dette settet av funksjoner (kjent som <span
                class="emphasis"><em>paravirt_ops</em></span>, eller <span
                class="emphasis"><em>pv_ops</em></span>-grensesnittet). Ettersom Xen dupliserte noen av funksjonalitetene til dette grensesnittet, kunne de ikke bli akseptert offentlig .
			</div><div
              class="para">
				XenSource, selskapet bak Xen, måtte derfor legge til Xen i dette nye rammeverket, slik at Xens rettelser kunne flettes inn i den offisielle Linux-kjernen. Det betydde mye omskriving av kode, og selv om XenSource snart hadde en fungerende versjon basert på paravirt_ops-grensesnittet, ble rettelsene bare gradvis fusjonert inn den offisielle kjernen. Flettingen ble ferdigstilt i Linux 3.0. <div
                class="url">→ <a
                  href="http://wiki.xenproject.org/wiki/XenParavirtOps">http://wiki.xenproject.org/wiki/XenParavirtOps</a></div>
			</div><div
              class="para">
				Ettersom <span
                class="distribution distribution">Jessie</span> er basert på Linux-kjernens versjon 3.16, inkluderer standardpakkene <span
                class="pkg pkg">linux-image-686-pae</span>, og <span
                class="pkg pkg">linux-image-amd64</span> den nødvendige koden, og distribusjonsspesifikke rettelser som trengs til <span
                class="distribution distribution">Squeeze</span>, og tidligere versjoner av Debian er ikke nødvendig lenger. <div
                class="url">→ <a
                  href="http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix">http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix</a></div>
			</div></div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>NOTE</em></span> Arkitekturer som er kompatible med Xen</strong></p></div></div></div><div
              class="para">
				Xen er foreløpig kun tilgjengelig for i386-, AMD64-, ARM64- og ARHF-arkitekturer.
			</div></div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>KULTUR</em></span> Xen og ikke-Linux kjerner</strong></p></div></div></div><div
              class="para">
				Xen krever endringer i alle operativsystemer man ønsker å kjøre den på. Her har ikke alle kjerner samme nivå av modenhet. Mange er fullt funksjonelle, både som Dom0 og DomU: Linux 3.0 og senere, NetBSD 4.0 og senere, og OpenSolaris. Andre funger bare som en DomU. Du kan sjekke status for hvert operativsystem i Xen-Wikien: <div
                class="url">→ <a
                  href="http://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen">http://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen</a></div> <div
                class="url">→ <a
                  href="http://wiki.xenproject.org/wiki/DomU_Support_for_Xen">http://wiki.xenproject.org/wiki/DomU_Support_for_Xen</a></div>
			</div><div
              class="para">
				Men hvis Xen kan stole på maskinvarefunksjonene øremerket til virtualisering (som bare er til stede i nyere prosessorer), kan til og med ikke-modifiserte operativsystemer kjøres som DomU (inkludert Windows).
			</div></div><div
            class="para">
				Å bruke Xen under Debian krever tre komponenter:
			</div><div
            xmlns:d="http://docbook.org/ns/docbook"
            class="itemizedlist"><ul><li
                class="listitem"><div
                  class="para">
						Hypervisoren selv. Etter tilgjengelig maskinvare, vil den aktuelle pakken være enten <span
                    class="pkg pkg">xen-hypervisor-4.4-amd64</span>, <span
                    class="pkg pkg">xen-hypervisor-4.4-armhf</span>, eller <span
                    class="pkg pkg">xen-hypervisor-4.4-arm64</span>.
					</div></li><li
                class="listitem"><div
                  class="para">
						En kjerne som kjører på den aktuelle hypervisoren. Enhver kjerne nyere enn 3.0 vil gjøre det, inkludert 3.16 versjon i <span
                    class="distribution distribution">Jessie</span>.
					</div></li><li
                class="listitem"><div
                  class="para">
						i386-arkitekturen krever også et standard bibliotek med de riktige oppdateringer som drar nytte av Xen; dette er i <span
                    class="pkg pkg">libc6-xen</span>-pakken.
					</div></li></ul></div><div
            class="para">
				For å unngå å måtte velge disse komponentene for hånd, er noen hjelpepakker tilgjengelige (for eksempel <span
              class="pkg pkg">xen-linux-system-amd64</span>). De trekker alle inn en kjent, god kombinasjon med de aktuelle hypervisor- og kjernepakkene. Hypervisoren har også med <span
              class="pkg pkg">xen-utils-4.4</span>, som inneholder verktøy for å kontrollere hypervisoren fra Dom0. Dette bringer i sin tur det aktuelle standard biblioteket. Under installasjonen av alt dette, lager også konfigurasjonsskriptene en ny oppføring i Grub oppstartsmenyen, slik som å starte den valgte kjernen i en Xen Dom0. Merk imidlertid at denne inngangen vanligvis er satt som den første på listen, og vil derfor bli valgt som standard. Hvis det ikke er ønsket, vil følgende kommandoer endre det:
			</div><pre
            class="screen"><code
              class="computeroutput"># </code><strong
              class="userinput"><code>mv /etc/grub.d/20_linux_xen /etc/grub.d/09_linux_xen
</code></strong><code
              class="computeroutput"># </code><strong
              class="userinput"><code>update-grub
</code></strong></pre><div
            class="para">
				Når disse nødvendigheter er installert, er neste skritt å teste hvordan Dom0 selv virker. Dette innebærer omstart for hypervisoren og Xen-kjernen. Systemet skal starte på vanlig måte, med noen ekstra meldinger på konsollen under de tidlige initialiseringstrinnene.
			</div><div
            class="para">
				Nå er det faktisk på tide å installere nyttige systemer på DomU-systemene med verktøy fra <span
              class="pkg pkg">xen-tools</span>. Denne pakken leverer <code
              class="command">xen-create-image</code>-kommandoen, som i stor grad automatiserer oppgaven. Den eneste nødvendige parameteren er <code
              class="literal">--hostname</code>, som gir navn til DomU-en. Andre valg er viktige, men de kan lagres i <code
              class="filename">/etc/xen-tools/xen-tools.conf</code>-konfigurasjonsfilen, og fraværet deres fra kommandolinjen utløser ikke en feil. Det er derfor viktig å enten sjekke innholdet i denne filen før du oppretter bilder, eller å bruke ekstra parametre i bruken av <code
              class="command">xen-create-image</code>. Viktige parametre omfatter de følgende:
			</div><div
            xmlns:d="http://docbook.org/ns/docbook"
            class="itemizedlist"><ul><li
                class="listitem"><div
                  class="para">
						<code
                    class="literal">--memory</code>, for å spesifisere hvor mye RAM som er øremerket til det systemet som nettopp er laget;
					</div></li><li
                class="listitem"><div
                  class="para">
						<code
                    class="literal">--size</code> og <code
                    class="literal">--swap</code>, for å definere størrelsen på de «virtuelle disker» som er tilgjengelig for DomU-en;
					</div></li><li
                class="listitem"><div
                  class="para">
						<code
                    class="literal">--debootstrap</code>, for å få det nye systemet til å bli installert med <code
                    class="command">debootstrap</code>; i det tilfellet vil også <code
                    class="literal">--dist</code>-valget oftest bli brukt (med et distribusjonsnavn som <span
                    class="distribution distribution">jessie</span>).
					</div><div
                  class="sidebar"><div
                    class="titlepage"><div><div><p
                          class="title"><strong><span
                              class="emphasis"><em>FOR VIDEREKOMMENDE</em></span> Å installere et ikke-Debian-system i DomU</strong></p></div></div></div><div
                    class="para">
						Med et ikke-Linux-system må en, ved hjelp av <code
                      class="literal">--kernel</code>-valget, passe på å definere kjernen DomU må bruke.
					</div></div></li><li
                class="listitem"><div
                  class="para">
						<code
                    class="literal">--dhcp</code> sier at DomUs nettverkskonfigurasjon skal skaffes av DHCP, mens <code
                    class="literal">--ip</code> tillater å definere en statisk IP-adresse.
					</div></li><li
                class="listitem"><div
                  class="para">
						Til slutt må lagringsmetode velges for bildet som skal opprettes (de som vil bli sett på som harddisker fra DomU). Den enkleste metoden, tilsvarende <code
                    class="literal">--dir</code>-valget, er å opprette en fil på Dom0 for hver enhet der DomU skal være. For systemer som bruker LVM, er alternativet å bruke <code
                    class="literal">--lvm</code>-valget, fulgt av navnet på en volumgruppe; <code
                    class="command">xen-create-image</code> vil deretter opprette et nytt logisk volum inne i den gruppen, og dette logiske volumet vil bli tilgjengelig for DomU-et som en harddisk.
					</div><div
                  class="sidebar"><div
                    class="titlepage"><div><div><p
                          class="title"><strong><span
                              class="emphasis"><em>NOTE</em></span> Lagring i DomU</strong></p></div></div></div><div
                    class="para">
						Hele harddisker kan også bli eksportert til DomU, samt partisjoner, RAID-matriser, eller eksisterende logiske data fra tidligere. Disse operasjonene blir imidlertid ikke automatisert av <code
                      class="command">xen-create-image</code>, så å redigere Xen-bildets oppsettsfil er greit etter det første oppsettet med <code
                      class="command">xen-create-image</code>.
					</div></div></li></ul></div><div
            class="para">
				Så snart disse valgene er gjort, kan vi lage bildet til vår fremtidige Xen-DomU:
			</div><pre
            class="screen"><code
              class="computeroutput"># </code><strong
              class="userinput"><code>xen-create-image --hostname testxen --dhcp --dir /srv/testxen --size=2G --dist=jessie --role=udev</code></strong>
<code
              class="computeroutput">
[...]
General Information
--------------------
Hostname       :  testxen
Distribution   :  jessie
Mirror         :  http://ftp.debian.org/debian/
Partitions     :  swap            128Mb (swap)
                  /               2G    (ext3)
Image type     :  sparse
Memory size    :  128Mb
Kernel path    :  /boot/vmlinuz-3.16.0-4-amd64
Initrd path    :  /boot/initrd.img-3.16.0-4-amd64
[...]
Logfile produced at:
         /var/log/xen-tools/testxen.log

Installation Summary
---------------------
Hostname        :  testxen
Distribution    :  jessie
MAC Address     :  00:16:3E:8E:67:5C
IP-Address(es)  :  dynamic
RSA Fingerprint :  0a:6e:71:98:95:46:64:ec:80:37:63:18:73:04:dd:2b
Root Password   :  adaX2jyRHNuWm8BDJS7PcEJ
</code></pre><div
            class="para">
				Vi har nå en virtuell maskin, men den kjører ikke for øyeblikket (og bruker derfor bare plass på harddisken til Dom0). Selvfølgelig kan vi skape flere bilder, kanskje med ulike parametere.
			</div><div
            class="para">
				Før du slår disse virtuelle maskinene på, må vi definere tilgangen deres. De kan selvfølgelig sees som isolerte maskiner, som bare nås gjennom sine systemkonsoller. Men dette samsvarer sjelden med bruksmønsteret. Mesteparten av tiden blir en DomU betraktet som en ekstern tjener, og kun tilgjengelig gjennom et nettverk. Det vil være ganske upraktisk å legge til et nettverkskort for hver DomU; som er grunnen til at Xen tillater å lage virtuelle grensesnitt, som hvert domene kan se og bruke som standard. Merk at disse kortene, selv om de er virtuelle, bare vil være nyttige så snart de er koblet til et nettverk, selv et virtuelt et. Xen har flere nettverksmodeller for det:
			</div><div
            xmlns:d="http://docbook.org/ns/docbook"
            class="itemizedlist"><ul><li
                class="listitem"><div
                  class="para">
						Den enkleste er <span
                    class="emphasis"><em>bridge</em></span>-modellen. Alle eth0-nettverkskort (både i Dom0- og DomU-systemer) oppfører seg som om de var direkte koblet til en Ethernet-svitsj.
					</div></li><li
                class="listitem"><div
                  class="para">
						Så følger <span
                    class="emphasis"><em>routing</em></span>-modellen, hvor Dom0 oppfører seg som en ruter som står mellom DomU-systemer og det (fysiske) eksterne nettverket.
					</div></li><li
                class="listitem"><div
                  class="para">
						Til slutt, i <span
                    class="emphasis"><em>NAT</em></span>-modellen, der Dom0 igjen er mellom DomU-systemene og resten av nettverket, men DomU-systemene er ikke direkte tilgjengelig utenfra, og trafikken går gjennom noen nettverksadresseoversettelser på Dom0-et.
					</div></li></ul></div><div
            class="para">
				Disse tre nettverksnodene innebefatter en rekke grensesnitt med uvanlige navn, for eksempel <code
              class="filename">vif*</code>, <code
              class="filename">veth*</code>, <code
              class="filename">peth*</code> og <code
              class="filename">xenbr0</code>. Xen-hypervisoren setter dem opp, uansett med hvilken layout de er definert i, under kontroll av verktøyet for brukerrom. Siden NAT- og rutingmodellene bare er tilpasset det enkelte tilfelle, vil vi bare omhandle brobyggingsmodellen.
			</div><div
            class="para">
				Standardkonfigurasjon av Xen-pakkene endrer ikke hele systemets nettverksoppsett. Men <code
              class="command">xend</code>-nissen er konfigurert for å integrere inn virtuelle nettverksgrensesnitt i alle tilstedeværende nettverksbroer (der <code
              class="filename">xenbr0</code> tar forrang dersom flere slike broer finnes). Vi må derfor sette opp en bro i <code
              class="filename">/etc/network/interfaces</code> (som krever installasjon av <span
              class="pkg pkg">bridge-utils</span>-pakken, som er grunnen til at <span
              class="pkg pkg">xen-utils-4.4</span>-pakken anbefaler den) for å erstatte den eksisterende eth0-inngangen:
			</div><pre
            class="programlisting">auto xenbr0
iface xenbr0 inet dhcp
    bridge_ports eth0
    bridge_maxwait 0
</pre><div
            class="para">
				Etter omstart, for å sørge for at brua blir opprettet automatisk, kan vi nå starte DomU med Xen-kontrollverktøyet, spesielt <code
              class="command">xl</code>-kommandoen. Denne kommandoen tillater ulike håndteringer av domenene, inkludert å føre dem opp, og starte/stoppe dem.
			</div><pre
            class="screen"><code
              class="computeroutput"># </code><strong
              class="userinput"><code>xl list</code></strong>
<code
              class="computeroutput">Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0   463     1     r-----      9.8
# </code><strong
              class="userinput"><code>xl create /etc/xen/testxen.cfg</code></strong>
<code
              class="computeroutput">Parsing config from /etc/xen/testxen.cfg
# </code><strong
              class="userinput"><code>xl list</code></strong>
<code
              class="computeroutput">Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0   366     1     r-----     11.4
testxen                                      1   128     1     -b----      1.1</code></pre><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>VERKTØY</em></span> Valg av verktøysamling for å håndtere Xen VM</strong></p></div></div></div><a
              id="id-1.15.5.12.24.2"
              class="indexterm"></a><a
              id="id-1.15.5.12.24.3"
              class="indexterm"></a><div
              class="para">
				I Debian 7 og eldre versjoner, kommandolinjeverktøyet <code
                class="command">xm</code> var referansen når en skulle administrere Xen virtuelle maskiner. Nå er det erstattet av <code
                class="command">xl</code>, som er mest bakoverkompatibelt. Men de er ikke det eneste tilgjengelige verktøyet: <code
                class="command">virsh</code> i libvirt og <code
                class="command">xe</code> til XenServers XAPI (kommersielt tilbud for Xen), er alternative verktøy.
			</div></div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>VÆR VARSOM</em></span> Bare ett DomU per bilde!</strong></p></div></div></div><div
              class="para">
				Mens det selvfølgelig er mulig å ha flere DomU-systemer som kjører parallelt, har alle behov for å bruke sitt eget bilde, siden hvert DomU er laget for å tro det kjører på sin egen maskinvare (bortsett fra den lille biten av kjernen som snakker til hypervisor). Spesielt er det ikke mulig for to DomU-systemer, som kjører samtidig, å dele lagringsplass. Hvis DomU-systemene ikke kjører samtidig, er det imidlertid fullt mulig å gjenbruke en enkel vekselminnepartisjon, eller partisjonen som er vert for filsystemet <code
                class="filename">/home</code>.
			</div></div><div
            class="para">
				Merk at <code
              class="filename">testxen</code>-DomU bruker virkelig minne tatt fra RAM som ellers ville være tilgjengelig for Dom0, og ikke simulert minne. Når du bygger en tjener som skal være vert for Xen-bruk, pass på å sette av tilstrekkelig fysisk RAM.
			</div><div
            class="para">
				Se der! Vår virtuelle maskin starter opp. Vi får tilgang til den i en av to modi. Den vanlige måten er å koble seg til «eksternt» gjennom nettverket, slik som vi ville koble til en ekte maskin; Det vil som regel enten kreve oppsett av en DHCP-tjener, eller en DNS-konfigurasjon. Den andre måten, som kan være den eneste måten hvis nettverkskonfigurasjonen var feil, er å bruke <code
              class="filename">hvc0</code>-konsollet, med <code
              class="command">xl console</code>-kommandoen:
			</div><pre
            class="screen"><code
              class="computeroutput"># </code><strong
              class="userinput"><code>xl console testxen</code></strong>
<code
              class="computeroutput">[...]

Debian GNU/Linux 8 testxen hvc0

testxen login: </code></pre><div
            class="para">
				Man kan så åpne en sesjon, akkurat som man ville gjøre hvis du sitter med den virtuelle maskinens tastatur. Frakobling fra denne konsollen oppnås med <span
              class="keycap"><strong>Control</strong></span>+<span
              class="keycap"><strong>]</strong></span>-tastekombinasjon.
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>TIPS</em></span> Å få konsollen umiddelbart</strong></p></div></div></div><div
              class="para">
				Noen ganger ønsker man å starte et DomU-system, og med en gang få adgang til konsollen dens; dette er grunnen til at <code
                class="command">xl create</code>-kommandoen velger en <code
                class="literal">-c</code>-bryter. Å starte en DomU med denne bryteren vil vise alle meldingene når systemet starter.
			</div></div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>VERKTØY</em></span> OpenXenManager</strong></p></div></div></div><div
              class="para">
				OpenXenManager (i <span
                class="pkg pkg">openxenmanager</span>-pakken) er et grafisk grensesnitt som tillater fjernadministrasjon av Xen-domener via Xen API. Den kan dermed eksternt styre Xen-domener, og har med de fleste av funksjonene i <code
                class="command">xl</code>-kommandoen.
			</div></div><div
            class="para">
				Når DomU kjører, kan den brukes akkurat som en hvilken som helst annen tjener (siden den er et GNU/Linux-system tross alt). Imidlertid tillater den virtuelle maskinstatusen noen ekstra funksjoner. For eksempel kan en DomU midlertidig stoppes, og så begynne igjen, med <code
              class="command">xl pause</code>, og <code
              class="command">xl unpause</code>-kommandoer. Merk at selv om DomU i pause ikke bruker noen prosessorkraft, er det tildelte minne fortsatt i bruk. Det kan være interessant å vurdere <code
              class="command">xl save</code> og <code
              class="command">xl restore</code>kommandoene: Å spare en DomU frigjør ressursene den tidligere brukte, inkludert RAM. Når gjenopptatt (eller avpauset, for den saks skyld), legger ikke DomU en gang merke til noe utover tiden som går. Hvis en DomU var i gang når Dom0 er stengt, lagrer skriptpakken automatisk DomU-et, og gjenopprette den ved neste oppstart. Dette vil selvfølgelig medføre at de inntrufne standard ubekvemmelighetene påløper når en bærbar datamaskin settes i dvalemodus. For eksempel, spesielt hvis DomU er suspendert for lenge, kan nettverkstilkoblinger utløpe. Merk også at Xen så langt er uforenlig med en stor del av ACPI-strømstyring, noe som utelukker suspensjon av vert-(Dom0)systemet.
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>DOKUMENTASJON</em></span> <code
                        class="command">xl</code>-valg</strong></p></div></div></div><div
              class="para">
				De fleste av <code
                class="command">xl</code>-underkommandoer forventer ett eller flere argumenter, ofte et DomU-navn. Disse argumentene er godt beskrevet på denne manualsiden <span
                class="citerefentry"><span
                  class="refentrytitle">xl</span>(1)</span>.
			</div></div><div
            class="para">
				Stanse eller restarte en DomU kan gjøres enten fra DomU-et (med <code
              class="command">shutdown</code> command) eller fra Dom0, med <code
              class="command">xl shutdown</code>, eller <code
              class="command">xl reboot</code>.
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>FOR VIDEREKOMMENDE</em></span> Avansert Xen</strong></p></div></div></div><div
              class="para">
				Xen har mange flere funksjoner enn vi kan beskrive i et par avsnitt. Spesielt er systemet meget dynamisk, og mange parametere for et domene (for eksempel mengden av avsatt hukommelse, de synlige harddisker, oppførselen til oppgaveplanleggeren, og så videre) kan justeres selv når domenet er i gang. Et DomU kan også overføres på tvers av tjenere uten å bli slått av, og uten å miste sine nettverkstilkoblinger! For alle disse avanserte mulighetene er primærkilden til informasjon den offisielle Xen-dokumentasjonen. <div
                class="url">→ <a
                  href="http://www.xen.org/support/documentation.html">http://www.xen.org/support/documentation.html</a></div>
			</div></div></div><div
          class="section"><div
            class="titlepage"><div><div><h3
                  class="title"><a
                    id="sect.lxc"></a>12.2.2. LXC</h3></div></div></div><a
            id="id-1.15.5.13.2"
            class="indexterm"></a><div
            class="para">
				Selv om den brukes til å bygge «virtuelle maskiner», er LXC strengt tatt ikke et virtualiseringssystem, men et system for å isolere grupper av prosesser fra hverandre, selv om de alle kjører på den samme verten. Den trekker veksler på et sett av nyere utviklinger i Linux-kjernen, velkjent som <span
              class="emphasis"><em>kontrollgrupper</em></span>, der forskjellige sett med prosesser som kalles «grupper» har forskjellige visninger av forskjellige aspekter ved det totale systemet. Mest kjent blant disse aspektene er prosessidentifikatorene, nettverkskonfigurasjonene og monteringspunktene. En slik gruppe av isolerte prosesser vil ikke ha noen adgang til de andre prosesser i systemet, og gruppens adgang til filsystemet kan være begrenset til en spesifikk undergruppe. Den kan også ha sitt eget nettverksgrensesnitt og rutingstabell, og den kan være konfigurert til å bare se et delsett av de tilgjengelige verktøy som finnes i systemet.
			</div><div
            class="para">
				Disse funksjonene kan kombineres for å isolere en hel prosessfamilie som starter fra <code
              class="command">init</code>-prossessen, og det resulterende settet ser mye ut som en virtuell maskin. Det offisielle navnet på et slikt oppsett er en «beholder» (derav LXC-forkortelsen:<span
              class="emphasis"><em>LinuX Containers</em></span>), men en ganske viktig forskjell til «ekte» virtuelle maskiner, som leveres av Xen eller KVM, er at det ikke er noen andrekjerne; beholderen bruker den samme kjernen som vertssystemet. Dette har både fordeler og ulemper: Fordelene inkluderer utmerket ytelse grunnet total mangel på ekstrabelastning, og det faktum at kjernen har full oversikt over alle prosesser som kjører på systemet, slik at planleggingen kan være mer effektiv enn hvis to uavhengige kjerner skulle planlegge ulike oppgavesett. Den største blant ulempene er at det er umulig å kjøre en annen kjerne i en beholder (enten en annen Linux-versjon, eller et annet operativsystem i det hele tatt).
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>NOTE</em></span> LXC isolasjonsgrenser</strong></p></div></div></div><div
              class="para">
				LXC beholdere gir ikke det isolasjonsnivået som oppnås med tyngre emulatorer eller virutaliseringer. Spesielt:
			</div><div
              xmlns:d="http://docbook.org/ns/docbook"
              class="itemizedlist"><ul><li
                  class="listitem"><div
                    class="para">
						Ettersom kjernen er delt mellom vertssystemet og beholderne, kan prosesser avgrenset til beholdere fortsatt få tilgang til kjernemeldinger, noe som kan føre til informasjonslekkasje hvis meldingene er sendt ut fra en beholder;
					</div></li><li
                  class="listitem"><div
                    class="para">
						av lignende grunner, hvis en beholder er kompromittert og et sikkerhetsproblem i kjernen utnyttes, kan de øvrige beholdere også bli påvirket;
					</div></li><li
                  class="listitem"><div
                    class="para">
						på filsystemet, kjernen sjekker tillatelser etter de numeriske iidentifikatorer for brukere og grupper. Disse identifikatorene kan utpeke ulike brukere og grupper avhengig av beholderen, noe en bør huske på om skrivbare deler av filsystemet er delt mellom beholdere.
					</div></li></ul></div></div><div
            class="para">
				Siden vi har å gjøre med isolasjon, og ikke vanlig virtualisering, er å sette opp LXC-beholdere mer komplisert enn bare å kjøre en Debian-installer på en virtuell maskin. Vi vil beskrive noen forutsetninger, og deretter gå videre til nettverkskonfigurasjonen. Da vil vi faktisk være i stand til å lage systemet som skal kjøres i beholderen.
			</div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      id="id-1.15.5.13.7"></a>12.2.2.1. Innledende skritt</h4></div></div></div><div
              class="para">
					Pakken <span
                class="pkg pkg">lxc</span> inneholder de verktøyene som kreves for å kjøre LXC, og må derfor være installert.
				</div><div
              class="para">
					LXC krever også oppsettssystemet <span
                class="emphasis"><em>kontrollgrupper</em></span>, som er et virtuelt filsystem til å monteres på <code
                class="filename">/sys/fs/cgroup</code>. Ettersom Debian 8 byttet til systemd, som også er avhengig av kontrollgrupper, gjøres dette nå automatisk ved oppstart uten ytterligere konfigurasjon.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      id="sect.lxc.network"></a>12.2.2.2. Nettverksoppsett</h4></div></div></div><div
              class="para">
					Målet med å installere LXC er å sette opp virtuelle maskiner; mens vi selvfølgelig kan holde dem isolert fra nettverket, og bare kommunisere med dem via filsystemet, innebærer de fleste brukstilfeller i det minste å gi minimal nettverkstilgang til beholderne. I det typiske tilfellet vil hver beholder få et virtuelt nettverksgrensesnitt koblet til det virkelige nettverket via en bro. Dette virtuelle grensesnittet kan kobles enten direkte på vertens fysiske nettverksgrensesnitt (der beholderen er direkte på nettverket), eller på et annet virtuelt grensesnitt som er definert hos verten (og verten kan da filtrere eller rute trafikk). I begge tilfelle kreves <span
                class="pkg pkg">bridge-utils</span>-pakken.
				</div><div
              class="para">
					Det enkle tilfellet gjelder bare redigering <code
                class="filename">/etc/network/interfaces</code>, å flytte oppsettet for det fysiske grensesnittet (for eksempel <code
                class="literal">eth0</code>) til et brogrensesnitt (vanligvis <code
                class="literal">br0</code>), og konfigurere koblingen mellom dem. For eksempel, hvis nettverkskonfigurasjonsfilen i utgangspunktet inneholder oppføringer som de følgende:
				</div><pre
              class="programlisting">auto eth0
iface eth0 inet dhcp</pre><div
              class="para">
					Bør de deaktiveres og erstattes med følgende:
				</div><pre
              class="programlisting">#auto eth0
#iface eth0 inet dhcp

auto br0
iface br0 inet dhcp
  bridge-ports eth0</pre><div
              class="para">
					Effekten av denne konfigurasjonen vil ligne på hva som ville blitt oppnådd dersom beholderne var maskiner koblet til det samme fysiske nettverket som vert. Bro-konfigurasjon (“bridge”-konfigurasjonen) håndterer transitt av Ethernet-rammer mellom alle bro-grensesnitt som inkluderer fysisk <code
                class="literal">eth0</code>, samt grensesnittet definert for beholderne.
				</div><div
              class="para">
					I tilfeller der denne konfigurasjonen ikke kan brukes (for eksempel hvis ingen offentlige IP-adresser kan tildeles beholderne), blir et virtuelt <span
                class="emphasis"><em>tap</em></span>-grensesnitt opprettet og koblet til broen. Den tilsvarende nettverkssammenhengen blir da som en vert med et andre nettverkskort koblet til en egen bryter, med også beholderne koblet til denne bryteren. Verten fungerer da som en inngangsport for beholdere hvis de er ment å kommunisere med omverdenen.
				</div><div
              class="para">
					I tillegg til <span
                class="pkg pkg">bridge-utils</span>, krever denne «rike» konfigurasjonen <span
                class="pkg pkg">vde2</span>-pakken; <code
                class="filename">/etc/network/interfaces</code>-filen blir da:
				</div><pre
              class="programlisting"># Interface eth0 is unchanged
auto eth0
iface eth0 inet dhcp

# Virtual interface 
auto tap0
iface tap0 inet manual
  vde2-switch -t tap0

# Bridge for containers
auto br0
iface br0 inet static
  bridge-ports tap0
  address 10.0.0.1
  netmask 255.255.255.0
</pre><div
              class="para">
					Nettverket kan så bli satt opp enten statisk i beholderne, eller dynamisk med en DHCP-tjener som kjører hos verten. En slik DHCP-tjener må konfigureres til å svare på spørsmål om <code
                class="literal">br0</code>-grensesnittet.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      id="id-1.15.5.13.9"></a>12.2.2.3. Å sette opp systemet</h4></div></div></div><div
              class="para">
					La oss nå sette opp filsystemet som skal brukes av beholderen. Siden denne «virtuelle maskinen» ikke vil kjøres direkte på maskinvare, er noen finjusteringer nødvendige sammenlignet med et standard filsystem, spesielt så langt som kjernen, enheter og konsollene angår. Heldigvis inkluderer <span
                class="pkg pkg">lxc</span> skript som stort sett automatiserer denne konfigurasjonen. For eksempel vil følgende kommandoer (som krever <span
                class="pkg pkg">debootstrap</span> og <span
                class="pkg pkg">rsync</span>-packages) installere en Debian beholder:
				</div><pre
              class="screen"><code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>lxc-create -n testlxc -t debian
</code></strong><code
                class="computeroutput">debootstrap is /usr/sbin/debootstrap
Checking cache download in /var/cache/lxc/debian/rootfs-jessie-amd64 ... 
Downloading debian minimal ...
I: Retrieving Release 
I: Retrieving Release.gpg 
[...]
Download complete.
Copying rootfs to /var/lib/lxc/testlxc/rootfs...
[...]
Root password is 'sSiKhMzI', please change !
root@mirwiz:~# </code>
</pre><div
              class="para">
					Merk at filsystemet opprinnelig er opprettet i <code
                class="filename">/var/cache/lxc</code>, og deretter flyttet til den katalogen filsystemet skal til. Dette gjør det mulig å lage identiske beholdere mye raskere, ettersom det da bare kreves kopiering.
				</div><div
              class="para">
					Merk at Debian-skriptet, for å opprette maler, godtar et <code
                class="option">--arch</code>-valg for å spesifisere arkitekturen til systemet som skal installeres, og et <code
                class="option">--release</code>-valg hvis du ønsker å installere noe annet enn den nåværende stabile utgaven av Debian. Du kan også sette omgivelsesvariabelen <code
                class="literal">MIRROR</code> til å peke på et lokalt Debian speil.
				</div><div
              class="para">
					Nå inneholder det nyopprettede filsystemet et minimalt Debian-system, og som standard har ikke beholderen nettverksgrensesnitt (utover filmonteringen). Siden dette ikke er virkelig ønsket, vil vi endre beholderens konfigurasjonsfil (<code
                class="filename">/var/lib/lxc/testlxc/config</code>), og legge til noen få <code
                class="literal">lxc.network.*</code>-innganger:
				</div><pre
              class="programlisting">lxc.network.type = veth
lxc.network.flags = up
lxc.network.link = br0
lxc.network.hwaddr = 4a:49:43:49:79:20</pre><div
              class="para">
					Disse oppføringene betyr, henholdsvis, at et virtuelt grensesnitt vil bli opprettet i beholderen; at det automatisk vil bli vist når det blir meldt at beholderen er startet; at det automatisk vil bli koblet til <code
                class="literal">br0</code>-broen hos verten; og at MAC-adressen vil være som spesifisert. Skulle denne siste posten mangle eller være deaktivert, vil det genereres en tilfeldig MAC-adresse.
				</div><div
              class="para">
					En annen nyttig inngang i den filen er innstillingen for vertsnavnet:
				</div><pre
              class="programlisting">lxc.utsname = testlxc</pre></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      id="id-1.15.5.13.10"></a>12.2.2.4. Å starte beholderen</h4></div></div></div><div
              class="para">
					Nå som vårt virtuelle maskinbilde er klart, la oss starte beholderen:
				</div><pre
              class="screen scale"
              width="94"><code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>lxc-start --daemon --name=testlxc
</code></strong><code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>lxc-console -n testlxc
</code></strong><code
                class="computeroutput">Debian GNU/Linux 8 testlxc tty1

testlxc login: </code><strong
                class="userinput"><code>root</code></strong><code
                class="computeroutput">
Password: 
Linux testlxc 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt11-1 (2015-05-24) x86_64

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
root@testlxc:~# </code><strong
                class="userinput"><code>ps auxwf</code></strong>
<code
                class="computeroutput">USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.2  28164  4432 ?        Ss   17:33   0:00 /sbin/init
root        20  0.0  0.1  32960  3160 ?        Ss   17:33   0:00 /lib/systemd/systemd-journald
root        82  0.0  0.3  55164  5456 ?        Ss   17:34   0:00 /usr/sbin/sshd -D
root        87  0.0  0.1  12656  1924 tty2     Ss+  17:34   0:00 /sbin/agetty --noclear tty2 linux
root        88  0.0  0.1  12656  1764 tty3     Ss+  17:34   0:00 /sbin/agetty --noclear tty3 linux
root        89  0.0  0.1  12656  1908 tty4     Ss+  17:34   0:00 /sbin/agetty --noclear tty4 linux
root        90  0.0  0.1  63300  2944 tty1     Ss   17:34   0:00 /bin/login --     
root       117  0.0  0.2  21828  3668 tty1     S    17:35   0:00  \_ -bash
root       268  0.0  0.1  19088  2572 tty1     R+   17:39   0:00      \_ ps auxfw
root        91  0.0  0.1  14228  2356 console  Ss+  17:34   0:00 /sbin/agetty --noclear --keep-baud console 115200 38400 9600 vt102
root       197  0.0  0.4  25384  7640 ?        Ss   17:38   0:00 dhclient -v -pf /run/dhclient.eth0.pid -lf /var/lib/dhcp/dhclient.e
root       266  0.0  0.1  12656  1840 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty5 linux
root       267  0.0  0.1  12656  1928 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty6 linux
root@testlxc:~# </code></pre><div
              class="para">
					Nå er vi i beholderen; vår tilgang til prosessene er begrenset til bare dem som er startet fra beholderen selv, og vår tilgang til filsystemet er tilsvarende begrenset til den øremerkede undergruppen i hele filsystemet (<code
                class="filename">/var/lib/lxc/testlxc/rootfs</code>). Vi kan gå ut av konsollet med <span
                class="keycap"><strong>Control</strong></span>+<span
                class="keycap"><strong>a</strong></span> <span
                class="keycap"><strong>q</strong></span>.
				</div><div
              class="para">
					Legg merke til at vi kjørte beholderen som en bakgrunnsprosess, takket være <code
                class="option">--daemon</code>-valget til <code
                class="command">lxc-start</code>. Vi kan avbryte beholderen med en kommando slik som <code
                class="command">lxc-stop --name=testlxc</code>.
				</div><div
              class="para">
					Pakken <span
                class="pkg pkg">lxc</span> inneholder et initialiseringsskript som automatisk kan starte en eller flere beholdere når verten starter opp (det er avhengig av <code
                class="command">lxc-autostart</code> som starter beholdere der <code
                class="literal">lxc.start.auto</code>-valget er satt til 1). Mer finkornet kontroll over oppstartsrekkefølgen er mulig med <code
                class="literal">lxc.start.order</code> og <code
                class="literal">lxc.group</code>. Som standard starter klargjøringsskriptet først beholdere som er en del av <code
                class="literal">onboot</code>-gruppen, og deretter beholdere som ikke er en del av en gruppe. I begge tilfeller er rekkefølgen innenfor en gruppe definert av <code
                class="literal">lxc.start.order</code>-valget.
				</div><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>FOR VIDEREKOMMENDE</em></span> Synliggjøring av masse</strong></p></div></div></div><div
                class="para">
					Siden LXC er et meget lett isolasjonssystem, kan det spesielt tilpasses til å være et massivt vertskap for virtuelle servere. Nettverkskonfigurasjonen vil trolig være litt mer avansert enn hva vi beskrev ovenfor, men den «rike» konfigurasjon som bruker <code
                  class="literal">tap</code> og <code
                  class="literal">veth</code>-grensesnitt skulle i mange tilfelle være nok.
				</div><div
                class="para">
					Det kan også være fornuftig å ha en del av filsystemet felles, slik som <code
                  class="filename">/usr</code> og <code
                  class="filename">/lib</code>-undertrærne, slik at man unngår å duplisere programvaren som kanskje må være felles for flere containere. Dette vil vanligvis oppnås med <code
                  class="literal">lxc.mount.entry</code>-innganger i beholdernes konfigurasjonsfil. En interessant bieffekt er at prosessene da vil bruke mindre fysisk minne, siden kjernen er i stand til å oppdage felles programmer. Den marginale belastningen for en ekstra beholder kan da reduseres til diskplassen øremerket til dens spesifikke data, og noen ekstra prosesser som kjernen må planlegge, og administrere.
				</div><div
                class="para">
					Vi har selvfølgelig ikke beskrevet alle de tilgjengelige alternativene. Mer omfattende informasjon kan fås fra <span
                  class="citerefentry"><span
                    class="refentrytitle">lxc</span>(7)</span> og <span
                  class="citerefentry"><span
                    class="refentrytitle">lxc.container.conf</span>(5)</span>-manualsider og sidene de refererer til.
				</div></div></div></div><div
          class="section"><div
            class="titlepage"><div><div><h3
                  class="title"><a
                    id="id-1.15.5.14"></a>12.2.3. Virtualisering med KVM</h3></div></div></div><a
            id="id-1.15.5.14.2"
            class="indexterm"></a><div
            class="para">
				KVM, som står for <span
              class="emphasis"><em>Kernel-based Virtual Machine</em></span>, er først og fremst en kjernemodul som gir det meste av infrastrukturen som kan brukes av en visualiserer, men er ikke selv en visualiserer. Faktisk kontroll av visualiseringen håndteres av en QEMU-basert applikasjon. Ikke være bekymret om denne seksjonen nevner <code
              class="command">qemu-*</code>-kommandoer, den handler fremdeles om KVM.
			</div><div
            class="para">
				I motsetning til andre visualiseringssystemer, ble KVM fusjonert inn i Linux-kjernen helt fra starten. Utviklerne valgte å dra nytte av prosessorens instruksjonssett øremerket til visualisering (Intel-VT og AMD-V), som holder KVM lett, elegant og ikke ressurskrevende. Motstykket, selvfølgelig, er at KVM ikke fungerer på alle datamaskiner, men bare på dem med riktige prosessorer. For x86-datamaskiner kan du bekrefte at du har en slik prosessor ved å se etter «vmx» eller «svm» i CPU-flagg oppført i <code
              class="filename">/proc/cpuinfo</code>.
			</div><div
            class="para">
				Med Red Hats aktive støtte til utviklingen, har KVM mer eller mindre blitt referansen for Linux-virtualisering.
			</div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      id="id-1.15.5.14.6"></a>12.2.3.1. Innledende skritt</h4></div></div></div><a
              id="id-1.15.5.14.6.2"
              class="indexterm"></a><div
              class="para">
					I motsetning til verktøy som VirtualBox, har KVM selv ikke noe brukergrensesnitt for å opprette og administrere virtuelle maskiner. Pakken <span
                class="pkg pkg">qemu-kvm</span> gir bare en kjørbar som kan starte en virtuell maskin, samt et initialiseringsskript som laster de aktuelle kjernemodulene.
				</div><a
              id="id-1.15.5.14.6.4"
              class="indexterm"></a><a
              id="id-1.15.5.14.6.5"
              class="indexterm"></a><div
              class="para">
					Heldigvis gir Red Hat også et annet sett med verktøy for å løse dette problemet ved utvikling av <span
                class="emphasis"><em>libvirt</em></span>-bibliotektet og de tilhørende <span
                class="emphasis"><em>virtual machine manager</em></span>-verktøyene. libvirt kan administrere virtuelle maskiner på en enhetlig måte, uavhengig av virtualiseringen bak i kulissene (det støtter for tiden QEMU, KVM, Xen, LXC, OpenVZ, VirtualBox, VMWare og UML). <code
                class="command">virtual-manager</code> er et grafisk grensesnitt som bruker libvirt til å opprette og administrere virtuelle maskiner.
				</div><a
              id="id-1.15.5.14.6.7"
              class="indexterm"></a><div
              class="para">
					Vi installerer først de nødvendige pakker med <code
                class="command">apt-get install qemu-kvm libvirt-bin virtinst virt-manager virt-viewer</code>. <span
                class="pkg pkg">libvirt-bin</span> gir <code
                class="command">libvirtd</code>-nissen, som tillater (potensielt ekstern) håndtering av virtuelle maskiner som kjører på verten, og starter de nødvendige VM-er når verten starter opp. I tillegg gir denne pakken <code
                class="command">virsh</code>-kommandolinjeverktøy som gjør det mulig å styre <code
                class="command">libvirtd</code>-håndterte maskiner.
				</div><div
              class="para">
					Pakken <span
                class="pkg pkg">virtinst</span> leverer <code
                class="command">virt-install</code>, som tillater å lage virtuelle maskiner fra kommandolinjen. Avslutningsvis gir <span
                class="pkg pkg">virt-viewer</span> tilgang til en VM-grafiske konsoll.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      id="id-1.15.5.14.7"></a>12.2.3.2. Nettverksoppsett</h4></div></div></div><div
              class="para">
					Akkurat som i Xen og LXC, innebærer den hyppigste nettverkskonfigurasjon en bro som grupperer nettverksgrensesnittene og de virtuelle maskinene (se <a
                class="xref"
                href="sect.virtualization.html#sect.lxc.network">Seksjon 12.2.2.2, «Nettverksoppsett»</a>).
				</div><div
              class="para">
					Alternativt, og i standardkonfigurasjonen levert av KVM, er den virtuelle maskinen tildelt en privat adresse (i 192.168.122.0/24-området), og NAT er satt opp slik at VM kan få tilgang til nettverket utenfor.
				</div><div
              class="para">
					Resten av denne seksjonen forutsetter at verten har et <code
                class="literal">eth0</code> fysisk grensesnitt, og en <code
                class="literal">br0</code>-bro, og den første er knyttet til den siste.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      id="id-1.15.5.14.8"></a>12.2.3.3. Installasjon med <code
                      class="command">virt-install</code></h4></div></div></div><a
              id="id-1.15.5.14.8.2"
              class="indexterm"></a><div
              class="para">
					Å lage en virtuell maskin er svært lik å installere et normalt system, bortsett fra at den virtuelle maskinens egenskaper er beskrevet i en tilsynelatende uendelig kommandolinje.
				</div><div
              class="para">
					I praksis betyr dette at vi vil bruke Debians installasjonsprogram ved å starte den virtuelle maskinen på en virtuell DVD-ROM-stasjon som er tilordnet til et Debian DVD-bilde som ligger hos vertsystemet. VM vil eksportere sin grafiske konsoll over VNC-protokollen (se <a
                class="xref"
                href="sect.remote-login.html#sect.remote-desktops">Seksjon 9.2.2, «Å bruke eksterne grafiske skrivebord»</a> for detaljer), som tillater oss å kontrollere installasjonsprosessen.
				</div><div
              class="para">
					Vi må først fortelle libvirtd hvor diskbildene skal lagres, med mindre standardplasseringen (<code
                class="filename">/var/lib/libvirt/images/</code>) er grei.
				</div><pre
              class="screen"><code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>mkdir /srv/kvm</code></strong>
<code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>virsh pool-create-as srv-kvm dir --target /srv/kvm</code></strong>
<code
                class="computeroutput">Pool srv-kvm created

root@mirwiz:~# </code></pre><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>TIPS</em></span> Å legge til din bruker til libvirt-gruppen</strong></p></div></div></div><div
                class="para">
					Alle eksempler i denne seksjonen forutsetter at du kjører kommandoene som rot. Effektivt, hvis du ønsker å styre en lokal libvirt-nisse, må du enten være rot, eller være medlem av <code
                  class="literal">libvirt</code>-gruppen (som ikke er tilfelle som standard). Så hvis du ønsker å unngå å bruke rotrettigheter for ofte, kan du legge deg selv til <code
                  class="literal">libvirt</code>-gruppen, og kjøre de forskjellige kommandoene under din brukeridentitet.
				</div></div><div
              class="para">
					La oss nå starte installasjonsprosessen for den virtuelle maskinen, og ta en nærmere titt på de viktigste valgene til <code
                class="command">virt-install</code>. Denne kommandoen registrerer den virtuelle maskinen med parametre i libvirtd, og starter den deretter slik at installasjonen kan fortsette.
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>virt-install --connect qemu:///system  <span
                    id="virtinst.connect"><img
                      class="callout"
                      src="Common_Content/images/1.png"
                      alt="1" /></span>
               --virt-type kvm           <span
                    id="virtinst.type"><img
                      class="callout"
                      src="Common_Content/images/2.png"
                      alt="2" /></span>
               --name testkvm            <span
                    id="virtinst.name"><img
                      class="callout"
                      src="Common_Content/images/3.png"
                      alt="3" /></span>
               --ram 1024                <span
                    id="virtinst.ram"><img
                      class="callout"
                      src="Common_Content/images/4.png"
                      alt="4" /></span>
               --disk /srv/kvm/testkvm.qcow,format=qcow2,size=10 <span
                    id="virtinst.disk"><img
                      class="callout"
                      src="Common_Content/images/5.png"
                      alt="5" /></span>
               --cdrom /srv/isos/debian-8.1.0-amd64-netinst.iso  <span
                    id="virtinst.cdrom"><img
                      class="callout"
                      src="Common_Content/images/6.png"
                      alt="6" /></span>
               --network bridge=br0      <span
                    id="virtinst.network"><img
                      class="callout"
                      src="Common_Content/images/7.png"
                      alt="7" /></span>
               --vnc                     <span
                    id="virtinst.vnc"><img
                      class="callout"
                      src="Common_Content/images/8.png"
                      alt="8" /></span>
               --os-type linux           <span
                    id="virtinst.os"><img
                      class="callout"
                      src="Common_Content/images/9.png"
                      alt="9" /></span>
               --os-variant debianwheezy
</code></strong><code
                class="computeroutput">
Starting install...
Allocating 'testkvm.qcow'             |  10 GB     00:00
Creating domain...                    |    0 B     00:00
Guest installation complete... restarting guest.
</code></pre><div
              class="calloutlist"><table
                border="0"
                summary="Callout list"><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.connect"><img
                          class="callout"
                          src="Common_Content/images/1.png"
                          alt="1" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							Valget <code
                        class="literal">--connect</code> spesifiserer «hypervisoren» som skal brukes. Den har samme format som en URL som inneholder et virtualiseringssystem (<code
                        class="literal">xen://</code>, <code
                        class="literal">qemu://</code>, <code
                        class="literal">lxc://</code>, <code
                        class="literal">openvz://</code>, <code
                        class="literal">vbox://</code>, og så videre), og den maskinen som skal være vert for VM (dette kan være tomt når det gjelder den lokale verten). I tillegg til det, og i QEMU/KVM tilfellet, kan hver bruker administrere virtuelle maskiner som arbeider med begrensede tillatelser, og URL-banen tillater å skille «system»-maskiner (<code
                        class="literal">/system</code>) fra andre (<code
                        class="literal">/session</code>).
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.type"><img
                          class="callout"
                          src="Common_Content/images/2.png"
                          alt="2" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							Siden KVM forvaltes på samme måte som QEMU, tillater <code
                        class="literal">--virt-type kvm</code> å spesifisere bruken av KVM selv om nettadressen ser ut som QEMU.
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.name"><img
                          class="callout"
                          src="Common_Content/images/3.png"
                          alt="3" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							Valget V<code
                        class="literal">--name</code> definerer et (unikt) navn for den virtuelle maskinen.
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.ram"><img
                          class="callout"
                          src="Common_Content/images/4.png"
                          alt="4" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							Valget <code
                        class="literal">--ram</code> kan spesifisere hvor mye RAM (i MB) som skal avsettes til den virtuelle maskinen.
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.disk"><img
                          class="callout"
                          src="Common_Content/images/5.png"
                          alt="5" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							<code
                        class="literal">--disk</code> angir plasseringen av bildefilen som skal representere harddisken til vår virtuelle maskinen; denne filen er laget, hvis den ikke allerede er til stede, med størrelsen (i GB) spesifisert av <code
                        class="literal">size</code>-parameteret. <code
                        class="literal">format</code>-parameteret gjør det mulig å velge mellom flere måter for lagring av bildefilen. Standardformatet (<code
                        class="literal">raw</code>) er en enkelt fil som samsvarer nøyaktig med diskens størrelse og innhold. Vi plukket ut et avansert format her, spesifikk for QEMU, og tillater starting med en liten fil som bare vokser når den virtuelle maskinen faktisk begynner å bruke plass.
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.cdrom"><img
                          class="callout"
                          src="Common_Content/images/6.png"
                          alt="6" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							<code
                        class="literal">--cdrom</code>-valget brukes til å indikere hvor en finner den optiske disken til bruk ved installasjon. Banen kan enten være en lokal bane for en ISO-fil, en URL der man kan få tak i filen, eller fra disk-filen i en fysisk CD-ROM-stasjon (dvs. <code
                        class="literal">/dev/cdrom</code>).
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.network"><img
                          class="callout"
                          src="Common_Content/images/7.png"
                          alt="7" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							<code
                        class="literal">--network</code> angir hvordan det virtuelle nettverkskortet integreres i vertens nettverksoppsett. Standard oppførsel (som vi eksplisitt håndhevet/tvang i vårt eksempel) er å integrere det inn i hvilken som helst foreliggende nettverksbro. Hvis en slik bro ikke finnes, vil den virtuelle maskinen kun nå det fysiske nettverket gjennom NAT, så det får en adresse i et privat delnettsområde (192.168.122.0/24).
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.vnc"><img
                          class="callout"
                          src="Common_Content/images/8.png"
                          alt="8" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							<code
                        class="literal">--vnc</code> sier at den grafiske konsollen skal gjøres tilgjengelig ved hjelp av VNC. Standard virkemåte for den tilknyttede VNC-tjeneren er å bare lytte til det lokale grensesnitt; hvis VNC-klienten skal kjøres på en annen vert, krever opprettelse av forbindelsen at det settes opp en SSH-tunnel (se <a
                        class="xref"
                        href="sect.remote-login.html#sect.ssh-port-forwarding">Seksjon 9.2.1.3, «Å lage krypterte tunneler med portvideresending (Port Forwarding)»</a>). Alternativt kan <code
                        class="literal">--vnclisten=0.0.0.0</code> anvendes slik at VNC-tjeneren er tilgjengelig fra alle grensesnitt. Vær oppmerksom på at hvis du gjør det, må du virkelig sette opp din brannmur tilsvarende .
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="left"><p><a
                        href="#virtinst.os"><img
                          class="callout"
                          src="Common_Content/images/9.png"
                          alt="9" /></a> </p></td><td
                    valign="top"
                    align="left"><div
                      class="para">
							<code
                        class="literal">--os-type</code> og <code
                        class="literal">--os-variant</code>-valgene kan optimalisere noen parametere for den virtuelle maskinen, basert på noen av de kjente funksjonene i operativsystemet nevnt der.
						</div></td></tr></table></div><div
              class="para">
					Nå kjører den virtuelle maskinen, og vi må koble til den grafiske konsollen for å fortsette med installasjonen. Hvis den forrige operasjonen ble kjørt fra et grafisk skrivebordsmiljø, bør denne forbindelsen startes automatisk. Hvis ikke, eller hvis vi operere eksternt, kan <code
                class="command">virt-viewer</code> kjøres fra et hvilket som helst grafisk miljø for å åpne den grafiske konsollen (merk at det spørres om rot-passordet til den eksterne verten to ganger, fordi operasjonen krever 2 SSH-forbindelser):
				</div><pre
              class="screen"><code
                class="computeroutput">$ </code><strong
                class="userinput"><code>virt-viewer --connect qemu+ssh://root@<em
                    class="replaceable">server</em>/system testkvm
</code></strong><code
                class="computeroutput">root@server's password: 
root@server's password: </code></pre><div
              class="para">
					Når installasjonsprosessen er ferdig, blir den virtuelle maskinen startet på nytt, nå klar til bruk.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      id="id-1.15.5.14.9"></a>12.2.3.4. Å håndtere maskiner med <code
                      class="command">virsh</code></h4></div></div></div><a
              id="id-1.15.5.14.9.2"
              class="indexterm"></a><div
              class="para">
					Nå som installasjonen er ferdig, la oss se hvordan man skal håndtere de tilgjengelige virtuelle maskinene. Det første du må prøve, er å spørre <code
                class="command">libvirtd</code> om listen over de virtuelle maskinene den forvalter:
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>virsh -c qemu:///system list --all
 Id Name                 State
----------------------------------
  - testkvm              shut off
</code></strong></pre><div
              class="para">
					La oss starte vår test av den virtuell maskinen:
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>virsh -c qemu:///system start testkvm
</code></strong><code
                class="computeroutput">Domain testkvm started</code></pre><div
              class="para">
					Vi kan nå få tilkoblingsinstruksjonene til det grafiske konsollet (den returnerte VNC-skjermen kan gis som parameter til <code
                class="command">vncviewer</code>):
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>virsh -c qemu:///system vncdisplay testkvm
</code></strong><code
                class="computeroutput">:0</code></pre><div
              class="para">
					Andre tilgjengelige underkommandoer inkluderer <code
                class="command">virsh</code>:
				</div><div
              xmlns:d="http://docbook.org/ns/docbook"
              class="itemizedlist"><ul><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">reboot</code> for å restarte en virtuell maskin;
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">shutdown</code> for å utløse en ren avslutning;
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">destroy</code>, for å stoppe den brutalt;
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">suspend</code> for å pause den;
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">resume</code> for å avslutte pause;
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">autostart</code> for å aktivere (eller deaktivere, med <code
                      class="literal">--disable</code>-valget) automatisk start av den virtuelle maskinen når verten starter;
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">undefine</code> for å fjerne alle spor etter den virtuelle maskinen fra <code
                      class="command">libvirtd</code>.
						</div></li></ul></div><div
              class="para">
					Alle disse underkommandoene tar en virtuell maskins identifikator som et parameter.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      id="id-1.15.5.14.10"></a>12.2.3.5. Å installere et RPM-basert system i Debian med yum</h4></div></div></div><div
              class="para">
					Hvis den virtuelle maskinen er ment til å kjøre en Debian (eller en av dens derivater), kan systemet bli initialisert med <code
                class="command">debootstrap</code>, som beskrevet ovenfor. Men hvis den virtuelle maskinen skal monteres med et RPM-basert system (som Fedora, CentOS eller Scientific Linux), vil oppsettet måtte gjøres med <code
                class="command">yum</code>-verktøyet (tilgjengelig i pakken med samme navn).
				</div><div
              class="para">
					Prosedyren krever bruk av <code
                class="command">rpm</code> for å pakke ut et innledende sett med filer, medregnet spesielt <code
                class="command">yum</code>-oppsettfiler, og så påkalle <code
                class="command">yum</code> for å pakke opp de gjenværende pakkesettene. Men siden vi kaller <code
                class="command">yum</code> fra utsiden av chrooten, trenger vi å gjøre noen midlertidige endringer. I eksemplet nedenfor, er mål-chrooten <code
                class="filename">/srv/centos</code>.
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>rootdir="/srv/centos"
</code></strong><code
                class="computeroutput"># </code><strong
                class="userinput"><code>mkdir -p "$rootdir" /etc/rpm
</code></strong><code
                class="computeroutput"># </code><strong
                class="userinput"><code>echo "%_dbpath /var/lib/rpm" &gt; /etc/rpm/macros.dbpath
</code></strong><code
                class="computeroutput"># </code><strong
                class="userinput"><code>wget http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm
</code></strong><code
                class="computeroutput"># </code><strong
                class="userinput"><code>rpm --nodeps --root "$rootdir" -i centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm
</code></strong><code
                class="computeroutput">rpm: RPM should not be used directly install RPM packages, use Alien instead!
rpm: However assuming you know what you are doing...
warning: centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY
# </code><strong
                class="userinput"><code>sed -i -e "s,gpgkey=file:///etc/,gpgkey=file://${rootdir}/etc/,g" $rootdir/etc/yum.repos.d/*.repo
</code></strong><code
                class="computeroutput"># </code><strong
                class="userinput"><code>yum --assumeyes --installroot $rootdir groupinstall core
</code></strong><code
                class="computeroutput">[...]
# </code><strong
                class="userinput"><code>sed -i -e "s,gpgkey=file://${rootdir}/etc/,gpgkey=file:///etc/,g" $rootdir/etc/yum.repos.d/*.repo
</code></strong></pre></div></div></div><ul
        class="docnav"><li
          class="previous"><a
            accesskey="p"
            href="advanced-administration.html"><strong>Forrige</strong>Kapittel 12. Avansert administrasjon</a></li><li
          class="up"><a
            accesskey="u"
            href="#"><strong>Opp</strong></a></li><li
          class="home"><a
            accesskey="h"
            href="index.html"><strong>Hjem</strong></a></li><li
          class="next"><a
            accesskey="n"
            href="sect.automated-installation.html"><strong>Neste</strong>12.3. Automatisert installasjon</a></li></ul><div
        id="translated_pages"><ul><li><a
              href="../ar-MA/sect.virtualization.html">ar-MA</a></li><li><a
              href="../da-DK/sect.virtualization.html">da-DK</a></li><li><a
              href="../de-DE/sect.virtualization.html">de-DE</a></li><li><a
              href="../el-GR/sect.virtualization.html">el-GR</a></li><li><a
              href="../en-US/sect.virtualization.html">en-US</a></li><li><a
              href="../es-ES/sect.virtualization.html">es-ES</a></li><li><a
              href="../fa-IR/sect.virtualization.html">fa-IR</a></li><li><a
              href="../fr-FR/sect.virtualization.html">fr-FR</a></li><li><a
              href="../hr-HR/sect.virtualization.html">hr-HR</a></li><li><a
              href="../id-ID/sect.virtualization.html">id-ID</a></li><li><a
              href="../it-IT/sect.virtualization.html">it-IT</a></li><li><a
              href="../ja-JP/sect.virtualization.html">ja-JP</a></li><li><a
              href="../ko-KR/sect.virtualization.html">ko-KR</a></li><li><a
              href="../nb-NO/sect.virtualization.html">nb-NO</a></li><li><a
              href="../pl-PL/sect.virtualization.html">pl-PL</a></li><li><a
              href="../pt-BR/sect.virtualization.html">pt-BR</a></li><li><a
              href="../ro-RO/sect.virtualization.html">ro-RO</a></li><li><a
              href="../ru-RU/sect.virtualization.html">ru-RU</a></li><li><a
              href="../tr-TR/sect.virtualization.html">tr-TR</a></li><li><a
              href="../vi-VN/sect.virtualization.html">vi-VN</a></li><li><a
              href="../zh-CN/sect.virtualization.html">zh-CN</a></li><li><a
              href="../zh-TW/sect.virtualization.html">zh-TW</a></li></ul></div></body></html>
