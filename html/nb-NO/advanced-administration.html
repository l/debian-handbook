<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html
    xmlns="http://www.w3.org/1999/xhtml"><head><meta
        http-equiv="Content-Type"
        content="text/html; charset=UTF-8" /><title
        xmlns:d="http://docbook.org/ns/docbook">Kapittel 12. Avansert administrasjon</title><link
        rel="stylesheet"
        type="text/css"
        href="Common_Content/css/default.css" /><link
        rel="stylesheet"
        media="print"
        href="Common_Content/css/print.css"
        type="text/css" /><meta
        xmlns:d="http://docbook.org/ns/docbook"
        name="generator"
        content="publican v4.3.2" /><meta
        xmlns:d="http://docbook.org/ns/docbook"
        name="package"
        content="Debian-debian-handbook-9-nb-NO-1.0-1" /><meta
        name="keywords"
        content="RAID, LVM, FAI, Forhåndsutfylling (preseed.cfg), Overvåking, virtualisering, Xen, LXC" /><link
        rel="home"
        href="index.html"
        title="Håndbok for Debian-administratoren" /><link
        rel="up"
        href="index.html"
        title="Håndbok for Debian-administratoren" /><link
        rel="prev"
        href="sect.rtc-services.html"
        title="11.8. Sanntids kommunikasjonstjenester" /><link
        rel="next"
        href="sect.virtualization.html"
        title="12.2. virtualisering" /><meta
        name="viewport"
        content="width=device-width, initial-scale=1" /><meta
        name="flattr:id"
        content="4pz9jq" /><link
        rel="canonical"
        href="http://l.github.io/debian-handbook/html/nb-NO/advanced-administration.html" /></head><body><noscript><iframe
          src="//www.googletagmanager.com/ns.html?id=GTM-5H35QX"
          height="0"
          width="0"
          style="display:none;visibility:hidden"></iframe></noscript><script
        type="text/javascript">(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&amp;l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-5H35QX');</script><div
        id="banner"><a
          href="../../"><span
            class="text">Download the ebook</span></a></div><p
        id="title"><a
          class="left"
          href="http://www.debian.org"><img
            alt="Product Site"
            src="Common_Content/images//image_left.png" /></a><a
          class="right"
          href="index.html"><img
            alt="Documentation Site"
            src="Common_Content/images//image_right.png" /></a></p><ul
        class="docnav top"><li
          class="previous"><a
            accesskey="p"
            href="sect.rtc-services.html"><strong>Forrige</strong></a></li><li
          class="home">Håndbok for Debian-administratoren</li><li
          class="next"><a
            accesskey="n"
            href="sect.virtualization.html"><strong>Neste</strong></a></li></ul><div
        xml:lang="nb-NO"
        class="chapter"
        lang="nb-NO"><div
          class="titlepage"><div><div><h1
                class="title"><a
                  id="advanced-administration"></a>Kapittel 12. Avansert administrasjon</h1></div></div></div><div
          class="toc"><dl
            class="toc"><dt><span
                class="section"><a
                  href="advanced-administration.html#sect.raid-and-lvm">12.1. RAID og LVM</a></span></dt><dd><dl><dt><span
                    class="section"><a
                      href="advanced-administration.html#sect.raid-soft">12.1.1. Programvare RAID</a></span></dt><dt><span
                    class="section"><a
                      href="advanced-administration.html#sect.lvm">12.1.2. LVM</a></span></dt><dt><span
                    class="section"><a
                      href="advanced-administration.html#sect.raid-or-lvm">12.1.3. RAID eller LVM?</a></span></dt></dl></dd><dt><span
                class="section"><a
                  href="sect.virtualization.html">12.2. virtualisering</a></span></dt><dd><dl><dt><span
                    class="section"><a
                      href="sect.virtualization.html#sect.xen">12.2.1. Xen</a></span></dt><dt><span
                    class="section"><a
                      href="sect.virtualization.html#sect.lxc">12.2.2. LXC</a></span></dt><dt><span
                    class="section"><a
                      href="sect.virtualization.html#id-1.15.5.14">12.2.3. Virtualisering med KVM</a></span></dt></dl></dd><dt><span
                class="section"><a
                  href="sect.automated-installation.html">12.3. Automatisert installasjon</a></span></dt><dd><dl><dt><span
                    class="section"><a
                      href="sect.automated-installation.html#sect.fai">12.3.1. Fully Automatic Installer (FAI)</a></span></dt><dt><span
                    class="section"><a
                      href="sect.automated-installation.html#sect.d-i-preseeding">12.3.2. Å forhåndsutfylle Debian-Installer
			
			</a></span></dt><dt><span
                    class="section"><a
                      href="sect.automated-installation.html#sect.simple-cdd">12.3.3. Simple-CDD: Alt i ett løsningen</a></span></dt></dl></dd><dt><span
                class="section"><a
                  href="sect.monitoring.html">12.4. Overvåking</a></span></dt><dd><dl><dt><span
                    class="section"><a
                      href="sect.monitoring.html#sect.munin">12.4.1. Å sette opp Munin</a></span></dt><dt><span
                    class="section"><a
                      href="sect.monitoring.html#sect.nagios">12.4.2. Å sette opp Nagios</a></span></dt></dl></dd></dl></div><div
          class="highlights"><div
            class="para">
		Dette kapittelet tar opp igjen noen aspekter vi allerede har beskrevet, med et annet perspektiv: I stedet for å installere en enkelt datamaskin, vil vi studere massedistribusjonssystemer; i stedet for å opprette RAID eller LVM ved nye installasjoner, vil vi lære å gjøre det for hånd, slik at vi senere kan endre våre første valg. Til slutt vil vi diskutere overvåkingsverktøy og virtualiseringsteknikker. Som en konsekvens, er dette kapittelet mer spesielt rettet mot profesjonelle administratorer, og fokuserer litt mindre på personer med ansvar for sine hjemmenettverk .
	</div></div><div
          class="section"><div
            class="titlepage"><div><div><h2
                  class="title"><a
                    id="sect.raid-and-lvm"></a>12.1. RAID og LVM</h2></div></div></div><div
            class="para">
			<a
              class="xref"
              href="installation.html">Kapittel 4, <em>Installasjon</em></a> har vist disse teknologiene fra installeringssynspunkt, og hvordan de kan integreres til å gjøre utplasseringen lett fra start. Etter den første installasjonen, må en administrator kunne håndtere utvikling av lagringsplassbehov uten å måtte ty til en kostbar reinstallasjon. De må derfor forstå de nødvendige verktøy for å håndtere RAID- og LVM-volumer.
		</div><div
            class="para">
			RAID og LVM er begge teknikker til å trekke ut de monterte volumene fra sine fysiske motstykker (faktiske harddisker eller partisjoner); den første sikrer data mot maskinvarefeil ved å innføre redundans, sistnevnte gjør volumadministrasjon mer fleksibel og uavhengig av den faktiske størrelsen på de underliggende disker. I begge tilfeller ender systemet opp med nye blokk-enheter, som kan brukes til å lage filsystemer eller vekselfiler, uten nødvendigvis å ha dem tilordnet til en fysisk disk. RAID og LVM kommer fra helt forskjellige bakgrunner, men funksjonaliteten kan overlappe noe, de er derfor ofte nevnt sammen.
		</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>PERSPEKTIV</em></span> Btrfs kombinerer LVM og RAID</strong></p></div></div></div><div
              class="para">
			Mens LVM og RAID er to forskjellige kjerne-delsystemer som ligger mellom disk blokk-enheter og filsystemene deres, <span
                class="emphasis"><em>btrfs</em></span> er et nytt filsystem, opprinnelig utviklet i Oracle, som skal kombinere kjennetegnsettene til LVM og RAID, og mye mer. Det er for det meste funksjonelt, og selv om det fremdeles er merket «eksperimentell» fordi utviklingsstadiet er ufullstendig (noen funksjoner er ikke implementert ennå), er det allerede sett i bruk i produksjonsmiljøer. <div
                class="url">→ <a
                  href="http://btrfs.wiki.kernel.org/">http://btrfs.wiki.kernel.org/</a></div>
		</div><div
              class="para">
			Blant funksjonene verdt å legge merke til, er muligheten til på ethvert tidspunkt å ta et øyeblikksbilde av et filsystemtre. Denne øyeblikksbildekopien vil i utgangspunktet ikke bruke diskplass, data blir bare duplisert når en av kopiene blir endret. Filsystemet håndterer også gjennomsiktig komprimering av filer, og kontrollsummer sikrer integriteten til alle lagrede data.
		</div></div><div
            class="para">
			Både for RAID og LVM gir kjernen en blokk-spesialfil, lik dem som svarer til en harddisk eller en partisjon. Når et program, eller en annen del av kjernen, krever tilgang til en blokk innrettet slik, ruter det aktuelle subsystem blokken til det aktuelle fysiske laget. Avhengig av konfigurasjonen, kan denne blokken lagres på en eller flere fysiske disker, og den fysiske plasseringen kan ikke være direkte korrelert til plassering av blokken i den logiske enheten.
		</div><div
            class="section"><div
              class="titlepage"><div><div><h3
                    class="title"><a
                      id="sect.raid-soft"></a>12.1.1. Programvare RAID</h3></div></div></div><a
              id="id-1.15.4.6.2"
              class="indexterm"></a><div
              class="para">
				RAID betyr <span
                class="emphasis"><em>Redundant Array of Independent Disks</em></span>. Målet med dette systemet er å hindre tap av data i tilfelle feil på harddisken. Det generelle prinsippet er ganske enkelt: Data er lagret på flere fysiske disker i stedet for bare på én, med en konfigurerbar grad av redundans. Avhengig av denne redundansstørrelsen, og til og med i tilfelle av en uventet diskfeil, kan data uten tap bli rekonstruert fra de gjenværende disker.
			</div><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>KULTUR</em></span> <span
                          class="foreignphrase"><em
                            class="foreignphrase">Uavhengig</em></span> eller <span
                          class="foreignphrase"><em
                            class="foreignphrase">billig</em></span>?</strong></p></div></div></div><div
                class="para">
				I-en i RAID sto opprinnelig for <span
                  class="emphasis"><em>inexpensive</em></span> (billig), fordi RAID tillot en drastisk økning i datasikkerhet uten å kreve investering i dyre high-end disker. Sannsynligvis på grunn av bildehensyn, er det imidlertid nå en mer vanlig vurdering å stå for <span
                  class="emphasis"><em>independent</em></span> (uavhengig), som ikke har den uønskede smaken av å være billig.
			</div></div><div
              class="para">
				RAID kan implementeres enten ved øremerket maskinvare (RAID-moduler integrert i SCSI eller SATA-kontrollerkort), eller ved bruk av programvare-sammendrag (kjernen). Enten maskinvare eller programvare, et RAID-system kan, med nok reserve, transparent fortsette operativt når en disk svikter; de øvre lag av stabelen (applikasjoner) kan også beholde tilgangen til data tross feilen. Denne «degradert modus» kan selvfølgelig ha en innvirkning på ytelsen, og reservekapasiteten er redusert, slik at en ytterligere diskfeil kan føre til tap av data. I praksis vil en derfor bestrebe seg på å bli værende med denne redusert driften bare så lenge som det tar å erstatte den ødelagte disken. Så snart den nye disken er på plass, kan RAID-systemet rekonstruere de nødvendige data, og gå tilbake til en sikker modus. Programmene vil ikke merke noe, bortsett fra en potensielt redusert tilgangshastighet, mens området er i redusert drift, eller under rekonstruksjonsfasen.
			</div><div
              class="para">
				Når RAID implementeres av maskinvare, skjer oppsettet vanligvis innen BIOS konfigurasjonsverktøy, og kjernen vil vurdere en RAID-tabell som en enkelt disk, som vil virke som en standard fysisk disk, selv om navnet på enheten kan være forskjellige (avhengig av driveren).
			</div><div
              class="para">
				Vi fokuserer bare på programvare RAID i denne boken.
			</div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        id="sect.raid-levels"></a>12.1.1.1. Ulike RAID-nivåer</h4></div></div></div><div
                class="para">
					RAID er faktisk ikke et enkelt system, men et spekter av systemer som identifiseres av sine nivåer. Nivåene skiller seg ved sin utforming og mengden av reserve (redundans) de gir. Jo større overflødighet, jo sikrere mot feil, siden systemet vil være i stand til å fortsette arbeidet med flere disker som feiler. Motstykket er at plassen som kan brukes, krymper for et gitt sett med disker, eller med andre ord; flere disker vil være nødvendig for å lagre en gitt mengde data.
				</div><div
                class="variablelist"><dl
                  class="variablelist"><dt><span
                      class="term">Lineær RAID</span></dt><dd><div
                      class="para">
								Selv om kjernens RAID-delsystem kan lage «lineær RAID», er dette egentlig ikke en ekte RAID, siden dette oppsettet ikke gir overskudd. Kjernen samler bare flere disker etter hverandre, og resulterer i et samlet volum som e n virtuell disk (en blokkenhet). Det er omtrent dens eneste funksjon. Dette oppsettet brukes sjelden i seg selv (se senere for unntak), spesielt siden mangelen på overskudd betyr at om en disk svikter, aggregerer det, og gjør alle data utilgjengelige.
							</div></dd><dt><span
                      class="term">RAID-0</span></dt><dd><div
                      class="para">
								Dette nivået gir intet overskudd heller, men diskene blir ikke ganske enkelt fastlåst etter hverandre: De blir delt i <span
                        class="emphasis"><em>striper</em></span>, og blokkene på den virtuelle enheten er lagret på striper på alternerende fysiske disker. I et to-disk RAID-0 oppsett, for eksempel, vil partalls blokker på den virtuelle enheten bli lagret på den første fysiske disken, mens oddetalls blokker vil komme på den andre fysiske disken.
							</div><div
                      class="para">
								Dette systemet har ikke som mål å øke pålitelighet, siden (som i det lineære tilfellet) tilgjengeligheten til alle data er i fare så snart en disk svikter, men å øke ytelsen: Under sekvensiell tilgang til store mengder sammenhengende data, vil kjernen være i stand til å lese fra begge disker (eller skrive til dem) i parallell, noe som øker hastigheten på dataoverføringen. Imidlertid krymper RAID-0-bruken når nisjen dens fylles med LVM (se senere).
							</div></dd><dt><span
                      class="term">RAID-1</span></dt><dd><div
                      class="para">
								Dette nivået, også kjent som "RAID-speiling", er både det enkleste og det mest brukte oppsettet. I standardformen bruker den to fysiske disker av samme størrelse, og gir et logisk volum av samme størrelse på nytt. Data er lagret identisk på begge disker, derav kallenavnet «speile». Når en disk svikter, finnes dataene fremdeles på den andre. For virkelig kritiske data, kan RAID-1 selvsagt settes opp på mer enn to disker, med direkte konsekvenser for forholdet mellom maskinvarekostnader opp mot tilgjengelig plass for nyttelast.
							</div><div
                      class="sidebar"><div
                        class="titlepage"><div><div><p
                              class="title"><strong><span
                                  class="emphasis"><em>NOTE</em></span> Disk- og klyngestørrelser</strong></p></div></div></div><div
                        class="para">
								Hvis to disker av forskjellige størrelse er satt opp i et speil, vil ikke den største bli brukt fullt ut, siden den vil inneholde de samme dataene som den minste og ingenting mer. Denne nyttige tilgjengelige plassen levert av et RAID-1-volum passer derfor til størrelsen på den minste disken i rekken. Dette gjelder likevel for RAID-volumer med høyere RAID-nivå, selv om overskudd er lagret på en annen måte.
							</div><div
                        class="para">
								Det er derfor viktig når du setter opp RAID-matriser (unntatt for RAID-0 og «lineær RAID»), å bare montere disker av identiske eller svært nære størrelser, for å unngå å sløse med ressurser.
							</div></div><div
                      class="sidebar"><div
                        class="titlepage"><div><div><p
                              class="title"><strong><span
                                  class="emphasis"><em>NOTE</em></span> Reservedisker</strong></p></div></div></div><div
                        class="para">
								RAID-nivåer som inkluderer overskudd tillater tilordning av flere disker enn det som kreves til en matrise. De ekstra diskene blir brukt som reservedeler når en av de viktigste diskene svikter. For eksempel, i et speilbilde av to plater pluss en i reserve; dersom en av de to første diskene svikter, vil kjernen automatisk (og umiddelbart) rekonstruere speilet ved hjelp av en ekstra disk, slik at overskuddet forblir sikret etter gjenoppbyggingstidspunktet. Dette kan brukes som en annen form for ekstra sikkerhet for kritiske data.
							</div><div
                        class="para">
								En ville bli tilgitt hvis man undrer seg over hvordan dette er bedre enn bare speiling på tre disker til å begynne med. Fordelen med «ledig disk»-oppsettet er at en ekstra disk kan deles på tvers av flere RAID-volumer. For eksempel kan man ha tre speilende volumer, med overskudd sikret selv i tilfelle av disksvikt med bare syv disker (tre par, pluss en felles i reserve), i stedet for de ni diskene som ville være nødvendig med tre trillinger.
							</div></div><div
                      class="para">
								Dette RAID-nivået, selv om det er dyrere (da bare halvparten av de fysiske lagringsplass, i beste fall, er i bruk), er mye brukt i praksis. Det er enkelt å forstå, og det gir svært enkle sikkerhetskopier: Siden begge diskene har identisk innhold, kan en av dem bli midlertidig ekstrahert uten noen innvirkning på systemet ellers. Leseytelsen er ofte økt siden kjernen kan lese halvparten av dataene på hver disk parallelt, mens skriveytelsen ikke er altfor alvorlig svekket. I tilfelle med en RAID-matrise med N disker, forblir data tilgjengelig selv med N-1 diskfeil.
							</div></dd><dt><span
                      class="term">RAID-4</span></dt><dd><div
                      class="para">
								Dette RAID-nivået, ikke mye brukt, bruker N plater til å lagre nyttige data, og en ekstra disk til å lagre overskuddsinformasjon. Hvis den disken svikter, kan systemet rekonstruere innholdet fra den andre N-en. Hvis en av de N datadiskene svikter, inneholder den gjenværende N-1 kombinert med «paritets»-disken nok informasjon til å rekonstruere de nødvendige dataene.
							</div><div
                      class="para">
								RAID-4 er ikke for dyrt siden det bare omfatter en one-in-N økning i kostnader, og har ingen merkbar innvirkning på leseytelsen, men skriving går langsommere. Videre, siden et skript til hvilket som helst av N platene også omfatter et skript til paritetsdisken, ser den sistnevnte mange flere skriveoperasjoner enn den første, og dens levetid kan forkortes dramatisk som et konsekvens. Data på en RAID-4-matrise er bare trygg opp til en feilende disk (av de N + 1).
							</div></dd><dt><span
                      class="term">RAID-5</span></dt><dd><div
                      class="para">
								RAID-5 løser asymmetrispørsmålet til RAID-4: Paritetsblokker er spredt over alle N + 1 disker, uten at en enkeltdisk har en bestemt rolle.
							</div><div
                      class="para">
								Lese- og skrivehastighet er identiske til RAID-4. Her igjen forblir systemet funksjonelt med opp til en disk som feiler (av de N+1), men ikke flere.
							</div></dd><dt><span
                      class="term">RAID-6</span></dt><dd><div
                      class="para">
								RAID-6 kan betraktes som en forlengelse av RAID-5, der hver serie med N blokker involverer to reserveblokker, og hver slik serie med N+2 blokker er spredt over N+2 disker.
							</div><div
                      class="para">
								Dette RAID-nivået er litt dyrere enn de to foregående, men det bringer litt ekstra sikkerhet siden opptil to stasjoner (i N+2) kan svikte uten at det går ut over datatilgjengeligheten. Motstykket er at skriveoperasjoner nå innebærer å skrive ut på en datablokk og to reserveblokker, noe som gjør dem enda tregere.
							</div></dd><dt><span
                      class="term">RAID-1+0</span></dt><dd><div
                      class="para">
								Dette er ikke strengt tatt et RAID-nivå, men en samling av to RAID-grupperinger. Med start fra 2×N disker, setter man dem først opp i parene i N RAID-1-volumer; Disse N volumene blir så samlet til ett, enten ved «lineær RAID», eller (i økende grad) av LVM. Dette siste tilfellet går lenger enn ren RAID, men det er ikke noe problem med det.
							</div><div
                      class="para">
								RAID-1+0 kan overleve flere diskfeil: opp til N i 2xN matrisen som er beskrevet ovenfor, forutsatt at minst en disk fortsetter å virke i hver av RAID-1-parene.
							</div><div
                      class="sidebar"><a
                        id="sidebar.raid-10"></a><div
                        class="titlepage"><div><div><p
                              class="title"><strong><span
                                  class="emphasis"><em>FOR VIDEREKOMMENDE</em></span> RAID-10</strong></p></div></div></div><div
                        class="para">
								RAID-10 er generelt ansett som synonym for RAID-1+0, men en spesiell funksjon (en særegenhet/spesifisitet) i Linux gjør det faktisk til en generalisering. Dette oppsettet gjør det mulig for et system, der hver blokk er lagret på to forskjellige disker, selv med et oddetall disker, at kopiene blir spredt ut i en konfigurerbar modell.
							</div><div
                        class="para">
								Yteevnen vil variere avhengig av valgt repartisjonsmodell og reservenivå, og av arbeidsmengden til det det logiske volumet.
							</div></div></dd></dl></div><div
                class="para">
					Selvfølgelig vil RAID-nivået velges ut fra begrensningene og kravene til hvert program. Merk at en enkelt datamaskin kan ha flere forskjellige RAID-matriser med forskjellige konfigurasjoner.
				</div></div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        id="sect.raid-setup"></a>12.1.1.2. Å sette opp RAID</h4></div></div></div><a
                id="id-1.15.4.6.9.2"
                class="indexterm"></a><div
                class="para">
					Å sette opp RAID-volumer krever <span
                  class="pkg pkg">mdadm</span>-pakken; den leverer <code
                  class="command">mdadm</code>-kommandoen, som gjør det mulig å lage og håndtere RAID-tabeller, samt prosedyrer og verktøy som integrerer den i resten av systemet, inkludert overvåkningssystemet.
				</div><div
                class="para">
					Vårt eksempel vil være en tjener med en rekke disker, der noen er allerede brukt, og resten blir tilgjengelig til å sette opp RAID. Vi har i utgangspunktet følgende disker og partisjoner:
				</div><div
                xmlns:d="http://docbook.org/ns/docbook"
                class="itemizedlist"><ul><li
                    class="listitem"><div
                      class="para">
							<code
                        class="filename">sdb</code>-disken, 4 GB, er fullt tilgjengelig;
						</div></li><li
                    class="listitem"><div
                      class="para">
							<code
                        class="filename">sdc</code>-disken, 4 GB, er også fullt ut tilgjengelig;
						</div></li><li
                    class="listitem"><div
                      class="para">
							På <code
                        class="filename">sdd</code>-disken, er bare partisjonen <code
                        class="filename">sdd2</code> (rundt 4 GB) tilgjengelig;
						</div></li><li
                    class="listitem"><div
                      class="para">
							til slutt er en <code
                        class="filename">sde</code>-disk, også på 4 GB, fullt ut tilgjengelig.
						</div></li></ul></div><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>NOTE</em></span> Identifisere eksisterende RAID-volumer</strong></p></div></div></div><div
                  class="para">
					Filen <code
                    class="filename">/proc/mdstat</code> lister eksisterende volumer og tilstanden deres. Når du oppretter et nytt RAID-volum, bør man være forsiktig for å ikke gi det samme navnet som på et eksisterende volum.
				</div></div><div
                class="para">
					Vi kommer til å bruke disse fysiske elementer for å bygge to volumer, en RAID-0 og ett speil (RAID-1). La oss starte med RAID-0-volumet:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm --create /dev/md0 --level=0 --raid-devices=2 /dev/sdb /dev/sdc</code></strong>
<code
                  class="computeroutput">mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.
# </code><strong
                  class="userinput"><code>mdadm --query /dev/md0</code></strong>
<code
                  class="computeroutput">/dev/md0: 8.00GiB raid0 2 devices, 0 spares. Use mdadm --detail for more detail.
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md0</code></strong>
<code
                  class="computeroutput">/dev/md0:
        Version : 1.2
  Creation Time : Wed May  6 09:24:34 2015
     Raid Level : raid0
     Array Size : 8387584 (8.00 GiB 8.59 GB)
   Raid Devices : 2
  Total Devices : 2
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:24:34 2015
          State : clean 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0

     Chunk Size : 512K

           Name : mirwiz:0  (local to host mirwiz)
           UUID : bb085b35:28e821bd:20d697c9:650152bb
         Events : 0

    Number   Major   Minor   RaidDevice State
       0       8       16        0      active sync   /dev/sdb
       1       8       32        1      active sync   /dev/sdc
# </code><strong
                  class="userinput"><code>mkfs.ext4 /dev/md0</code></strong>
<code
                  class="computeroutput">mke2fs 1.42.12 (29-Aug-2014)
Creating filesystem with 2095104 4k blocks and 524288 inodes
Filesystem UUID: fff08295-bede-41a9-9c6a-8c7580e520a6
Superblock backups stored on blocks: 
        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done 
# </code><strong
                  class="userinput"><code>mkdir /srv/raid-0</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mount /dev/md0 /srv/raid-0</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>df -h /srv/raid-0</code></strong>
<code
                  class="computeroutput">Filesystem      Size  Used Avail Use% Mounted on
/dev/md0        7.9G   18M  7.4G   1% /srv/raid-0
</code></pre><div
                class="para">
					Kommandoen <code
                  class="command">mdadm --create</code> krever flere parametre: Navnet på volumet som skal lages (<code
                  class="filename">/dev/md*</code>, der MD står for <span
                  class="foreignphrase"><em
                    class="foreignphrase">Multiple Device</em></span>), RAID-nivået, antall disker (som er obligatorisk til tross for at det er mest meningsfullt bare med RAID-1 og over), og de fysiske enhetene som skal brukes. Når enheten er opprettet, kan vi bruke den som vi ville bruke en vanlig partisjon, opprette et filsystem på den, montere dette filsystemet, og så videre. Vær oppmerksom på at vår etablering av et RAID-0-volum på <code
                  class="filename">md0</code> ikke er et sammentreff, og nummereringen av tabellen ikke trenger å være korrelert til den valgte størrelsen på reservekapasiteten. Det er også mulig å lage navngitte RAID-arrays, ved å gi <code
                  class="command">mdadm</code> parametre slik som <code
                  class="filename">/dev/md/linear</code> i stedet for <code
                  class="filename">/dev/md0</code>.
				</div><div
                class="para">
					Opprettelse av en RAID-1 følger opp en lignende måte, forskjellene blir bare merkbare etter etableringen:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm --create /dev/md1 --level=1 --raid-devices=2 /dev/sdd2 /dev/sde</code></strong>
<code
                  class="computeroutput">mdadm: Note: this array has metadata at the start and
    may not be suitable as a boot device.  If you plan to
    store '/boot' on this device please ensure that
    your boot-loader understands md/v1.x metadata, or use
    --metadata=0.90
mdadm: largest drive (/dev/sdd2) exceeds size (4192192K) by more than 1%
Continue creating array? </code><strong
                  class="userinput"><code>y</code></strong>
<code
                  class="computeroutput">mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md1 started.
# </code><strong
                  class="userinput"><code>mdadm --query /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1: 4.00GiB raid1 2 devices, 0 spares. Use mdadm --detail for more detail.
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
        Version : 1.2
  Creation Time : Wed May  6 09:30:19 2015
     Raid Level : raid1
     Array Size : 4192192 (4.00 GiB 4.29 GB)
  Used Dev Size : 4192192 (4.00 GiB 4.29 GB)
   Raid Devices : 2
  Total Devices : 2
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:30:40 2015
          State : clean, resyncing (PENDING) 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 0
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 0

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       1       8       64        1      active sync   /dev/sde
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
          State : clean
[...]
</code></pre><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>TIPS</em></span> RAID, disker og partisjoner</strong></p></div></div></div><div
                  class="para">
					Som illustrert ved vårt eksempel, kan RAID-enheter bygges fra diskpartisjoner, og krever ikke hele disker.
				</div></div><div
                class="para">
					Noen få merknader er på sin plass. Først, <code
                  class="command">mdadm</code> merker at de fysiske elementene har forskjellige størrelser; siden dette innebærer at noe plass går tapt på de større elementene, kreves en bekreftelse.
				</div><div
                class="para">
					Enda viktigere er det å merke tilstanden til speilet. Normal tilstand for et RAID-speil er at begge diskene har nøyaktig samme innhold. Men ingenting garanterer at dette er tilfelle når volumet blir opprettet. RAID-subsystem vil derfor gi denne garantien selv, og det vil være en synkroniseringsfase så snart RAID-enheten er opprettet. Etter en tid (den nøyaktige tiden vil avhenge av den faktiske størrelsen på diskene ...), skifter RAID-tabellen til «aktiv» eller «ren» tilstand. Legg merke til at i løpet av denne gjenoppbyggingsfasen, er speilet i en degradert modus, og reservekapasitet er ikke sikret. En disk som svikter på dette trinnet kan føre til at alle data mistes. Store mengder av viktige data er imidlertid sjelden lagret på en nyopprettet RAID før den første synkroniseringen. Legg merke til at selv i degradert modus, vil <code
                  class="filename">/dev/md1</code> kunne brukes, og et filsystem kan opprettes på den, så vel som noe data kopieres på den.
				</div><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>TIPS</em></span> Å starte et speil i degradert modus</strong></p></div></div></div><div
                  class="para">
					Noen ganger er to disker ikke umiddelbart tilgjengelig når man ønsker å starte et RAID-1-speil, for eksempel fordi en av diskene en planlegger å inkludere, allerede er brukt til å lagre dataene man ønsker å flytte til matrisen. I slike tilfeller er det mulig å bevisst skape en degradert RAID-1-tabell ved å sende <code
                    class="filename">missing</code> i stedet for en enhetsfil som ett av argumentene til <code
                    class="command">mdadm</code>. Når dataene er blitt kopiert til «speilet», kan den gamle disken legges til matrisen. Så vil det finne sted en synkronisering, noe som gir oss den reservekapasitet som var ønsket i første omgang.
				</div></div><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>TIPS</em></span> Å sette opp et speil uten synkronisering</strong></p></div></div></div><div
                  class="para">
					RAID-1-volumer er ofte laget for å bli brukt som en ny disk, ofte betraktet som blank. Det faktiske innledende innholdet på disken er dermed ikke så relevant, siden man bare trenger å vite at dataene som er skrevet etter etableringen av volumet, spesielt filsystemet, kan nås senere.
				</div><div
                  class="para">
					Man kan derfor lure på om poenget med å synkronisere begge diskene ved tidpunktet for opprettelsen er god. Hvorfor bry seg om innholdet er identisk på soner i volumet som vi vet kun vil leses etter at vi har skrevet til dem?
				</div><div
                  class="para">
					Heldigvis kan denne synkroniseringfasen unngås ved å gå forbi <code
                    class="literal">--assume-clean</code>-valget til <code
                    class="command">mdadm</code>. Imidlertid kan dette alternativet føre til overraskelser i tilfeller hvor de første dataene vil bli lest (for eksempel hvis et filsystem allerede er der på de fysiske diskene), som er grunnen til at den ikke er aktivert som standard.
				</div></div><div
                class="para">
					La oss nå se hva som skjer når et av elementene i RAID-1-tabellen svikter. <code
                  class="command">mdadm</code>, spesielt <code
                  class="literal">--fail</code>-valget tillater å simulere en slik diskfeiling:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm /dev/md1 --fail /dev/sde</code></strong>
<code
                  class="computeroutput">mdadm: set /dev/sde faulty in /dev/md1
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
    Update Time : Wed May  6 09:39:39 2015
          State : clean, degraded 
 Active Devices : 1
Working Devices : 1
 Failed Devices : 1
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 19

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       0        0        2      removed

       1       8       64        -      faulty   /dev/sde</code></pre><div
                class="para">
					Innholdet i volumet er fortsatt tilgjengelig (og, hvis det er montert, legger ikke programmene merke til en ting), men datasikkerheten er ikke trygg lenger: Skulle <code
                  class="filename">sdd</code>-disken i sin tur svikte, vil dataene gå tapt. Vi ønsker å unngå denne risikoen, så vi erstatter den ødelagte disken med en ny en, <code
                  class="filename">sdf</code>:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm /dev/md1 --add /dev/sdf</code></strong>
<code
                  class="computeroutput">mdadm: added /dev/sdf
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
   Raid Devices : 2
  Total Devices : 3
    Persistence : Superblock is persistent

    Update Time : Wed May  6 09:48:49 2015
          State : clean, degraded, recovering 
 Active Devices : 1
Working Devices : 2
 Failed Devices : 1
  Spare Devices : 1

 Rebuild Status : 28% complete

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 26

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      spare rebuilding   /dev/sdf

       1       8       64        -      faulty   /dev/sde
# </code><strong
                  class="userinput"><code>[...]</code></strong>
<code
                  class="computeroutput">[...]
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
    Update Time : Wed May  6 09:49:08 2015
          State : clean 
 Active Devices : 2
Working Devices : 2
 Failed Devices : 1
  Spare Devices : 0

           Name : mirwiz:1  (local to host mirwiz)
           UUID : 6ec558ca:0c2c04a0:19bca283:95f67464
         Events : 41

    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      active sync   /dev/sdf

       1       8       64        -      faulty   /dev/sde</code></pre><div
                class="para">
					Her igjen utløser kjernen automatisk en rekonstruksjonsfase der volumet fortsatt er tilgjengelig, men i en degradert modus. Når gjenoppbyggingen er over, er RAID-matrisen tilbake i normal tilstand. Man kan da si til systemet at <code
                  class="filename">sde</code>-disken er i ferd med å bli fjernet fra matrisen, for å ende opp med et klassisk RAID-speil på to disker:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm /dev/md1 --remove /dev/sde</code></strong>
<code
                  class="computeroutput">mdadm: hot removed /dev/sde from /dev/md1
# </code><strong
                  class="userinput"><code>mdadm --detail /dev/md1</code></strong>
<code
                  class="computeroutput">/dev/md1:
[...]
    Number   Major   Minor   RaidDevice State
       0       8       50        0      active sync   /dev/sdd2
       2       8       80        1      active sync   /dev/sdf</code></pre><div
                class="para">
					Fra da av kan driveren fysisk fjernes når tjeneren nærmest er slått av, eller til og med hot-fjernes når maskinvareoppsettet tillater det. Slike konfigurasjoner inkluderer noen SCSI-kontrollere, de fleste SATA-disker, og eksterne harddisker som opererer via USB eller Firewire.
				</div></div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        id="sect.backup-raid-config"></a>12.1.1.3. Sikkerhetskopi av konfigureringen</h4></div></div></div><div
                class="para">
					De fleste av meta-dataene som gjelder gjelder RAID-volumer lagres direkte på diskene til disse matrisene, slik at kjernen kan oppdage matriser og komponentene deres, og montere dem automatisk når systemet starter opp. Men det oppmuntres til sikkerhetskopiering av denne konfigurasjonen, fordi denne deteksjonen ikke er feilfri, og det er bare å forvente at den vil svikte akkurat under sensitive omstendigheter. I vårt eksempel, hvis en svikt i <code
                  class="filename">sde</code>-disken hadde vært virkelig (i stedet for simulert), og systemet har blitt startet på nytt uten å fjerne denne <code
                  class="filename">sde</code>-disken, kunne denne disken begynne å jobbe igjen etter å ha blitt undersøkt under omstarten. Kjernen vil da ha tre fysiske elementer, som hver utgir seg for å inneholde halvparten av det samme RAID-volumet. En annen kilde til forvirring kan komme når RAID-volumer fra to tjenere er konsolidert inn i bare en tjener. Hvis disse matrisene kjørte normalt før diskene ble flyttet, ville kjernen være i stand til å oppdage og montere parene riktig; men hvis de flyttede diskene hadde blitt samlet i en <code
                  class="filename">md1</code> på den gamle tjeneren, og den nye tjeneren allerede har en <code
                  class="filename">md1</code>, ville et av speilene få nytt navn.
				</div><div
                class="para">
					Å sikkerhetskopiere konfigurasjonen er derfor viktig, om bare som referanse. Den vanlige måten å gjøre det på er å redigere <code
                  class="filename">/etc/mdadm/mdadm.conf</code>-filen, et eksempel på det er listet her:
				</div><div
                class="example"><a
                  id="example.mdadm-conf"></a><p
                  class="title"><strong>Eksempel 12.1. <code
                      class="command">mdadm</code>-konfigurasjonsfil</strong></p><div
                  class="example-contents"><pre
                    class="programlisting"># mdadm.conf
#
# Please refer to mdadm.conf(5) for information about this file.
#

# by default (built-in), scan all partitions (/proc/partitions) and all
# containers for MD superblocks. alternatively, specify devices to scan, using
# wildcards if desired.
DEVICE /dev/sd*

# auto-create devices with Debian standard permissions
CREATE owner=root group=disk mode=0660 auto=yes

# automatically tag new arrays as belonging to the local system
HOMEHOST &lt;system&gt;

# instruct the monitoring daemon where to send mail alerts
MAILADDR root

# definitions of existing MD arrays
ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb
ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464

# This configuration was auto-generated on Thu, 17 Jan 2013 16:21:01 +0100
# by mkconf 3.2.5-3
</pre></div></div><div
                class="para">
					En av de mest nyttige detaljer er <code
                  class="literal">DEVICE</code>-valget, som viser enhetene som systemet automatisk vil se etter deler av RAID-volumer ved oppstartstidspunktet. I vårt eksempel erstattet vi standardverdien, <code
                  class="literal">partitions containers</code>, med en eksplisitt liste over enhetsfiler, siden vi valgte å bruke hele disker, og ikke bare partisjoner for noen volumer.
				</div><div
                class="para">
					De to siste linjene i vårt eksempel er de som tillater kjernen trygt å velge hvilke volumnummer som skal tilordnes hvilken matrise. Metadataene som er lagret på selve diskene er nok til å sette volumene sammen igjen, men ikke for å bestemme volumnummeret (og det matchende <code
                  class="filename">/dev/md*</code>-enhetsnavn).
				</div><div
                class="para">
					Heldigvis kan disse linjene generes automatisk:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mdadm --misc --detail --brief /dev/md?</code></strong>
<code
                  class="computeroutput">ARRAY /dev/md0 metadata=1.2 name=mirwiz:0 UUID=bb085b35:28e821bd:20d697c9:650152bb
ARRAY /dev/md1 metadata=1.2 name=mirwiz:1 UUID=6ec558ca:0c2c04a0:19bca283:95f67464</code></pre><div
                class="para">
					Innholdet i disse to siste linjene er ikke avhengig av listen over disker som inngår i volumet. Det er derfor ikke nødvendig å regenerere disse linjene når du skifter ut en feilet disk med en ny. På den annen side må man sørge for å oppdatere filen når du oppretter eller sletter et RAID-oppsett.
				</div></div></div><div
            class="section"><div
              class="titlepage"><div><div><h3
                    class="title"><a
                      id="sect.lvm"></a>12.1.2. LVM</h3></div></div></div><a
              id="id-1.15.4.7.2"
              class="indexterm"></a><a
              id="id-1.15.4.7.3"
              class="indexterm"></a><div
              class="para">
				LVM, <span
                class="emphasis"><em>Logical Volume Manager</em></span> ( logisk volumhåndtering), er en annen tilnærming for å abstrahere logiske volumer fra sin fysiske forankring, som fokuserer på økt fleksibilitet i stedet for økt pålitelighet. LVM lar deg endre et logisk volum transparent så langt programmene angår; for eksempel er det mulig å legge til nye disker, overføre dataene til dem, og fjerne gamle disker, uten at volumet demonteres.
			</div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        id="sect.lvm-concepts"></a>12.1.2.1. LVM-konsepter</h4></div></div></div><div
                class="para">
					Denne fleksibilitet oppnås med et abstraksjonsnivå som involverer tre konsepter.
				</div><div
                class="para">
					Først, PV (<span
                  class="emphasis"><em>Physical Volume</em></span>, fysisk volum) er enheten nærmest maskinvaren: Det kan være partisjoner på en disk, en full disk, eller til og med en annen blokkenhet (inkludert, for eksempel, en RAID-matrise). Merk at når et fysisk element er satt opp til å bli en PV for LVM, skal den kun være tilgjengelige via LVM, ellers vil systemet bli forvirret.
				</div><div
                class="para">
					Et antall PV-er kan samles i en VG (<span
                  class="emphasis"><em>Volume Group</em></span>, volumgruppe), som kan sammenlignes med både virtuelle og utvidbare disker. VG er abstrakte, og vises ikke i en enhetsfil i <code
                  class="filename">/dev</code>-hierarkiet, så det er ingen risiko for å bruke dem direkte.
				</div><div
                class="para">
					Den tredje typen objekt er LV (<span
                  class="emphasis"><em>Logical Volume</em></span>, logisk volum), som er en del av en VG; hvis vi holder på VG-as-disk analogien, LV kan sammenlignes med en partisjon. LV-en fremstår som en blokkenhet med en oppføring i <code
                  class="filename">/dev</code>, og den kan brukes som en hvilken som helst annen fysisk partisjon (som oftest, for å være vert for et filsystem eller et vekselminne).
				</div><div
                class="para">
					Det viktige er at splittingen av en VG til LVS-er er helt uavhengig av dens fysiske komponenter (PVS-ene). En VG med bare en enkelt fysisk komponent (en disk for eksempel) kan deles opp i et dusin logiske volumer; på samme måte kan en VG bruke flere fysiske disker, og fremstå som et eneste stort logisk volum. Den eneste begrensningen, selvsagt, er at den totale størrelsen allokert (fordelt) til LV-er kan ikke være større enn den totale kapasiteten til PV-ene i volumgruppen.
				</div><div
                class="para">
					Det er imidlertid ofte fornuftig å ha en viss form for homogenitet blant de fysiske komponentene i en VG, og dele VG-en i logiske volumer som vil ha lignende brukermønstre. For eksempel, hvis tilgjengelig maskinvare inkluderer raske og tregere disker, kan de raske bli gruppert i en VG, og de tregere i en annen; deler av den første kan deretter bli tildelt til applikasjoner som krever rask tilgang til data, mens den andre kan beholdes til mindre krevende oppgaver.
				</div><div
                class="para">
					I alle fall, husk at en LV ikke er spesielt knyttet til en bestemt PV. Det er mulig å påvirke hvor data fra en LV fysisk er lagret, men å bruk denne muligheten på daglig basis er ikke nødvendig. Tvert imot: Ettersom settet med fysiske komponenter i en VG utvikler seg, kan de fysiske lagringsstedene som tilsvarer en bestemt LV, migreres over disker (mens den selvfølgelig blir værende innenfor PV-er tildelt VG-en).
				</div></div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        id="sect.lvm-setup"></a>12.1.2.2. Å sette opp LVM</h4></div></div></div><div
                class="para">
					La oss nå følge, trinn for trinn, prosessen med å sette opp LVM for et typisk brukstilfelle: Vi ønsker å forenkle en kompleks lagringssituasjon. En slik situasjon skjer vanligvis etter en lang og innfløkt historie med akkumulerte midlertidige tiltak. For illustrasjonsformål vil vi vurdere en tjener der lagringsbehovene har endret seg over tid, og endte opp i en labyrint av tilgjengelige partisjoner fordelt over flere delvis brukte disker. Mer konkret er følgende partisjoner tilgjengelige:
				</div><div
                xmlns:d="http://docbook.org/ns/docbook"
                class="itemizedlist"><ul><li
                    class="listitem"><div
                      class="para">
							på <code
                        class="filename">sdb</code>-disken, en <code
                        class="filename">sdb2</code>-partisjon, 4 GB;
						</div></li><li
                    class="listitem"><div
                      class="para">
							på <code
                        class="filename">sdc</code>-disken, en <code
                        class="filename">sdc3</code>-partisjon, 3 GB;
						</div></li><li
                    class="listitem"><div
                      class="para">
							<code
                        class="filename">sdd</code>-disken, 4 GB, fullt tilgjengelig;
						</div></li><li
                    class="listitem"><div
                      class="para">
							på <code
                        class="filename">sdf</code>-disken, en <code
                        class="filename">sdf1</code>-partisjon, 4 GB; og en <code
                        class="filename">sdf2</code>-partisjon, 5 GB.
						</div></li></ul></div><div
                class="para">
					I tillegg, la oss anta at diskene <code
                  class="filename">sdb</code> og <code
                  class="filename">sdf</code> er raskere enn de andre to.
				</div><div
                class="para">
					Vårt mål er å sette opp tre logiske volumer for tre ulike programmer: En filtjener som krever 5 GB lagringsplass, en database (1 GB), og noe plass for sikkerhetskopiering (12 GB). De to første trenger god ytelse, men sikkerhetskopiering er mindre kritisk med tanke på tilgangshastighet. Alle disse begrensninger forhindrer bruk av partisjoner på egen hånd; å bruke LVM kan samle den fysiske størrelsen på enhetene, slik at den totale tilgjengelige plassen er den eneste begrensningen.
				</div><div
                class="para">
					De verktøy som kreves er i <span
                  class="pkg pkg">lvm2</span>-pakken og det den krever. Når de er installert, skal det tre trinn til for å sette opp LVM som svarer til de tre konseptnivåene.
				</div><div
                class="para">
					Først forbereder vi de fysiske volumene ved å bruke <code
                  class="command">pvcreate</code>:
				</div><a
                id="screen.pvcreate"></a><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>pvdisplay</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>pvcreate /dev/sdb2</code></strong>
<code
                  class="computeroutput">  Physical volume "/dev/sdb2" successfully created
# </code><strong
                  class="userinput"><code>pvdisplay</code></strong>
<code
                  class="computeroutput">  "/dev/sdb2" is a new physical volume of "4.00 GiB"
  --- NEW Physical volume ---
  PV Name               /dev/sdb2
  VG Name               
  PV Size               4.00 GiB
  Allocatable           NO
  PE Size               0   
  Total PE              0
  Free PE               0
  Allocated PE          0
  PV UUID               0zuiQQ-j1Oe-P593-4tsN-9FGy-TY0d-Quz31I

# </code><strong
                  class="userinput"><code>for i in sdc3 sdd sdf1 sdf2 ; do pvcreate /dev/$i ; done</code></strong>
<code
                  class="computeroutput">  Physical volume "/dev/sdc3" successfully created
  Physical volume "/dev/sdd" successfully created
  Physical volume "/dev/sdf1" successfully created
  Physical volume "/dev/sdf2" successfully created
# </code><strong
                  class="userinput"><code>pvdisplay -C</code></strong>
<code
                  class="computeroutput">  PV         VG   Fmt  Attr PSize PFree
  /dev/sdb2       lvm2 ---  4.00g 4.00g
  /dev/sdc3       lvm2 ---  3.09g 3.09g
  /dev/sdd        lvm2 ---  4.00g 4.00g
  /dev/sdf1       lvm2 ---  4.10g 4.10g
  /dev/sdf2       lvm2 ---  5.22g 5.22g
</code></pre><div
                class="para">
					Så langt så bra. Vær oppmerksom på at en PV kan settes opp på en full disk, samt på individuelle partisjoner på den. Som vist ovenfor <code
                  class="command">pvdisplay</code>-kommandoen lister eksisterende PV-er, med to mulige utdataresultater.
				</div><div
                class="para">
					Nå la oss sette sammen disse fysiske elementer til VG-er ved å bruke <code
                  class="command">vgcreate</code>. Vi vil samle bare PV-er fra de raske diskene inn i en <code
                  class="filename">vg_critical</code>-VG. Den andre VG-en, <code
                  class="filename">vg_normal</code>, vil også inkludere langsommere elementer.
				</div><a
                id="screen.vgcreate"></a><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>vgdisplay</code></strong>
<code
                  class="computeroutput">  No volume groups found
# </code><strong
                  class="userinput"><code>vgcreate vg_critical /dev/sdb2 /dev/sdf1</code></strong>
<code
                  class="computeroutput">  Volume group "vg_critical" successfully created
# </code><strong
                  class="userinput"><code>vgdisplay</code></strong>
<code
                  class="computeroutput">  --- Volume group ---
  VG Name               vg_critical
  System ID             
  Format                lvm2
  Metadata Areas        2
  Metadata Sequence No  1
  VG Access             read/write
  VG Status             resizable
  MAX LV                0
  Cur LV                0
  Open LV               0
  Max PV                0
  Cur PV                2
  Act PV                2
  VG Size               8.09 GiB
  PE Size               4.00 MiB
  Total PE              2071
  Alloc PE / Size       0 / 0   
  Free  PE / Size       2071 / 8.09 GiB
  VG UUID               bpq7zO-PzPD-R7HW-V8eN-c10c-S32h-f6rKqp

# </code><strong
                  class="userinput"><code>vgcreate vg_normal /dev/sdc3 /dev/sdd /dev/sdf2</code></strong>
<code
                  class="computeroutput">  Volume group "vg_normal" successfully created
# </code><strong
                  class="userinput"><code>vgdisplay -C</code></strong>
<code
                  class="computeroutput">  VG          #PV #LV #SN Attr   VSize  VFree 
  vg_critical   2   0   0 wz--n-  8.09g  8.09g
  vg_normal     3   0   0 wz--n- 12.30g 12.30g
</code></pre><div
                class="para">
					Her igjen, kommandoer er ganske greie (og <code
                  class="command">vgdisplay</code> foreslår to utdataformater). Merk at det er fullt mulig å bruke to partisjoner på samme fysiske disk i to forskjellige VG-er. Merk også at vi brukte en <code
                  class="filename">vg_</code>-forstavelse til å navngi våre VG-er, men det er ikke noe mer enn en konvensjon.
				</div><div
                class="para">
					Vi har nå to «virtuelle disker», med størrelse henholdsvis ca. 8 GB og 12 GB. La oss nå riste dem opp i «virtuelle partisjoner» (LV-er). Dette innbefatter <code
                  class="command">lvcreate</code>-kommandoen, og en litt mer komplisert syntaks:
				</div><a
                id="screen.lvcreate"></a><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>lvdisplay</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>lvcreate -n lv_files -L 5G vg_critical</code></strong>
<code
                  class="computeroutput">  Logical volume "lv_files" created
# </code><strong
                  class="userinput"><code>lvdisplay</code></strong>
<code
                  class="computeroutput">  --- Logical volume ---
  LV Path                /dev/vg_critical/lv_files
  LV Name                lv_files
  VG Name                vg_critical
  LV UUID                J3V0oE-cBYO-KyDe-5e0m-3f70-nv0S-kCWbpT
  LV Write Access        read/write
  LV Creation host, time mirwiz, 2015-06-10 06:10:50 -0400
  LV Status              available
  # open                 0
  LV Size                5.00 GiB
  Current LE             1280
  Segments               2
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           253:0

# </code><strong
                  class="userinput"><code>lvcreate -n lv_base -L 1G vg_critical</code></strong>
<code
                  class="computeroutput">  Logical volume "lv_base" created
# </code><strong
                  class="userinput"><code>lvcreate -n lv_backups -L 12G vg_normal</code></strong>
<code
                  class="computeroutput">  Logical volume "lv_backups" created
# </code><strong
                  class="userinput"><code>lvdisplay -C</code></strong>
<code
                  class="computeroutput">  LV         VG          Attr     LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_base    vg_critical -wi-a---  1.00g                                           
  lv_files   vg_critical -wi-a---  5.00g                                           
  lv_backups vg_normal   -wi-a--- 12.00g</code></pre><div
                class="para">
					To parametere er nødvendig når du oppretter logiske volumer; de må sendes til <code
                  class="command">lvcreate</code> som valgmuligheter. Navnet på LV som skal opprettes er angitt med alternativet <code
                  class="literal">-n</code>, og størrelsen dens er generelt gitt ved å bruke <code
                  class="literal">-L</code>-alternativet. Vi trenger også, selvfølgelig, å fortelle kommandoen hvilken VG som skal brukes, derav den siste parameteren på kommandolinjen.
				</div><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>FOR VIDEREKOMMENDE</em></span> <code
                            class="command">lvcreate</code>-valgene</strong></p></div></div></div><div
                  class="para">
					Kommandoen <code
                    class="command">lvcreate</code> har flere alternativer for å tilpasse hvordan LV-en blir laget.
				</div><div
                  class="para">
					La oss først beskrive <code
                    class="literal">-l</code>-valget, der LVs størrelse kan gis som et antall blokker (i motsetning til de «menneskelige» enheter vi brukte ovenfor). Disse blokkene (kalt PES, <span
                    class="emphasis"><em>physical extents</em></span> (fysiske omfang), i LVM-termer) er sammenhengende enheter med lagringsplass i PV-er, og de kan ikke deles på tvers over LV-er. Når man ønsker å definere lagringsplass for en LV med noe presisjon, for eksempel å bruke hele det tilgjengelige rommet, vil <code
                    class="literal">-l</code>-valget trolig bli foretrukket fremfor <code
                    class="literal">-L</code>.
				</div><div
                  class="para">
					Det er også mulig å antyde den fysiske plasseringen for en LV, slik at dens omfang lagres på en bestemt PV (mens du selvfølgelig er innenfor den som er tildelt VG-en). Siden vi vet at <code
                    class="filename">sdb</code> er raskere enn <code
                    class="filename">sdf</code>, kan vi ønske å lagre <code
                    class="filename">lv_base</code> der hvis vi ønsker å gi en fordel til databasetjeneren i forhold til filtjeneren. Kommandolinjen blir: <code
                    class="command">lvcreate -n lv_base -L 1G vg_critical /dev/sdb2</code>. Merk at denne kommandoen kan mislykkes hvis PV-en ikke har nok ledig plass. I vårt eksempel ville vi trolig måtte lage <code
                    class="filename">lv_base</code> før <code
                    class="filename">lv_files</code> for å unngå denne situasjonen - eller frigjøre litt plass på <code
                    class="filename">sdb2</code> med <code
                    class="command">pvmove</code>-kommandoen.
				</div></div><div
                class="para">
					Logiske volumer, en gang laget, ender opp som blokkenhetsfiler i <code
                  class="filename">/dev/mapper/</code>:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>ls -l /dev/mapper</code></strong>
<code
                  class="computeroutput">total 0
crw------- 1 root root 10, 236 Jun 10 16:52 control
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_base -&gt; ../dm-1
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_critical-lv_files -&gt; ../dm-0
lrwxrwxrwx 1 root root       7 Jun 10 17:05 vg_normal-lv_backups -&gt; ../dm-2
# </code><strong
                  class="userinput"><code>ls -l /dev/dm-*</code></strong>
<code
                  class="computeroutput">brw-rw---T 1 root disk 253, 0 Jun 10 17:05 /dev/dm-0
brw-rw---- 1 root disk 253, 1 Jun 10 17:05 /dev/dm-1
brw-rw---- 1 root disk 253, 2 Jun 10 17:05 /dev/dm-2
</code></pre><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>NOTE</em></span> Automatisk gjenkjenning av LVM</strong></p></div></div></div><div
                  class="para">
					Når datamaskinen starter, kjører <code
                    class="filename">lvm2-activation</code> systemd tjenesteenhet <code
                    class="command">vgchange -aay</code> for å «aktivisere» volumgrupper. Den skanner de tilgjengelige enhetene; de som har blitt initialisert som fysiske volumer for LVM, er registrert i LVMs undersystem, de som tilhører volumgrupper monteres, og de aktuelle logiske volumer er startet og gjort tilgjengelige. Det er derfor ikke nødvendig å redigere konfigurasjonsfiler når du oppretter eller endrer LVM-volumer.
				</div><div
                  class="para">
					Merk imidlertid at utformingen av LVM-elementer (fysiske og logiske volumer, og volumgrupper) er sikkerhetskopiert i <code
                    class="filename">/etc/lvm/backup</code>, som kan være nyttig i tilfelle av et problem (eller bare for å snike seg til en titt under panseret).
				</div></div><div
                class="para">
					For å gjøre ting enklere er praktiske og egnede symbolske lenker også opprettet i kataloger som samsvarer med VG-er:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>ls -l /dev/vg_critical</code></strong>
<code
                  class="computeroutput">total 0
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_base -&gt; ../dm-1
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_files -&gt; ../dm-0
# </code><strong
                  class="userinput"><code>ls -l /dev/vg_normal</code></strong>
<code
                  class="computeroutput">total 0
lrwxrwxrwx 1 root root 7 Jun 10 17:05 lv_backups -&gt; ../dm-2</code></pre><div
                class="para">
					LV-er kan deretter brukes akkurat som standard partisjoner:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mkfs.ext4 /dev/vg_normal/lv_backups</code></strong>
<code
                  class="computeroutput">mke2fs 1.42.12 (29-Aug-2014)
Creating filesystem with 3145728 4k blocks and 786432 inodes
Filesystem UUID: b5236976-e0e2-462e-81f5-0ae835ddab1d
[...]
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done 
# </code><strong
                  class="userinput"><code>mkdir /srv/backups</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>mount /dev/vg_normal/lv_backups /srv/backups</code></strong>
<code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>df -h /srv/backups</code></strong>
<code
                  class="computeroutput">Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_normal-lv_backups   12G   30M   12G   1% /srv/backups
# </code><strong
                  class="userinput"><code>[...]</code></strong>
<code
                  class="computeroutput">[...]
# </code><strong
                  class="userinput"><code>cat /etc/fstab</code></strong>
<code
                  class="computeroutput">[...]
/dev/vg_critical/lv_base    /srv/base       ext4 defaults 0 2
/dev/vg_critical/lv_files   /srv/files      ext4 defaults 0 2
/dev/vg_normal/lv_backups   /srv/backups    ext4 defaults 0 2</code></pre><div
                class="para">
					Fra programsynspunkt har de utallige små partisjonene nå blitt abstrahert til ett stort 12 GB volum, med et vennligere navn.
				</div></div><div
              class="section"><div
                class="titlepage"><div><div><h4
                      class="title"><a
                        id="sect.lvm-over-time"></a>12.1.2.3. LVM over tid</h4></div></div></div><div
                class="para">
					Selv om muligheten til å aggregere partisjoner eller fysiske disker er praktisk, er dette ikke den viktigste fordelen LVM har brakt. Den fleksibiliteten den gir, er spesielt lagt merke til over tid, ettersom behovene utvikler seg. I vårt eksempel, la oss anta at nye store filer må lagres, og at LV øremerket til filtjeneren er for liten til å romme dem. Siden vi ikke har brukt hele plassen i <code
                  class="filename">vg_critical</code>, kan vi øke <code
                  class="filename">lv_files</code>. For det formålet bruker vi <code
                  class="command">lvresize</code>-kommandoen, deretter <code
                  class="command">resize2fs</code> for å tilpasse filsystemet tilsvarende:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>df -h /srv/files/</code></strong>
<code
                  class="computeroutput">Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_files  5.0G  4.6G  146M  97% /srv/files
# </code><strong
                  class="userinput"><code>lvdisplay -C vg_critical/lv_files</code></strong>
<code
                  class="computeroutput">  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_files vg_critical -wi-ao-- 5.00g
# </code><strong
                  class="userinput"><code>vgdisplay -C vg_critical</code></strong>
<code
                  class="computeroutput">  VG          #PV #LV #SN Attr   VSize VFree
  vg_critical   2   2   0 wz--n- 8.09g 2.09g
# </code><strong
                  class="userinput"><code>lvresize -L 7G vg_critical/lv_files</code></strong>
<code
                  class="computeroutput">  Size of logical volume vg_critical/lv_files changed from 5.00 GiB (1280 extents) to 7.00 GiB (1792 extents).
  Logical volume lv_files successfully resized
# </code><strong
                  class="userinput"><code>lvdisplay -C vg_critical/lv_files</code></strong>
<code
                  class="computeroutput">  LV       VG          Attr     LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync  Convert
  lv_files vg_critical -wi-ao-- 7.00g
# </code><strong
                  class="userinput"><code>resize2fs /dev/vg_critical/lv_files</code></strong>
<code
                  class="computeroutput">resize2fs 1.42.12 (29-Aug-2014)
Filesystem at /dev/vg_critical/lv_files is mounted on /srv/files; on-line resizing required
old_desc_blocks = 1, new_desc_blocks = 1
The filesystem on /dev/vg_critical/lv_files is now 1835008 (4k) blocks long.

# </code><strong
                  class="userinput"><code>df -h /srv/files/</code></strong>
<code
                  class="computeroutput">Filesystem                        Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_files  6.9G  4.6G  2.1G  70% /srv/files</code></pre><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>VÆR VARSOM</em></span> Endre størrelse på filsystemer</strong></p></div></div></div><div
                  class="para">
					Ikke alle filsystemer kan få størrelsen endret fra nettet; å endre størrelsen på et volum kan derfor kreve at filsystemet avmonteres først, og remonteres i etterkant. Selvfølgelig, hvis man ønsker å krympe den avsatte plassen til en LV, må filsystemet krympes først. Rekkefølgen reverseres når skaleringen går i motsatt retning: Det logiske volumet må utvides før det aktuelle filsystemet. Det er ganske enkelt, fordi filsystemet ikke på noe tidspunkt må være større enn blokkenheten den ligger på (enten enheten er en fysisk partisjon eller et logisk volum).
				</div><div
                  class="para">
					De ext3-, ext4- og xfs-filsystemer kan vokse på nettet, uten avmontering. Krymping krever avmontering. Reiserfs filsystem tillater online endring av størrelse i begge retninger. Den ærverdige ext2 gjør ingen av delene, og krever alltid demontering.
				</div></div><div
                class="para">
					Vi kunne fortsette på en lignende måte å utvide volumet som er vertskap for databasen, bare til vi har nådd VG-ens grense for tilgjengelig plass:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>df -h /srv/base/</code></strong>
<code
                  class="computeroutput">Filesystem                       Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_base 1008M  854M  104M  90% /srv/base
# </code><strong
                  class="userinput"><code>vgdisplay -C vg_critical</code></strong>
<code
                  class="computeroutput">  VG          #PV #LV #SN Attr   VSize VFree 
  vg_critical   2   2   0 wz--n- 8.09g 92.00m</code></pre><div
                class="para">
					Det er ikke noe problem ettersom LVM tillater å legge til fysiske volumer til eksisterende volumgrupper. Vi har for eksempel kanskje lagt merke til at <code
                  class="filename">sdb1</code>-partisjonen, som så langt ble brukt utenfor LVM, bare inneholdt arkiver som kan flyttes til <code
                  class="filename">lv_backups</code>. Vi kan nå resirkulere den, og integrere den i volumgruppen, og dermed gjenvinne noe ledig plass. Dette er hensikten med <code
                  class="command">vgextend</code>-kommandoen. Selvfølgelig må partisjonen på forhånd forberedes som et fysisk volum. Når VG er utvidet, kan vi bruke lignende kommandoer som tidligere for å utvide det logiske volumet, og deretter filsystemet:
				</div><pre
                class="screen"><code
                  class="computeroutput"># </code><strong
                  class="userinput"><code>pvcreate /dev/sdb1</code></strong>
<code
                  class="computeroutput">  Physical volume "/dev/sdb1" successfully created
# </code><strong
                  class="userinput"><code>vgextend vg_critical /dev/sdb1</code></strong>
<code
                  class="computeroutput">  Volume group "vg_critical" successfully extended
# </code><strong
                  class="userinput"><code>vgdisplay -C vg_critical</code></strong>
<code
                  class="computeroutput">  VG          #PV #LV #SN Attr   VSize VFree
  vg_critical   3   2   0 wz--n- 9.09g 1.09g
# </code><strong
                  class="userinput"><code>[...]</code></strong>
<code
                  class="computeroutput">[...]
# </code><strong
                  class="userinput"><code>df -h /srv/base/</code></strong>
<code
                  class="computeroutput">Filesystem                       Size  Used Avail Use% Mounted on
/dev/mapper/vg_critical-lv_base  2.0G  854M  1.1G  45% /srv/base</code></pre><div
                class="sidebar"><div
                  class="titlepage"><div><div><p
                        class="title"><strong><span
                            class="emphasis"><em>FOR VIDEREKOMMENDE</em></span> Avansert LVM</strong></p></div></div></div><div
                  class="para">
					LVM åpner også for mer avansert bruk, der mange detaljer kan spesifiseres for hånd. For eksempel kan en administrator justere størrelsen på blokkene som utgjør fysiske og logiske volumer, samt deres fysiske utforminger. Det er også mulig å flytte blokker mellom PV-er, for eksempel for å finjustere ytelsen, eller på en mer triviell måte, å frigjøre en PV når man trenger å trekke ut den tilsvarende fysiske disken fra VG-en (om det skal knytte den til en annen VG eller å fjerne den fra LVM helt). Manualsidene som beskriver kommandoene er generelt klare og detaljerte. Et god inngangspunkt er <span
                    class="citerefentry"><span
                      class="refentrytitle">lvm</span>(8)</span>-manualside.
				</div></div></div></div><div
            class="section"><div
              class="titlepage"><div><div><h3
                    class="title"><a
                      id="sect.raid-or-lvm"></a>12.1.3. RAID eller LVM?</h3></div></div></div><div
              class="para">
				RAID og LVM bringer begge udiskutable fordeler så snart man forlater det enkle tilfellet med en stasjonær datamaskin med en enkelt harddisk, der bruksmønster ikke endres over tid. Men RAID og LVM går i to forskjellige retninger, med divergerende mål, og det er legitimt å lure på hvilken som bør velges. Det mest hensiktsmessige svaret vil selvfølgelig avhenge av nåværende og forventede krav.
			</div><div
              class="para">
				Det finnes noen enkle tilfeller hvor spørsmålet egentlig ikke oppstår. Hvis kravet er å sikre data mot maskinvarefeil, så vil åpenbart RAID bli satt opp med en romslig matrise med disker, ettersom LVM ikke løser dette problemet. Dersom det, på den annen side, er behov for et fleksibelt lagringsopplegg der volumene lages uavhengig av den fysiske utformingen av diskene, bidrar ikke RAID med mye, og LVM vil være det naturlige valget.
			</div><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>NOTE</em></span> Hvis ytelse betyr noe…</strong></p></div></div></div><div
                class="para">
				Hvis input/output-hastighet er viktig, spesielt i form av aksesstid, kan det å bruke LVM / eller RAID i en av de mange kombinasjonene ha noen innvirkning på ytelser, og dette kan påvirke beslutninger om hvilken som skal velges. Men disse forskjellene i ytelse er veldig små, og vil bare være målbare i noen brukstilfeller. Hvis ytelsen betyr noe, er det størst gevinst ved å bruke ikke-roterende lagringsmedier (<a
                  id="id-1.15.4.8.4.2.1"
                  class="indexterm"></a><span
                  class="emphasis"><em>solid-state drives</em></span>, eller SSDs). Kostnaden deres per megabyte er høyere enn for standard harddisker, kapasiteten deres er vanligvis mindre, men de gir utmerkede resultater for tilfeldige aksesser. Hvis bruksmønster inneholder mange input/output-operasjoner spredt rundt i filsystemet, for eksempel for databaser der komplekse spørringer blir kjørt rutinemessig, så oppveier fordelen av å kjøre dem på en SSD langt hva som kan oppnås ved å velge LVM over RAID eller omvendt. I slike situasjoner bør valget bestemmes av andre hensyn enn ren fart, siden ytelsesaspektet lettest håndteres ved å bruke SSD.
			</div></div><div
              class="para">
				Det tredje bemerkelsesverdige brukstilfellet er når man bare ønsker å samle to disker i ett volum, enten av ytelseshensyn, eller for å ha et enkelt filsystem som er større enn noen av de tilgjengelige diskene. Dette tilfellet kan adresseres både med en RAID-0 (eller til og med en lineær-RAID), og med et LVM-volum. Når du er i denne situasjonen, og sperring gir ekstra begrensninger (for eksempel å måtte være på linje med resten av datamaskinene hvis de bare bruker RAID), vil konfigurasjonsvalget ofte være LVM. Første oppsett er snaut nok komplekst, og den svake økning i kompleksitet mer enn gjør opp for LVMs ekstra fleksibiliteten dersom kravene må endres, eller dersom nye disker må legges til.
			</div><div
              class="para">
				Så selvfølgelig er det det virkelig interessante brukereksempel, der lagringssystemet må gjøres både motstandsdyktig mot maskinvarefeil, og gi en fleksibel volumtildeling. Verken RAID eller LVM kan imøtekomme begge kravene på egen hånd. Uansett, det er her vi bruker begge samtidig - eller rettere sagt, den ene oppå den andre. Ordningen som har alt, men er blitt en standard siden RAID og LVM har nådd modenheten til å sikre datatallighet (dataredundans), først ved å gruppere disker i et lite antall store RAID-matriser, og å bruke disse RAID-matrisene som LVM fysiske volumer. Logiske partisjoner vil da bli meislet ut fra disse LV-ene for filsystemer. Salgspoenget med dette oppsettet er at når en disk svikter, vil bare et lite antall RAID-matriser trenge rekonstruering, og dermed begrense tiden administrator bruker for gjenoppretting.
			</div><div
              class="para">
				La oss ta et konkret eksempel: PR-avdelingen på Falcot Corp trenger en arbeidsstasjon for videoredigering, men avdelingens budsjett tillater ikke investere i dyr maskinvare fra bunnen av. Det er avgjort å favorisere maskinvaren som spesifikk for det grafiske arbeidets art (skjerm og skjermkort), og å fortsette med felles maskinvare for lagring. Men som viden kjent ,har digital video noen spesielle krav til mengden av date for lagring, og gjennomstrømningshastighet for lesing og skriving er viktig for den generelle systemytelsen (mer enn vanlig aksesstid, for eksempel). Disse begrensningene må være imøtekommet med felles maskinvare, i dette tilfellet med 300 GB SATA-harddisker. Systemdata må også gjøres motstandsdyktige mot maskinvarefeil, og også noen brukerdata. Redigerte videoklipp må faktisk være trygge, men for videoer som venter på redigering er det mindre kritisk, siden de fortsatt er på videobånd, eller på opptaksutstyret.
			</div><div
              class="para">
				RAID-1 og LVM kombineres for å tilfredsstille disse begrensningene. Diskene er knyttet til to forskjellige SATA-kontrollere for å optimalisere parallell tilgang, og redusere risikoen for samtidig svikt, og de synes derfor som <code
                class="filename">sda</code> og <code
                class="filename">sdc</code>. De er partisjonert likt langs det følgende skjemaet:
			</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>fdisk -l /dev/sda</code></strong>
<code
                class="computeroutput">
Disk /dev/sda: 300 GB, 300090728448 bytes, 586114704 sectors
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x00039a9f

Device    Boot     Start       End   Sectors Size Id Type
/dev/sda1 *         2048   1992060   1990012 1.0G fd Linux raid autodetect
/dev/sda2        1992061   3984120   1992059 1.0G 82 Linux swap / Solaris
/dev/sda3        4000185 586099395 582099210 298G 5  Extended
/dev/sda5        4000185 203977305 199977120 102G fd Linux raid autodetect
/dev/sda6      203977306 403970490 199993184 102G fd Linux raid autodetect
/dev/sda7      403970491 586099395 182128904  93G 8e Linux LVM</code></pre><div
              xmlns:d="http://docbook.org/ns/docbook"
              class="itemizedlist"><ul><li
                  class="listitem"><div
                    class="para">
						De første partisjonene til begge disker (ca 1 GB) er satt sammen til ett RAID-1-volum <code
                      class="filename">md0</code>. Dette speilet er direkte brukt til å lagre rotfilsystemet.
					</div></li><li
                  class="listitem"><div
                    class="para">
						Partisjonene <code
                      class="filename">sda2</code> og <code
                      class="filename">sdc2</code> brukes som vekselminnepartisjoner, noe som gir en totalt 2 GB vekselminne. Med 1 GB RAM, har arbeidsstasjonen en komfortabel mengde tilgjengelig minne.
					</div></li><li
                  class="listitem"><div
                    class="para">
						Partisjonene <code
                      class="filename">sda5</code> og <code
                      class="filename">sdc5</code>, så vel som <code
                      class="filename">sda6</code> og <code
                      class="filename">sdc6</code>, er samlet til to nye RAID-1-volumer på rundt 100 GB hver, <code
                      class="filename">md1</code> og <code
                      class="filename">md2</code>. Begge disse speilene er internalisert som fysiske volumer for LVM, og knyttet til volumgruppen <code
                      class="filename">vg_raid</code>. Denne VG-en inneholder derfor et trygt rom på 200 GB.
					</div></li><li
                  class="listitem"><div
                    class="para">
						De gjenstående partisjoner, <code
                      class="filename">sda7</code> og <code
                      class="filename">sdc7</code>, brukes direkte som fysiske volumer, og knyttet til en annen VG kalt <code
                      class="filename">vg_bulk</code>, som da ender opp med omtrent 200 GB lagringsplass.
					</div></li></ul></div><div
              class="para">
				Når VG-er er opprettet, kan de fordeles svært fleksibelt. Man må huske på at LV-er opprettet i <code
                class="filename">vg_raid</code>, blir bevart selv om en av diskene svikter, noe som ikke vil være tilfelle for LV-er opprettet i <code
                class="filename">vg_bulk</code>. På den annen side vil de sistnevnte fordeles i parallell på begge disker, som tillater høyere lese- eller skrivehastigheter for store filer.
			</div><div
              class="para">
				Vi vil derfor lage <code
                class="filename">lv_usr</code>, <code
                class="filename">lv_var</code> og <code
                class="filename">lv_home</code> LVs on <code
                class="filename">vg_raid</code> til å være vertskap for de matchende filsystemene. En annen stor LV, <code
                class="filename">lv_movies</code>, skal brukes som vert for endelige versjoner av filmer etter redigering. Den andre VG-en vil bli delt inn i et stort <code
                class="filename">lv_rushes</code>, for data rett fra det digitale videokameraet, og et <code
                class="filename">lv_tmp</code> for midlertidige filer. Plasseringen av arbeidsområdet er et mindre enkelt valg å ta. Mens god ytelse er nødvendig for det volumet, er det verdt å risikere å miste arbeid hvis en disk svikter under redigeringsøkten ? Avhengig av svaret på det spørsmålet, vil den aktuelle LV-en bli lagt til den ene VG-en, eller på den andre.
			</div><div
              class="para">
				Vi har nå både litt overskudd til viktige data, og mye fleksibilitet i hvordan den tilgjengelige plassen er delt på tvers av programmene. Skal ny programvare installeres senere (for å redigere lydklipp, for eksempel), kan LV-vertskapet <code
                class="filename">/usr/</code> utvides smertefritt.
			</div><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>NOTE</em></span> Hvorfor tre RAID-1-volumer?</strong></p></div></div></div><div
                class="para">
				Vi kunne ha satt opp ett RAID-1-volum bare for å tjene som et fysisk volum for <code
                  class="filename">vg_raid</code>. Hvorfor lage tre av dem da?
			</div><div
                class="para">
				Grunnen til den første delingen (<code
                  class="filename">md0</code> opp mot de andre) dreier seg om datasikkerhet. Data skrevet til begge elementer i et RAID-1-speil er nøyaktig de samme, og det er derfor mulig å omgå RAID-laget, og montere en av diskene direkte. I tilfelle av, for eksempel en kjernefeil, eller hvis LVM-metadata blir ødelagt, er det fortsatt mulig å starte opp et minimalt system for å få tilgang til viktige data som for eksempel utformingen av diskene i RAID-en og LVM-en. Metadataene kan så rekonstrueres, og filene kan igjen nås, slik at systemet kan bringes tilbake til sin nominelle tilstand.
			</div><div
                class="para">
				Begrunnelsen for den andre delingen (<code
                  class="filename">md1</code> mot <code
                  class="filename">md2</code>) er mindre entydig, og mer knyttet til erkjennelsen av at fremtiden er usikker. Når arbeidsstasjonen først er montert, er de eksakte kravene til oppbevaring ikke nødvendigvis kjent med perfekt presisjon. De kan også utvikle seg over tid. I vårt tilfelle kan vi ikke på forhånd vite det faktiske lagringsbehovet for video-opptak og komplette videoklipp. Hvis et bestemt klipp har en meget stor mengde uredigerte opptak, og VG-en øremerket til ledige data er mindre enn halvveis full, kan vi gjenbruke noe av den plassen som ikke trenges. Vi kan fjerne et av de fysiske volumene, la oss si <code
                  class="filename">md2</code>, fra <code
                  class="filename">vg_raid</code>, og enten knytte det til <code
                  class="filename">vg_bulk</code> direkte (hvis den forventede varigheten av operasjonen er kort nok til at vi kan leve med midlertidig fall i ytelsen), eller sette tilbake RAID-oppsettet på <code
                  class="filename">md2</code>, og integrere komponentene dens, <code
                  class="filename">sda6</code>, og <code
                  class="filename">sdc6</code>, i den store VG-en (som ekspanderer til 200 GB i stedet for 100 GB). Det logiske volumet <code
                  class="filename">lv_rushes</code> kan så ekspandere i tråd med det som kreves.
			</div></div></div></div></div><ul
        class="docnav"><li
          class="previous"><a
            accesskey="p"
            href="sect.rtc-services.html"><strong>Forrige</strong>11.8. Sanntids kommunikasjonstjenester</a></li><li
          class="up"><a
            accesskey="u"
            href="#"><strong>Opp</strong></a></li><li
          class="home"><a
            accesskey="h"
            href="index.html"><strong>Hjem</strong></a></li><li
          class="next"><a
            accesskey="n"
            href="sect.virtualization.html"><strong>Neste</strong>12.2. virtualisering</a></li></ul><div
        id="translated_pages"><ul><li><a
              href="../ar-MA/advanced-administration.html">ar-MA</a></li><li><a
              href="../da-DK/advanced-administration.html">da-DK</a></li><li><a
              href="../de-DE/advanced-administration.html">de-DE</a></li><li><a
              href="../el-GR/advanced-administration.html">el-GR</a></li><li><a
              href="../en-US/advanced-administration.html">en-US</a></li><li><a
              href="../es-ES/advanced-administration.html">es-ES</a></li><li><a
              href="../fa-IR/advanced-administration.html">fa-IR</a></li><li><a
              href="../fr-FR/advanced-administration.html">fr-FR</a></li><li><a
              href="../hr-HR/advanced-administration.html">hr-HR</a></li><li><a
              href="../id-ID/advanced-administration.html">id-ID</a></li><li><a
              href="../it-IT/advanced-administration.html">it-IT</a></li><li><a
              href="../ja-JP/advanced-administration.html">ja-JP</a></li><li><a
              href="../ko-KR/advanced-administration.html">ko-KR</a></li><li><a
              href="../nb-NO/advanced-administration.html">nb-NO</a></li><li><a
              href="../pl-PL/advanced-administration.html">pl-PL</a></li><li><a
              href="../pt-BR/advanced-administration.html">pt-BR</a></li><li><a
              href="../ro-RO/advanced-administration.html">ro-RO</a></li><li><a
              href="../ru-RU/advanced-administration.html">ru-RU</a></li><li><a
              href="../tr-TR/advanced-administration.html">tr-TR</a></li><li><a
              href="../vi-VN/advanced-administration.html">vi-VN</a></li><li><a
              href="../zh-CN/advanced-administration.html">zh-CN</a></li><li><a
              href="../zh-TW/advanced-administration.html">zh-TW</a></li></ul></div></body></html>
