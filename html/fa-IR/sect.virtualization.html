<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html
    xmlns="http://www.w3.org/1999/xhtml"><head><meta
        http-equiv="Content-Type"
        content="text/html; charset=UTF-8" /><title
        xmlns:d="http://docbook.org/ns/docbook">12.2. مجازی‌سازی</title><link
        rel="stylesheet"
        type="text/css"
        href="Common_Content/css/default.css" /><link
        rel="stylesheet"
        media="print"
        href="Common_Content/css/print.css"
        type="text/css" /><meta
        xmlns:d="http://docbook.org/ns/docbook"
        name="generator"
        content="publican v4.3.2" /><meta
        xmlns:d="http://docbook.org/ns/docbook"
        name="package"
        content="Debian-debian-handbook-9-fa-IR-1.0-1" /><meta
        name="keywords"
        content="RAID, LVM, FAI, Preseeding, مانیتورینگ, مجازی‌سازی, Xen, LXC" /><link
        rel="home"
        href="index.html"
        title="راهنمای جامع دبیان" /><link
        rel="up"
        href="advanced-administration.html"
        title="فصل 12. مدیریت پیشرفته" /><link
        rel="prev"
        href="advanced-administration.html"
        title="فصل 12. مدیریت پیشرفته" /><link
        rel="next"
        href="sect.automated-installation.html"
        title="12.3. نصب خودکار" /><meta
        name="viewport"
        content="width=device-width, initial-scale=1" /><meta
        name="flattr:id"
        content="4pz9jq" /><link
        rel="canonical"
        href="http://l.github.io/debian-handbook/html/fa-IR/sect.virtualization.html" /></head><body
      dir="rtl"><noscript><iframe
          src="//www.googletagmanager.com/ns.html?id=GTM-5H35QX"
          height="0"
          width="0"
          style="display:none;visibility:hidden"></iframe></noscript><script
        type="text/javascript">(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&amp;l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-5H35QX');</script><div
        id="banner"><a
          href="../../"><span
            class="text">Download the ebook</span></a></div><p
        id="title"><a
          class="left"
          href="http://www.debian.org"><img
            alt="Product Site"
            src="Common_Content/images//image_left.png" /></a><a
          class="right"
          href="index.html"><img
            alt="Documentation Site"
            src="Common_Content/images//image_right.png" /></a></p><ul
        class="docnav top"><li
          class="previous"><a
            accesskey="p"
            href="advanced-administration.html"><strong>قبلی</strong></a></li><li
          class="home">راهنمای جامع دبیان</li><li
          class="next"><a
            accesskey="n"
            href="sect.automated-installation.html"><strong>بعدی</strong></a></li></ul><div
        class="section"><div
          class="titlepage"><div><div><h2
                class="title"><a
                  id="sect.virtualization"></a>12.2. مجازی‌سازی</h2></div></div></div><a
          id="id-1.15.5.2"
          class="indexterm"></a><div
          class="para">
			مجازی‌سازی یکی از بزرگترین پیشرفت‌های علوم رایانه در سال‌های اخیر است. این عبارت شامل چندین مفهوم انتزاعی و تکنیکی است که در شبیه‌سازی رایانه‌های مجازی به طوری که مستقل از سخت‌افزار واقعی عمل می‌کنند. یک سرور فیزیکی می‌تواند شامل چندین سرور مجازی باشد که جدا از یکدیگر فعالیت می‌کنند. برنامه‌های کاربردی آن بسیار هستند که از این انزوا مشتق می‌شوند: برای نمونه، محیط‌های آزمایشی همراه با پیکربندی‌های متفاوت یا جداسازی سرویس‌های میزبانی بین چندین ماشین مجازی برای امنیت.
		</div><div
          class="para">
			راهکارهای گوناگون مجازی‌سازی هر کدام با نقاط قوت و ضعف خود وجود دارند. تمرکز این کتاب روی Xen، LXC و KVM است اما سایر پیاده‌سازی‌های قابل ذکر عبارتند از:
		</div><a
          id="id-1.15.5.5"
          class="indexterm"></a><a
          id="id-1.15.5.6"
          class="indexterm"></a><a
          id="id-1.15.5.7"
          class="indexterm"></a><a
          id="id-1.15.5.8"
          class="indexterm"></a><a
          id="id-1.15.5.9"
          class="indexterm"></a><a
          id="id-1.15.5.10"
          class="indexterm"></a><div
          xmlns:d="http://docbook.org/ns/docbook"
          class="itemizedlist"><ul><li
              class="listitem"><div
                class="para">
					QEMU یک شبیه‌ساز نرم‌افزاری برای یک رایانه کامل است؛ عملکرد آن چیزی بیشتر از سرعت است که می‌توان به آن دست یافت، اما این فرآیند امکان اجرای سیستم عامل‌های تغییرنیافته یا آزمایشی را روی سخت‌افزار شبیه‌سازی شده فراهم می‌کند. همچنین امکان شبیه‌سازی چندین معماری گوناگون سخت‌افزاری را نیز فراهم می‌کند: برای نمونه، یک سیستم <span
                  class="emphasis"><em>amd64</em></span> می‌تواند یک رایانه <span
                  class="emphasis"><em>arm</em></span> را شبیه‌سازی کند. QEMU نرم‌افزار آزاد است. <div
                  class="url">→ <a
                    href="http://www.qemu.org/">http://www.qemu.org/</a></div>
				</div></li><li
              class="listitem"><div
                class="para">
					... نیز یک ماشین مجازی آزاد دیگر است، اما تنها به شبیه‌سازی معماری‌های x86 می‌پردازد (i386 و amd64).
				</div></li><li
              class="listitem"><div
                class="para">
					VMWare یک ماشین مجازی انحصاری است؛ به عنوان یکی از قدیمی‌ترین گزینه‌های موجود، یکی از شناخته‌شده‌ترین راهکارهای مجازی‌سازی است. بر اساس مبانی مشترک با QEMU کار می‌کند. VMWare قابلیت‌های پیشرفته‌ای از جمله snapshot گرفتن از یک ماشین مجازی در حال اجرا را فراهم می‌کند. <div
                  class="url">→ <a
                    href="http://www.vmware.com/">http://www.vmware.com/</a></div>
				</div></li><li
              class="listitem"><div
                class="para">
					VirtualBox is a virtual machine that is mostly free software (some extra components are available under a proprietary license). Unfortunately it is in Debian's “contrib” section because it includes some precompiled files that cannot be rebuilt without a proprietary compiler and it currently only resides in Debian Unstable as Oracle's policies make it impossible to keep it secure in a Debian stable release (see <a
                  href="https://bugs.debian.org/794466">#794466</a>). While younger than VMWare and restricted to the i386 and amd64 architectures, it still includes some snapshotting and other interesting features. <div
                  class="url">→ <a
                    href="http://www.virtualbox.org/">http://www.virtualbox.org/</a></div>
				</div></li></ul></div><div
          class="section"><div
            class="titlepage"><div><div><h3
                  class="title"><a
                    id="sect.xen"></a>12.2.1. Xen</h3></div></div></div><div
            class="para">
				Xen <a
              id="id-1.15.5.12.2.1"
              class="indexterm"></a> یک راهکار “paravirtualization” است که شامل یک لایه انتزاعی سبک به نام “hypervisor” می‌باشد که بین سخت‌افزار و سیستم‌های بالایی قرار می‌گیرد؛ این لایه مانند یک داور دسترسی از ماشین‌های مجازی به سخت‌افزار را کنترل می‌کند. اگرچه، تنها شامل چند دستورالعمل کوتاه است، باقی عملیات به طور مستقیم از طرف سخت‌افزار و به نیابت از سیستم‌های دیگر انجام می‌شوند. مزیت اصلی آن این است که عملکرد کاهش نمی‌یابد و سیستم‌های تقریبا با سرعت اصلی سخت‌افزار اجرا می‌شوند؛ اشکال اصلی آن این است که کرنل‌های سیستم عامل‌ها به منظور استفاده از hypervisor باید سازگار با Xen باشند.
			</div><div
            class="para">
				بیایید چند عبارت را بررسی کنیم. hypervisor پایین‌ترین لایه است که به صورت مستقیم روی سخت‌افزار اجرا می‌شود، حتی پایین‌تر از کرنل. این hypervisor می‌تواند سایر نرم‌افزارها را درون چندین <span
              class="emphasis"><em>دامنه</em></span> قرار دهد، که می‌توانند به عنوان ماشین‌های مجازی دیده شوند. یکی از این دامنه‌ها (اولین آن‌ها که آغاز می‌شود) به عنوان <span
              class="emphasis"><em>dom0</em></span> شناخته می‌شود و نقش ویژه‌ای دارد، چرا که تنها این دامنه می‌تواند hypervisor و اجرای سایر دامنه‌ها را کنترل کند. سایر دامنه‌ها به عنوان <span
              class="emphasis"><em>domU</em></span> شناخته می‌شوند. به عبارت دیگر، و از دید کاربر، <span
              class="emphasis"><em>dom0</em></span> به عنوان “میزبان” برای سایر سیستم‌های مجازی عمل می‌کند در صورتی که <span
              class="emphasis"><em>domU</em></span> به عنوان یک “میهمان” دیده می‌شود.
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>فرهنگ</em></span> Xen و نسخه‌های مختلف از لینوکس</strong></p></div></div></div><div
              class="para">
				Xen در ابتدا به عنوان چندین اصلاحیه خارج از ساختار اصلی کرنل لینوکس توسعه یافت. در همان زمان، چندین راهکاری مجازی‌سازی دیگر (از جمله KVM) نیازمند چندین عملکرد عمومی مجازی‌سازی بودند تا یکپارچه‌سازی خود را تسهیل نمایند و کرنل لینوکس این مجموعه قابلیت‌ها را گردآوری کرد (که به عنوان رابط <span
                class="emphasis"><em>paravirt_ops</em></span> یا <span
                class="emphasis"><em>pv_ops</em></span> شناخته می‌شوند). از آنجا که اصلاحیه‌های Xen این قابلیت‌ها را به شیوه‌ای دیگر پیاده‌سازی می‌کردند، نتوانستند به صورت رسمی پذیرفته شوند.
			</div><div
              class="para">
				Xensource، شرکتی که پشت Xen است، مجبور شد تا Xen را به یک چارچوب جدید انتقال دهد به صورتی که اصلاحیه‌های Xen بتوانند درون کرنل رسمی لینوکس ادغام شوند. این کار به معنی بازنویسی قسمت اعظمی از کد بود و با اینکه Xensource به نسخه کارآمدی مبتنی بر رابط paravirt_ops رسیده بود، اصلاحیه‌ها با سرعت کمی درون کرنل رسمی قرار می‌گرفتند. این ادغام در لینوکس ۳.۰ کامل شد. <div
                class="url">→ <a
                  href="http://wiki.xenproject.org/wiki/XenParavirtOps">http://wiki.xenproject.org/wiki/XenParavirtOps</a></div>
			</div><div
              class="para">
				از آنجا که <span
                class="distribution distribution">Jessie</span> مبتنی بر نسخه ۳.۱۶ از کرنل لینوکس است، بسته‌های استاندارد <span
                class="pkg pkg">linux-image-686-pae</span> و <span
                class="pkg pkg">linux-image-amd64</span> شامل کدهای لازم می‌باشند و اصلاحیه‌های مختص به توزیع‌های <span
                class="distribution distribution">Squeeze</span> و قبل از آن دیگر مورد نیاز نیستند. <div
                class="url">→ <a
                  href="http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix">http://wiki.xenproject.org/wiki/Xen_Kernel_Feature_Matrix</a></div>
			</div></div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>یادداشت</em></span> معماری‌های سازگار با Xen</strong></p></div></div></div><div
              class="para">
				Xen هم اکنون تنها برای معماری‌های i386، amd64، arm64 و armhf موجود می‌باشد.
			</div></div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>فرهنگ</em></span> Xen کرنل‌های غیر از لینوکس</strong></p></div></div></div><div
              class="para">
				Xen نیازمند ایجاد تغییرات در تمام سیستم عامل‌هایی است که قصد اجرای روی آن را دارند؛ تمام کرنل‌های به این سطح از بلوغ نرسیده‌اند. بسیاری از آن‌ها کاملا کارآمد هستند، به عنوان dom0 و domU: لینوکس ۳.۰ به بعد، NetBSD ۴.۰ به بعد و OpenSolaris. سایر کرنل‌ها تنها به عنوان domU می‌توانند کار کنند. وضعیت هر کدام از سیستم عامل‌‌ها را می‌توانید در صفحه ویکی Xen مشاهده کنید: <div
                class="url">→ <a
                  href="http://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen">http://wiki.xenproject.org/wiki/Dom0_Kernels_for_Xen</a></div> <div
                class="url">→ <a
                  href="http://wiki.xenproject.org/wiki/DomU_Support_for_Xen">http://wiki.xenproject.org/wiki/DomU_Support_for_Xen</a></div>
			</div><div
              class="para">
				با این حال، اگر Xen بتواند روی قابلیت‌های سخت‌افزاری موجود برای مجازی‌سازی تکیه کند (که تنها در پردازنده‌های جدید مشاهده می‌شوند)، حتی سیستم عامل‌های غیر-قابل تغییر مانند ویندوز نیز می‌توانند به عنوان domU استفاده گردند.
			</div></div><div
            class="para">
				استفاده از Xen تحت دبیان نیازمند سه مولفه است:
			</div><div
            xmlns:d="http://docbook.org/ns/docbook"
            class="itemizedlist"><ul><li
                class="listitem"><div
                  class="para">
						خود hypervisor. با توجه به سخت‌افزار موجود، بسته مناسب آن یکی از گزینه‌های <span
                    class="pkg pkg">xen-hypervisor-4.4-amd64</span>، <span
                    class="pkg pkg">xen-hypervisor-4.4-armhf</span> یا <span
                    class="pkg pkg">xen-hypervisor-4.4-arm64</span> خواهد بود.
					</div></li><li
                class="listitem"><div
                  class="para">
						کرنلی که روی آن hypervisor اجرا می‌شود. هر کرنل جدیدتر از ۳.۰ اینکار را انجام می‌دهد، از جمله ۳.۱۶ موجود در <span
                    class="distribution distribution">Jessie</span>.
					</div></li><li
                class="listitem"><div
                  class="para">
						معماری i386 همچنین نیازمند یک کتابخانه استاندارد است که بتواند از اصلاحیه‌های موجود در Xen بهره‌مند شود؛ این کتابخانه در بسته <span
                    class="pkg pkg">libc6-xen</span> موجود است.
					</div></li></ul></div><div
            class="para">
				به منظور اینکه انتخاب این مولفه‌ها بدون دردسر انجام شود، چندین بسته کاربردی (از جمله <span
              class="pkg pkg">xen-linux-system-amd64</span>) ایجاد شده‌اند؛ این بسته‌ها با ترکیب خوبی از hypervisor مناسب و بسته‌های کرنل آن قرار گرفته‌اند. hypervisor همچنین شامل بسته <span
              class="pkg pkg">xen-utils-4.4</span> است که ابزار لازم برای کنترل آن از طریق dom0 را فراهم می‌آورد. این عمل در حقیقت کتابخانه استاندارد را نصب می‌کند. طی نصب این بسته‌ها، اسکریپت‌های پیکربندی همچنین یک مدخل درون منوی راه‌اندازی Grub ایجاد می‌کنند تا کرنل انتخابی برای آغاز dom0 مشخص گردد. به یاد داشته باشید که این مدخل معمولا به عنوان گزینه اول در فهرست قرار نمی‌گیرد، به همین منظور به صورت پیشفرض انتخاب نمی‌گردد. اگر این عملکرد مطلوب شما نباشد، دستورات زیر می‌توانند آن را تغییر دهند:
			</div><pre
            class="screen"><code
              class="computeroutput"># </code><strong
              class="userinput"><code>mv /etc/grub.d/20_linux_xen /etc/grub.d/09_linux_xen
</code></strong><code
              class="computeroutput"># </code><strong
              class="userinput"><code>update-grub
</code></strong></pre><div
            class="para">
				زمانی که این پیشنیازها نصب شوند، گام بعدی آزمایش عملکرد خود dom0 است؛ این عمل شامل راه‌اندازی مجدد hypervisor و کرنل Xen می‌باشد. سیستم باید به شیوه استاندارد خود راه‌اندازی شده، همراه با چندین پیام که طی گام‌های اولیه راه‌اندزای در کنسول نمایش می‌یابند.
			</div><div
            class="para">
				اکنون زمان آن فرا رسیده است که با استفاده از ابزار موجود در <span
              class="pkg pkg">xen-tools</span> به نصب سیستم‌های کاربردی روی domU بپردازیم. این بسته شامل دستور <code
              class="command">xen-create-image</code> می‌باشد که تقریبا این فرآیند را خودکارسازی می‌کند. تنها پارامتر ضروری آن <code
              class="literal">--hostname</code> است که نام domU را مشخص می‌کند؛ سایر گزینه‌ها نیز مهم هستند ولی می‌توانند درون فایل پیکربندی <code
              class="filename">/etc/xen-tools/xen-tools.conf</code> قرار بگیرند که نبود آن‌ها خطایی را در هنگام اجرای دستور صادر نمی‌کند. بنابراین مهم است که محتوای این فایل را قبل از ایجاد هر image بررسی کرده یا از پارامترهای اضافی هنگام فراخوانی <code
              class="command">xen-create-image</code> استفاده کنیم. پارامترهای مهم قابل ذکر عبارتند از:
			</div><div
            xmlns:d="http://docbook.org/ns/docbook"
            class="itemizedlist"><ul><li
                class="listitem"><div
                  class="para">
						<code
                    class="literal">--memory</code>، برای مشخص کردن میزان RAM اختصاص یافته به سیستم جدید؛
					</div></li><li
                class="listitem"><div
                  class="para">
						<code
                    class="literal">--size</code> و <code
                    class="literal">--swap</code>، برای تعریف اندازه "دیسک‌های مجازی" موجود برای domU؛
					</div></li><li
                class="listitem"><div
                  class="para">
						<code
                    class="literal">--debootstrap</code>، برای نصب شدن سیستم جدید با استفاده از <code
                    class="command">debootstrap</code>؛ در این مورد، از گزینه <code
                    class="literal">--dist</code> اغلب استفاده می‌شود (همراه با نام یک توزیع مانند <span
                    class="distribution distribution">jessie</span>).
					</div><div
                  class="sidebar"><div
                    class="titlepage"><div><div><p
                          class="title"><strong><span
                              class="emphasis"><em>مطالعه بیشتر</em></span> نصب یک سیستم غیر دبیان در یک domU</strong></p></div></div></div><div
                    class="para">
						در مورد یک سیستم غیر-لینوکس، برای تعریف کرنل مورد استفاده domU باید دقت کرد، با استفاده از گزینه <code
                      class="literal">--kernel</code>.
					</div></div></li><li
                class="listitem"><div
                  class="para">
						<code
                    class="literal">--dhcp</code> بیان می‌کند که پیکربندی شبکه domU باید از طریق DHCP انجام شود در صورتی که <code
                    class="literal">--ip</code> امکان استفاده از نشانی ایستای IP را فراهم می‌کند.
					</div></li><li
                class="listitem"><div
                  class="para">
						در انتها، از یک روش ذخیره‌سازی به منظور ایجاد image استفاده کرد (آن‌هایی که به عنوان درایوهای هارد دیسک از domU در نظر گرفته می‌شوند). ساده‌ترین روش، با توجه به گزینه <code
                    class="literal">--dir</code>، ایجاد یک فایل در dom0 به ازای هر دستگاه domU فراهم‌کننده آن است. برای سیستم‌هایی که از LVM استفاده می‌کنند، گزینه جایگزین استفاده از <code
                    class="literal">--lvm</code> است، که همراه با نام یک گروه آرایه آورده می‌شود؛ سپس <code
                    class="command">xen-create-image</code> اقدام به ایجاد یک گروه منطقی درون آرایه‌ها می‌کند که این گروه منطقی به عنوان یک درایو هارد دیسک به domU معرفی می‌گردد.
					</div><div
                  class="sidebar"><div
                    class="titlepage"><div><div><p
                          class="title"><strong><span
                              class="emphasis"><em>یادداشت</em></span> ذخیره‌سازی در domU</strong></p></div></div></div><div
                    class="para">
						علاوه بر پارتیشن‌ها، آرایه‌های RAID و گروه‌های منطقی موجود در LVM، هارد دیسک‌های کامل نیز می‌توانند به domU انتقال یابند. این عملیات توسط <code
                      class="command">xen-create-image</code> به صورت خودکار انجام نمی‌شوند، با این حال، ویرایش فایل پیکربندی Xen معمولا پس از فراخوانی اولیه <code
                      class="command">xen-create-image</code> صورت می‌گیرد.
					</div></div></li></ul></div><div
            class="para">
				زمانی که این انتخاب‌ّها صورت گیرد، می‌توانیم image خود را برای domU بعدی در Xen ایجاد کنیم:
			</div><pre
            class="screen"><code
              class="computeroutput"># </code><strong
              class="userinput"><code>xen-create-image --hostname testxen --dhcp --dir /srv/testxen --size=2G --dist=jessie --role=udev</code></strong>
<code
              class="computeroutput">
[...]
General Information
--------------------
Hostname       :  testxen
Distribution   :  jessie
Mirror         :  http://ftp.debian.org/debian/
Partitions     :  swap            128Mb (swap)
                  /               2G    (ext3)
Image type     :  sparse
Memory size    :  128Mb
Kernel path    :  /boot/vmlinuz-3.16.0-4-amd64
Initrd path    :  /boot/initrd.img-3.16.0-4-amd64
[...]
Logfile produced at:
         /var/log/xen-tools/testxen.log

Installation Summary
---------------------
Hostname        :  testxen
Distribution    :  jessie
MAC Address     :  00:16:3E:8E:67:5C
IP-Address(es)  :  dynamic
RSA Fingerprint :  0a:6e:71:98:95:46:64:ec:80:37:63:18:73:04:dd:2b
Root Password   :  adaX2jyRHNuWm8BDJS7PcEJ
</code></pre><div
            class="para">
				اکنون دارای یک ماشین مجازی هستیم که اجرا نمی‌شود (و تنها از فضای موجود در هارد دیسک dom0 استفاده می‌کند). البته که می‌توانیم با استفاده از پارامترهای گوناگون دیگر image بیشتری بسازیم.
			</div><div
            class="para">
				قبل از اینکه این ماشین‌های مجازی را روشن کنیم باید راجع به چگونگی دسترسی به آن‌ها تصمیم بگیریم. آن‌ها می‌توانند به عنوان ماشین‌های ایزوله شده تنها از طریق کنسول سیستم خود تعریف شوند، اما این روش به ندرت از الگوی کارکرد تبعیت می‌کند. اکثر مواقع، یک domU به عنوان یک سرور راه دور در نظر گرفته می‌شود که تنها از طریق یک شبکه قابل دسترسی است. با این حال، اختصاص یک کارت شبکه به هر domU ممکن است مناسب نباشد؛ به همین دلیل است که Xen امکان ایجاد رابط‌های مجازی که هر دامنه قادر به مشاهده و استفاده استاندارد از آن‌ها باشد را می‌دهد. به یاد داشته باشید که این کارت‌ها، با وجود مجازی بودن، تنها زمانی مفید هستند که به یک شبکه متصل گردند. Xen دارای چندین مدل شبکه برای این منظور است:
			</div><div
            xmlns:d="http://docbook.org/ns/docbook"
            class="itemizedlist"><ul><li
                class="listitem"><div
                  class="para">
						ساده‌ترین مدل <span
                    class="emphasis"><em>bridge</em></span> است؛ تمام کارت‌های شبکه eth0 (در سیستم‌های dom0 و domU) طوری عمل می‌کنند گویی به سوئیچ اصلی Ethernet متصل شده‌اند.
					</div></li><li
                class="listitem"><div
                  class="para">
						سپس مدل <span
                    class="emphasis"><em>routing</em></span> قرار دارد، به صورتی که dom0 به عنوان یک مسیریاب میان سیستم‌های domU و شبکه خارجی (فیزیکی) قرار می‌گیرد.
					</div></li><li
                class="listitem"><div
                  class="para">
						در نهایت، در مدل <span
                    class="emphasis"><em>NAT</em></span>، dom0 بین سیستم‌های domU و باقی شبکه قرار می‌گیرد، اما سیستم‌های domU به صورت مستقیم از خارج قابل دسترس نیستند و ترافیک از طریق ترجمه نشانی شبکه یا NAT با استفاده از dom0 انتقال می‌یابد.
					</div></li></ul></div><div
            class="para">
				این سه مدل شبکه دارای تعدادی رابط با نام‌های نامتعارف هستند از جمله <code
              class="filename">vif*</code>، <code
              class="filename">veth*</code>، <code
              class="filename">peth*</code> و <code
              class="filename">xenbr0</code>. hypervisor موجود در Xen با توجه به لایه تعریف شده آن‌ها را مرتب‌سازی می‌کند که این کار با استفاده از ابزارهای سمت-کاربر صورت می‌گیرد. از آنجا که مدل‌های NAT و routing تنها برای موارد خاص کاربرد دارند، تنها به بررسی مدل bridge می‌پردازیم.
			</div><div
            class="para">
				پیکربندی استاندارد بسته‌های Xen تغییری در پیکربندی شبکه در کل سیستم ایجاد نمی‌کند. با این حال، فرآیند پس‌زمینه <code
              class="command">xend</code> طوری پیکربندی شده است تا رابط‌های مجازی شبکه را با هر شبکه bridge از پیش موجود یکپارچه سازد (در صورت وجود چندین bridge گزینه <code
              class="filename">xenbr0</code> اولویت می‌یابد). برای اینکار نیازمند برپایی یک bridge در <code
              class="filename">/etc/network/interfaces</code> هستیم تا مدخل موجود eth0 جایگزین گردد (که نیازمند نصب بسته <span
              class="pkg pkg">bridge-utils</span> می‌باشد، به همین دلیل است که بسته <span
              class="pkg pkg">xen-utils-4.4</span> توصیه می‌شود):
			</div><pre
            class="programlisting">auto xenbr0
iface xenbr0 inet dhcp
    bridge_ports eth0
    bridge_maxwait 0
</pre><div
            class="para">
				پس از راه‌اندازی مجدد و اطمینان از اینکه bridge به طور خودکار ایجاد شده است، اکنون می‌توانیم domU را با استفاده از ابزار کنترلی Xen، به خصوص دستور <code
              class="command">xl</code>، آغاز کنیم. این دستور امکان چندین تغییر روی دامنه‌ها را همراه با فهرست‌سازی آن‌ها و آغاز/پایان هر کدام فراهم می‌آورد.
			</div><pre
            class="screen"><code
              class="computeroutput"># </code><strong
              class="userinput"><code>xl list</code></strong>
<code
              class="computeroutput">Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0   463     1     r-----      9.8
# </code><strong
              class="userinput"><code>xl create /etc/xen/testxen.cfg</code></strong>
<code
              class="computeroutput">Parsing config from /etc/xen/testxen.cfg
# </code><strong
              class="userinput"><code>xl list</code></strong>
<code
              class="computeroutput">Name                                        ID   Mem VCPUs      State   Time(s)
Domain-0                                     0   366     1     r-----     11.4
testxen                                      1   128     1     -b----      1.1</code></pre><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>ابزار</em></span> انتخاب جعبه ابزار برای مدیریت ماشین مجازی Xen</strong></p></div></div></div><a
              id="id-1.15.5.12.24.2"
              class="indexterm"></a><a
              id="id-1.15.5.12.24.3"
              class="indexterm"></a><div
              class="para">
				در دبیان ۷ و نسخه‌های قبل از آن، <code
                class="command">xm</code> ابزار خط فرمان مرجع برای مدیریت ماشین‌های مجازی Xen بود. اکنون با <code
                class="command">xl</code> جایگزین شده است که در اکثر موارد با آن سازگاری دارد. اما این گزینه‌ها تنها ابزار موجود برای اینکار نیستند: <code
                class="command">virsh</code> از libvirt و <code
                class="command">xe</code> از XAPI موجود در XenServer (بسته تجاری Xen) ابزارهای جایگزین هستند.
			</div></div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>احتیاط</em></span> تنها یک domU برای هر image!</strong></p></div></div></div><div
              class="para">
				از آنجا که امکان استفاده از چندین سیستم domU مختلف به صورت موازی وجود دارد، تمام آن‌ها نیازمند استفاده از image مخصوص به خود هستند، چرا که برای هر domU انتظار می‌رود که روی سخت‌افزار مختص به خود اجرا گردد (جدا از تکه بسیار کوچک کرنل که با hypervisor تعامل می‌کند). به طور مشخص، امکان استفاده همزمان از دو سیستم domU که از یک فضای ذخیره‌سازی اشتراکی بهره می‌برند وجود ندارد. اگر سیستم‌های domU در یک زمان واحد اجرا نشوند، تقریبا امکان استفاده از یک پارتیشن swap یا پارتیشنی که از فایل سیستم <code
                class="filename">/home</code> میزبانی می‌کند وجود دارد.
			</div></div><div
            class="para">
				به یاد داشته باشید که domU ایجاد شده بنام ... از حافظه واقعی گرفته شده از RAM که معمولا برای dom0 در نظر گرفته می‌شود، استفاده می‌کند و نه یک حافظه شبیه‌سازی شده. بنابراین هنگام راه‌اندازی یک سرور مبتی بر Xen باید دقت عمل در تخصیص حافظه به خرج داد.
			</div><div
            class="para">
				بسیار خوب! ماشین مجازی ما راه‌اندازی شد. دو روش دسترسی به آن وجود دارد. روش معمول اتصال ... به آن است، مانند یک ماشین حقیقی که از طریق شبکه متصل می‌شویم؛ اینکار نیازمند برپایی یک سرور DHCP یا پیکربندی DNS جداگانه است. روش دیگر، که در صورت اشتباه بودن پیکربندی شبکه می‌تواند تنها روش ممکن باشد، استفاده از کنسول <code
              class="filename">hvc0</code> همراه با دستور <code
              class="command">xl console</code> است:
			</div><pre
            class="screen"><code
              class="computeroutput"># </code><strong
              class="userinput"><code>xl console testxen</code></strong>
<code
              class="computeroutput">[...]

Debian GNU/Linux 8 testxen hvc0

testxen login: </code></pre><div
            class="para">
				در این حالت می‌توان یک نشست جداگانه برای دسترسی به ماشین مجازی ایجاد کرد. قطع اتصال این کنسول با استفاده از کلید ترکیبی <span
              class="keycap"><strong>Control</strong></span>+<span
              class="keycap"><strong>]</strong></span> انجام می‌شود.
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>نکته</em></span> دسترسی مستقیم به کنسول</strong></p></div></div></div><div
              class="para">
				بعضی وقت‌ها شاید بخواهیم پس از راه‌اندازی سیستم domU مستقیم وارد کنسول آن شویم؛ به همین دلیل است که دستور <code
                class="command">xl create</code> یک گزینه <code
                class="literal">-c</code> را نیز می‌پذیرد. آغاز یک domU همراه با این سوئیچ تمام پیام‌های موجود هنگام راه‌اندازی آن را نمایش می‌دهد.
			</div></div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>ابزار</em></span> OpenXenManager</strong></p></div></div></div><div
              class="para">
				OpenXenManager (در بسته <span
                class="pkg pkg">openxenmanager</span>) یک ابزار گرافیکی است که امکان مدیریت دامنه‌های Xen را با استفاده از API آن فراهم می‌کند. همچنین امکان کنترل دامنه‌های Xen از راه‌دور نیز وجود دارد. این بسته تقریبا تمام ویژگی‌های دستور <code
                class="command">xl</code> را فراهم می‌کند.
			</div></div><div
            class="para">
				زمانی که domU آغاز گردد، می‌تواند مانند هر سرور دیگری مورد استفاده قرار گیرد (چرا که یک سیستم گنو/لینوکس است). اگرچه، وضعیت ماشین مجازی آن برخی قابلیت‌های دیگر را فراهم می‌کند. برای نمونه، یک domU با استفاده از دستورات <code
              class="command">xl pause</code> و <code
              class="command">xl unpause</code> می‌تواند به صورت موقت متوقف شده یا ادامه یابد. به یاد داشته باشید که یک domU متوقف شده با اینکه از قدرت پردازنده استفاده نمی‌کند، اما هم اکنون حافظه اختصاص یافته به خود را دارد. استفاده از دستورات <code
              class="command">xl save</code> و <code
              class="command">xl restore</code> نیز قابل توجه است: ذخیره‌سازی یک domU تمام منابع اختصاص یافته به آن، از جمله RAM، را آزادسازی می‌کند. در زمان بازگرداندن (یا ادامه، به این منظور) یک domU چیزی به جز گذشت زمان را احساس نمی‌کند. اگر هنگام خاموش کردن dom0 یک domU در حال اجرا باشد، اسکریپت‌های پیکربندی به صورت خودکار domU را ذخیره‌سازی کرده تا در راه‌اندازی بعدی از سر گرفته شود. این عمل البته ناملایمت‌های عملیات hibernate کردن یک رایانه لپ‌تاپ را به همراه دارد، برای نمونه؛ به طور مشخص اگر domU به مدت زمان طولانی در حالت تعلیق باشد، ارتباطات شبکه ممکن است منقضی گردند. به یاد داشته باشید که Xen به شدت ناسازگار با بخش مدیریت قدرت ACPI است، که عملیات متوقف‌سازی سیستم میزبان (dom0) را انجام می‌دهد.
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>مستندات</em></span> گزینه‌های <code
                        class="command">xl</code></strong></p></div></div></div><div
              class="para">
				اغلب زیردستورات <code
                class="command">xl</code> شامل یک یا چند آرگومان هستند، که بیشتر شامل نام یک domU می‌شود. این آرگومان‌ها به خوبی در صفحه راهنمای <span
                class="citerefentry"><span
                  class="refentrytitle">xl</span>(1)</span> توضیح داده شده‌اند.
			</div></div><div
            class="para">
				متوقف‌سازی یا راه‌اندازی مجدد یک domU می‌تواند از طریق خود آن (با دستور <code
              class="command">shutdown</code>) یا از طریق dom0 با استفاده از <code
              class="command">xl shutdown</code> یا <code
              class="command">xl reboot</code> انجام شود.
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>مطالعه بیشتر</em></span> Xen پیشرفته</strong></p></div></div></div><div
              class="para">
				Xen دارای قابلیت‌های بسیاری است که نمی‌توان در چند پاراگراف به آن‌ها اشاره کرد. به طور مشخص، سیستم بسیار پویا است و امکان تنظیم چندین پارامتر برای یک دامنه هنگام اجرای آن وجود دارد (از جمله میزان حافظه اختصاص یافته، هارد درایوهای قابل مشاهده، رفتار زمان‌بند وظایف و از این قبیل). یک domU حتی می‌تواند بین چندین سرور انتقال یابد به گونه‌‌ای که نه خاموش گردد و نه ارتباط شبکه را از دست دهد! برای اطلاع از تمام این جنبه‌های پیشرفته، منبع اولیه اطلاعات، مستندات رسمی Xen است. <div
                class="url">→ <a
                  href="http://www.xen.org/support/documentation.html">http://www.xen.org/support/documentation.html</a></div>
			</div></div></div><div
          class="section"><div
            class="titlepage"><div><div><h3
                  class="title"><a
                    id="sect.lxc"></a>12.2.2. LXC</h3></div></div></div><a
            id="id-1.15.5.13.2"
            class="indexterm"></a><div
            class="para">
				با اینکه از آن برای ساخت “ماشین‌های مجازی” استفاده می‌شود، LXC به طور دقیق یک سیستم مجازی‌سازی نیست، بلکه سیستمی برای جدا کردن گروهی از فرآیندها نسبت به یکدیگر می‌باشد که درون یک میزبان اجرا می‌شوند. این سیستم از پیشرفت‌های اخیر در کرنل لینوکس بهره می‌برد، که بنام <span
              class="emphasis"><em>گروه‌های کنترل</em></span> شناخته می‌شوند، به این معنی که مجموعه‌های مختلف از فرآیندها که “گروه” نامیده می‌شوند دید متفاوتی نسبت به جنبه‌های کلی سیستم دارند. از جمله این جنبه‌ها می‌توان به شناسه‌های فرآیند، پیکربندی شبکه و نقاط اتصال اشاره کرد. چنین گروهی از فرآیندهای ایزوله‌شده هیچ گونه دسترسی دیگر به سایر فرآیندهای سیستم ندارند و دسترسی آن‌ها به فایل سیستم تنها محدود به مجموعه‌ای کوچک می‌گردد. از این رو می‌تواند رابط شبکه و جدول مسیریابی مربوط به خود را داشته باشد و می‌تواند طوری پیکربندی شود که تنها مجموعه کوچکی از دستگاه‌های موجود در سیستم را مشاهده کند.
			</div><div
            class="para">
				این ویژگی‌ها می‌توانند به منظور جدا کردن خانواده فرآیند آغازی توسط <code
              class="command">init</code> با یکدیگر ترکیب شده که نتیجه نهایی آن مشابه با ماشین مجازی است. نام رسمی چنین تنظیمی “مخزن” است (با توجه به نام LXC که برابر است با: <span
              class="emphasis"><em>LinuX Containers</em></span>) اما تفاوت عمده آن با ماشین‌های مجازی “واقعی” مانند Xen یا KVM در نبود یک کرنل دوم است؛ مخزن از همان کرنل سیستم میزبان استفاده می‌کند. اینکار مزایا و معایب خود را دارد: مزایای آن شامل عملکرد فوق‌العاده به دلیل نبود overhead و این حقیقت که کرنل یک دید سراسری نسبت به تمام فرآیندهای اجرایی روی سیستم دارد، به این منظور که عملیات زمان‌بندی می‌تواند به شیوه‌ای موثرتر انجام شود نسبت به حالتی که دو کرنل جداگانه باید مجموعه‌‌های مختلف از وظایف را مدیریت می‌کردند. از میان معایت نیز می‌توان به غیرممکن بودن اجرای یک کرنل مختلف درون یک مخزن اشاره کرد (خواه یک نسخه متفاوت لینوکس خواه یک سیستم عامل دیگر).
			</div><div
            class="sidebar"><div
              class="titlepage"><div><div><p
                    class="title"><strong><span
                        class="emphasis"><em>یادداشت</em></span> محدودیت‌های انزوای LXC</strong></p></div></div></div><div
              class="para">
				مخازن LXC سطحی از انزوا را مانند شبیه‌سازهای قدرتمند یا مجازی‌سازهای دیگر فراهم نمی‌کنند. به طور مشخص:
			</div><div
              xmlns:d="http://docbook.org/ns/docbook"
              class="itemizedlist"><ul><li
                  class="listitem"><div
                    class="para">
						از آنجا که کرنل بین سیستم میزبان و مخازن مشترک است، فرآیندهایی که محدود به مخازن هستند کماکان می‌توانند به پیام‌های کرنل دسترسی داشته باشند که در صورت انتشار پیام‌ها توسط یک مخزن می‌تواند منجر به افشای اطلاعات شود؛
					</div></li><li
                  class="listitem"><div
                    class="para">
						به دلیل مشابه، اگر به یک مخزن نفوذ شود و از یک آسیب‌پذیری کرنل استفاده گردد، سایر مخازن نیز تاثیر منفی می‌پذیرند؛
					</div></li><li
                  class="listitem"><div
                    class="para">
						در فایل سیستم، کرنل به بررسی مجوزهای کاربران و گروه‌ها مبتنی بر شناسه‌های عددی می‌پردازد؛ این شناسه‌ها ممکن است به کاربران و گروه‌های مختلف با توجه به مخزن اختصاص یابند که در صورت اشتراکی بودن قسمت‌های قابل نوشتن فایل سیستم بین مخازن باید مورد توجه قرار گیرد.
					</div></li></ul></div></div><div
            class="para">
				از آنجا که با مفهوم ایزوله کردن و نه یک راهکار مجازی‌سازی ساده سروکار داریم، برپایی مخازن LXC بسیار پیچیده‌تر از اجرای debian-installer در یک ماشین مجازی است. ابتدا چندین پیشنیاز را بررسی کرده سپس به قسمت پیکربندی شبکه می‌رویم؛ در این قسمت است که می‌توانیم سیستم را درون یک مخزن اجرا کنیم.
			</div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      id="id-1.15.5.13.7"></a>12.2.2.1. گام‌های مقدماتی</h4></div></div></div><div
              class="para">
					بسته <span
                class="pkg pkg">lxc</span> شامل ابزار مورد نیاز برای نصب و اجرای LXC است.
				</div><div
              class="para">
					LXC همچنین به سیستم پیکربندی <span
                class="emphasis"><em>control groups</em></span> نیاز دارد که به صورت یک فایل سیستم مجازی به <code
                class="filename">/sys/fs/cgroup</code> متصل می‌شود. از آنجا که دبیان ۸ به systemd روی آورده، که خود مبتنی بر گروه‌های کنترل است، اینکار در زمان راه‌اندازی سیستم بدون هیچ پیکربندی خاص صورت می‌گیرد.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      id="sect.lxc.network"></a>12.2.2.2. پیکربندی شبکه</h4></div></div></div><div
              class="para">
					هدف از نصب LXC برپایی ماشین‌های مجازی است؛ با اینکه می‌توانیم آن‌ها را به صورت ایزوله در شبکه قرار دهیم و تنها از طریق فایل سیستم با آن‌ها تعامل کنیم، اکثر موارد کاربردی شامل دسترسی حداقلی شبکه به مخازن است. در حالت معمولی، به هر مخزن یک رابط مجازی شبکه اختصاص می‌یابد که از طریق bridge به یک رابط حقیقی شبکه متصل است. این رابط مجازی هم می‌تواند به رابط فیزیکی میزبان (که در این صورت مخزن به صورت مستقیم در شبکه قرار می‌گیرد) هم می‌تواند به رابط مجازی دیگری در میزبان متصل شود (که میزبان کار فیلتر و مسیریابی ترافیک را انجام می‌دهد). در هر صورت، بسته <span
                class="pkg pkg">bridge-utils</span> مورد نیاز خواهد بود.
				</div><div
              class="para">
					مورد اول به سادگی ویرایش فایل <code
                class="filename">/etc/network/interfaces</code>، انتقال پیکربندی برای رابط فیزیکی (برای نمونه <code
                class="literal">eth0</code>) به رابط bridge (معمولا <code
                class="literal">br0</code>) و پیکربندی پیوند بین آن‌ها است. برای نمونه، اگر فایل پیکربندی رابط شبکه شامل مدخل‌های زیر باشد:
				</div><pre
              class="programlisting">auto eth0
iface eth0 inet dhcp</pre><div
              class="para">
					آن‌ها باید غیرفعال شده و با مدخل‌های زیر جایگزین گردند:
				</div><pre
              class="programlisting">#auto eth0
#iface eth0 inet dhcp

auto br0
iface br0 inet dhcp
  bridge-ports eth0</pre><div
              class="para">
					تاثیر این پیکربندی مشابه با حالتی خواهد بود که مخازن به صورت ماشین‌هایی به شبکه فیزیکی یکسانی از طریق میزبان متصل می‌شدند. پیکربندی “bridge” انتقال فریم‌های Ethernet را بین تمام رابط‌های bridged مدیریت می‌کند که شامل <code
                class="literal">eth0</code> همراه با رابط‌های تعریف شده برای مخازن می‌باشد.
				</div><div
              class="para">
					در مواری که این پیکربندی نمی‌تواند استفاده شود (برای نمونه اگر هیچ نشانی عمومی IP نتواند به مخازن اختصاص یابد)، یک رابط مجازی <span
                class="emphasis"><em>tap</em></span> ایجاد و به bridge متصل می‌شود. معادل توپولوژی شبکه سپس به میزبانی با یک کارت شبکه ثانویه تبدیل شده که به یک سوئیچ جداگانه متصل است، همراه با مخازن متصل به آن سوئیچ. میزبان باید به صورت یک gateway برای مخازنی عمل کند که قصد ارتباط با دنیای خارج را دارند.
				</div><div
              class="para">
					علاوه بر <span
                class="pkg pkg">bridge-utils</span>، این پیکربندی “غنی” نیازمند بسته <span
                class="pkg pkg">vde2</span> است؛ فایل <code
                class="filename">/etc/network/interfaces</code> سپس به صورت زیر در می‌آید:
				</div><pre
              class="programlisting"># Interface eth0 is unchanged
auto eth0
iface eth0 inet dhcp

# Virtual interface 
auto tap0
iface tap0 inet manual
  vde2-switch -t tap0

# Bridge for containers
auto br0
iface br0 inet static
  bridge-ports tap0
  address 10.0.0.1
  netmask 255.255.255.0</pre><div
              class="para">
					شبکه می‌تواند یا به صورت ایستا درون مخازن یا به صورت پویا از طریق سرور DHCP درون میزبان برپا شود. چنین سرور DHCP باید طوری پیکربندی شود که به پرس و جوهای موجود در رابط <code
                class="literal">br0</code> پاسخ دهد.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      id="id-1.15.5.13.9"></a>12.2.2.3. برپایی سیستم</h4></div></div></div><div
              class="para">
					اکنون بیاید فایل سیستم مورد نیاز مخزن را برپا کنیم. از آنجا که این “ماشین مجازی” به صورت مستقیم روی سخت‌افزار اجرا نخواهد شد، در مقایسه با یک فایل سیستم استاندارد رعایت برخی نکات ضروری است، به خصوص تا آنجا که به کرنل، دستگاه‌ها و کنسول‌ها مربوط باشد. خوشبختانه <span
                class="pkg pkg">lxc</span> شامل اسکریپت‌هایی است که اکثر این پیکربندی را به صورت خودکار انجام می‌دهند. برای نمونه، دستورات پیش رو (که نیازمند بسته‌های <span
                class="pkg pkg">debootstrap</span> و <span
                class="pkg pkg">rsync</span> هستند) اقدام به نصب یک مخزن دبیان می‌کنند:
				</div><pre
              class="screen"><code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>lxc-create -n testlxc -t debian
</code></strong><code
                class="computeroutput">debootstrap is /usr/sbin/debootstrap
Checking cache download in /var/cache/lxc/debian/rootfs-jessie-amd64 ... 
Downloading debian minimal ...
I: Retrieving Release 
I: Retrieving Release.gpg 
[...]
Download complete.
Copying rootfs to /var/lib/lxc/testlxc/rootfs...
[...]
Root password is 'sSiKhMzI', please change !
root@mirwiz:~# </code>
</pre><div
              class="para">
					به یاد داشته باشید که فایل سیستم به صورت اولیه در <code
                class="filename">/var/cache/lxc</code> ایجاد شد، سپس به مقصد خود انتقال یافت. اینکار امکان ایجاد مخازن مشابه را با سرعت بیشتری فراهم می‌کند، چرا که تنها عملیات رونوشت‌گیری مورد نیاز است.
				</div><div
              class="para">
					به یاد داشته باشید که اسکریپت ایجاد قالب دبیان یک گزینه <code
                class="option">--arch</code> به منظور تعیین معماری سیستم و یک گزینه <code
                class="option">--release</code> به منظور نصب نسخه‌ای بجز نسخه انتشار اصلی از دبیان را قبول می‌کند. همچنین می‌توانید با استفاده از متغیر محیطی <code
                class="literal">MIRROR</code> از یک mirror مخصوص به دبیان استفاده کنید.
				</div><div
              class="para">
					فایل سیستم تازه ایجاد شده اکنون شامل یک سیستم پایه دبیان است و مخزن پیشفرض آن هیچ رابط شبکه‌ای ندارد (بجز گزینه loopback). از آنجا که این مورد نظر ما نیست، به ویرایش فایل پیکربندی مخزن (<code
                class="filename">/var/lib/lxc/testlxc/config</code>) پرداخته و چندین مدخل <code
                class="literal">lxc.network.*</code> را در آن ایجاد می‌کنیم:
				</div><pre
              class="programlisting">lxc.network.type = veth
lxc.network.flags = up
lxc.network.link = br0
lxc.network.hwaddr = 4a:49:43:49:79:20</pre><div
              class="para">
					وجود این مدخل‌ها به این معنی است که یک رابط مجازی برای مخزن ایجاد خواهد شد؛ که به صورت خودکار هنگام آغاز مخزن شروع می‌شوند؛ که به صورت خودکار به bridge موجود در میزبان بنام <code
                class="literal">br0</code> متصل می‌شوند؛ که نشانی MAC آن مطابق بالا خواهد بود. در صورت فقدان یا غیرفعال بودن این گزینه آخر، از یک نشانی MAC تصادفی استفاده خواهد شد.
				</div><div
              class="para">
					یک مدخل مفید دیگر در آن فایل تنظیم نام میزبان است:
				</div><pre
              class="programlisting">lxc.utsname = testlxc</pre></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      id="id-1.15.5.13.10"></a>12.2.2.4. آغاز مخزن</h4></div></div></div><div
              class="para">
					اکنون که image ماشین مجازی آماده است، بیایید مخزن را آغاز کنیم:
				</div><pre
              class="screen scale"
              width="94"><code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>lxc-start --daemon --name=testlxc
</code></strong><code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>lxc-console -n testlxc
</code></strong><code
                class="computeroutput">Debian GNU/Linux 8 testlxc tty1

testlxc login: </code><strong
                class="userinput"><code>root</code></strong><code
                class="computeroutput">
Password: 
Linux testlxc 3.16.0-4-amd64 #1 SMP Debian 3.16.7-ckt11-1 (2015-05-24) x86_64

The programs included with the Debian GNU/Linux system are free software;
the exact distribution terms for each program are described in the
individual files in /usr/share/doc/*/copyright.

Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent
permitted by applicable law.
root@testlxc:~# </code><strong
                class="userinput"><code>ps auxwf</code></strong>
<code
                class="computeroutput">USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.2  28164  4432 ?        Ss   17:33   0:00 /sbin/init
root        20  0.0  0.1  32960  3160 ?        Ss   17:33   0:00 /lib/systemd/systemd-journald
root        82  0.0  0.3  55164  5456 ?        Ss   17:34   0:00 /usr/sbin/sshd -D
root        87  0.0  0.1  12656  1924 tty2     Ss+  17:34   0:00 /sbin/agetty --noclear tty2 linux
root        88  0.0  0.1  12656  1764 tty3     Ss+  17:34   0:00 /sbin/agetty --noclear tty3 linux
root        89  0.0  0.1  12656  1908 tty4     Ss+  17:34   0:00 /sbin/agetty --noclear tty4 linux
root        90  0.0  0.1  63300  2944 tty1     Ss   17:34   0:00 /bin/login --     
root       117  0.0  0.2  21828  3668 tty1     S    17:35   0:00  \_ -bash
root       268  0.0  0.1  19088  2572 tty1     R+   17:39   0:00      \_ ps auxfw
root        91  0.0  0.1  14228  2356 console  Ss+  17:34   0:00 /sbin/agetty --noclear --keep-baud console 115200 38400 9600 vt102
root       197  0.0  0.4  25384  7640 ?        Ss   17:38   0:00 dhclient -v -pf /run/dhclient.eth0.pid -lf /var/lib/dhcp/dhclient.e
root       266  0.0  0.1  12656  1840 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty5 linux
root       267  0.0  0.1  12656  1928 ?        Ss   17:39   0:00 /sbin/agetty --noclear tty6 linux
root@testlxc:~# </code></pre><div
              class="para">
					اکنون درون مخزن هستیم؛ دسترسی به فرآیندها تنها محدود به آن‌هایی است که توسط مخزن آغاز شده باشند و دسترسی به فایل سیستم تنها بخش کوچکی از فایل سیستم کامل <code
                class="filename">/var/lib/lxc/testlxc/rootfs</code> را شامل می‌شود. با استفاده از کلید ترکیبی <span
                class="keycap"><strong>Control</strong></span>+<span
                class="keycap"><strong>a</strong></span> <span
                class="keycap"><strong>q</strong></span> می‌توانیم از کنسول خارج شویم.
				</div><div
              class="para">
					به یاد داشته باشید که مخزن را به عنوان یک فرآیند پس‌زمینه اجرا کردیم، به لطف گزینه <code
                class="option">--daemon</code> از <code
                class="command">lxc-start</code>. با استفاده از دستور <code
                class="command">lxc-stop --name=testlxc</code> می‌توانیم مخزن را متوقف کنیم.
				</div><div
              class="para">
					بسته <span
                class="pkg pkg">lxc</span> شامل یک اسکریپت راه‌اندازی است که به صورت خودکار یک یا چند مخزن را در زمان راه‌اندازی میزبان آغاز می‌کند (مبتنی بر <code
                class="command">lxc-autostart</code> است که به صورت خودکار مخازن شامل گزینه <code
                class="literal">lxc.start.auto</code> برابر ۱ را راه‌اندازی می‌کند). با استفاده از <code
                class="literal">lxc.start.order</code> و <code
                class="literal">lxc.group</code> می‌توان کنترل بیشتری روی ترتیب اجرای مخازن اعمال کرد: به صورت پیشفرض، اسکریپت راه‌اندازی ابتدا مخازنی را آغاز می‌کند که جزو گروه <code
                class="literal">onboot</code> باشند سپس به سراغ مخازن دیگر می‌رود). در هر دو مورد، ترتیب درون هر گروه توسط گزینه <code
                class="literal">lxc.start.order</code> مشخص می‌شود.
				</div><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>مطالعه بیشتر</em></span> مجازی‌سازی انبوه</strong></p></div></div></div><div
                class="para">
					از آنجا که LXC یک سیستم ایزوله‌کردن سبک به حساب می‌آید، می‌تواند به منظور میزبانی از سرورهای مجازی انبوه سازگار شود. پیکربندی شبکه از آنچه در این قسمت توضیح دادیم به مراتب پیچیده‌تر خواهد بود اما پیکربندی “غنی” با استفاده از رابط‌های <code
                  class="literal">tap</code> و <code
                  class="literal">veth</code> در اکثر موارد به شیوه توضیح داده شده کافی خواهد بود.
				</div><div
                class="para">
					به منظور پیشگیری از نصب مجدد نرم‌افزارهایی که برای چندین مخزن کاربردی هستند، معقول بنظر می‌رسد که قسمتی از فایل سیستم مانند <code
                  class="filename">/usr</code> و <code
                  class="filename">/lib</code> را به اشتراک بگذاریم. اینکار معمولا با مدخل‌های <code
                  class="literal">lxc.mount.entry</code> درون فایل پیکربندی مخازن انجام می‌شود. یک تاثیر جانبی جالب این است که فرآیندها از حافظه فیزیکی کمتری استفاده می‌کنند، چرا که کرنل قادر به تشخیص برنامه‌هایی است که به صورت اشتراکی کار می‌کنند. هزینه حاشیه‌ای یک مخزن اضافی دیگر می‌تواند به فضای دیسک اختصاص یافته به داده خاص و چند فرآیند اضافی که کرنل برای زمان‌بندی و مدیریت استفاده می‌کند، کاهش یابد.
				</div><div
                class="para">
					البته، تمام گزینه‌های موجود را بررسی نکردیم؛ اطلاعات جامع بیشتر از طریق صفحات راهنمای <span
                  class="citerefentry"><span
                    class="refentrytitle">lxc</span>(7)</span> و <span
                  class="citerefentry"><span
                    class="refentrytitle">lxc.container.conf</span>(5)</span> همراه با سایر مراجع آن قابل دسترس است.
				</div></div></div></div><div
          class="section"><div
            class="titlepage"><div><div><h3
                  class="title"><a
                    id="id-1.15.5.14"></a>12.2.3. مجازی‌سازی با KVM</h3></div></div></div><a
            id="id-1.15.5.14.2"
            class="indexterm"></a><div
            class="para">
				KVM، که مخفف عبارت <span
              class="emphasis"><em>Kernel-based Virtual Machine</em></span> است، در درجه اول یک افزونه کرنل به حساب می‌آید که اکثر زیرساخت مورد نیاز یک مجازی‌ساز را فراهم می‌کند اما خود یک مجازی‌ساز نیست. کنترل واقعی مجازی‌سازی توسط برنامه‌ای مبتنی بر QEMU انجام می‌شود. نگران نباشید اگر در این قسمت دستورات مربوط به <code
              class="command">qemu-*</code> را مشاهده کنید: تمام آن‌ها مرتبط با KVM هستند.
			</div><div
            class="para">
				برخلاف سایر سیستم‌های مجازی‌سازی، KVM از ابتدا درون کرنل لینوکس قرار گرفت. توسعه‌دهندگان آن تصمیم گرفتند از مجموعه دستورالعمل‌های پردازنده برای مجازی‌سازی (Intel-VT و AMD-V) استفاده کنند که اینکار باعث می‌شود KVM سبک، ظریف و سازگار با منابع پایین باشد. نقطه ضعف آن این است که KVM روی هر رایانه‌ای نمی‌تواند اجرا شود بلکه فقط برخی پردازنده‌های خاص از آن پشتیبانی می‌کنند. برای رایانه‌های مبتنی بر x86، می‌توانید به دنبال پرچم‌های مخصوص پردازنده به نام “vmx” یا “svm” در فایل <code
              class="filename">/proc/cpuinfo</code> بگردید.
			</div><div
            class="para">
				با پشتیبانی مداوم Red Hat از توسعه آن، KVM کم و بیش به مرجع مجازی‌سازی در لینوکس تبدیل شده است.
			</div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      id="id-1.15.5.14.6"></a>12.2.3.1. گام‌های مقدماتی</h4></div></div></div><a
              id="id-1.15.5.14.6.2"
              class="indexterm"></a><div
              class="para">
					KVM برخلاف ابزاری مانند VirtualBox، شامل رابط کاربری برای مدیریت ماشین‌های مجازی نیست. بسته <span
                class="pkg pkg">qemu-kvm</span> تنها شامل یک فایل اجرایی برای آغاز یک ماشین مجازی است، همراه با اسکریپت‌های راه‌اندازی که اقدام به بارگیری افزونه‌های مناسب کرنل می‌کنند.
				</div><a
              id="id-1.15.5.14.6.4"
              class="indexterm"></a><a
              id="id-1.15.5.14.6.5"
              class="indexterm"></a><div
              class="para">
					خوشبختانه، Red Hat برای غلبه بر این مشکل مجموعه ابزاری فراهم کرده است که شامل کتابخانه <span
                class="emphasis"><em>libvirt</em></span> و ابزارهای <span
                class="emphasis"><em>virtual machine manager</em></span> می‌شوند. libvirt امکان مدیریت ماشین‌های مجازی را به یک شیوه یکسان فراهم می‌کند، جدا از سیستم مجازی‌سازی که در پشت صحنه قرار دارد (هم اکنون از QEMU، KVM، Xen، LXC، OpenVZ، VirtualBox، VMWare و UML پشتیبانی می‌کند). <code
                class="command">virtual-manager</code> یک رابط گرافیکی است که با استفاده از libvirt ماشین‌های مجازی را مدیریت می‌کند.
				</div><a
              id="id-1.15.5.14.6.7"
              class="indexterm"></a><div
              class="para">
					ابتدا با استفاده از <code
                class="command">apt-get install qemu-kvm libvirt-bin virtinst virt-manager virt-viewer</code> بسته‌های مورد نیاز را نصب می‌کنیم. <span
                class="pkg pkg">libvirt-bin</span> فرآیند پس‌زمینه <code
                class="command">libvirtd</code> را فراهم می‌کند، که امکان مدیریت (معمولا راه دور) ماشین‌های مجازی اجرای در سیستم میزبان را فراهم کرده و ماشین‌های مجازی مورد نیاز را در زمان راه‌اندازی میزبان آغاز می‌کند. علاوه بر این، این بسته ابزار خط-فرمان <code
                class="command">virsh</code> را فراهم می‌کند که امکان کنترل ماشین‌های <code
                class="command">libvirtd</code> را بوجود می‌آورد.
				</div><div
              class="para">
					بسته <span
                class="pkg pkg">virtinst</span> شامل <code
                class="command">virt-install</code> می‌شود که امکان ایجاد ماشین‌های مجازی از خط فرمان را فراهم می‌کند. در نهایت، <span
                class="pkg pkg">virt-viewer</span> اجازه دسترسی به کنسول گرافیکی یک ماشین مجازی را بوجود می‌آورد.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      id="id-1.15.5.14.7"></a>12.2.3.2. پیکربندی شبکه</h4></div></div></div><div
              class="para">
					درست مانند Xen و LXC، متداول‌ترین پیکربندی شبکه شامل یک bridge که رابط‌های شبکه ماشین‌های مجازی را گروه‌بندی می‌کند، می‌باشد (<a
                class="xref"
                href="sect.virtualization.html#sect.lxc.network">
      قسمت 12.2.2.2, “پیکربندی شبکه”
    </a> را مشاهده کنید).
				</div><div
              class="para">
					به طور متقابل، در پیکربندی پیشفرض فراهم شده توسط KVM، یک نشانی خصوصی به ماشین مجازی اختصاص می‌یابد (در محدوده 192.168.122.0/24) و NAT طوری تنظیم می‌شود که ماشین مجازی بتواند به شبکه خارجی دسترسی داشته باشد.
				</div><div
              class="para">
					باقیمانده این قسمت با توجه به اینکه میزبان دارای یک رابط فیزیکی <code
                class="literal">eth0</code> و bridge <code
                class="literal">br0</code> است ادامه می‌یابد، به طوری که اولی به دومی متصل شده است.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      id="id-1.15.5.14.8"></a>12.2.3.3. نصب با <code
                      class="command">virt-install</code></h4></div></div></div><a
              id="id-1.15.5.14.8.2"
              class="indexterm"></a><div
              class="para">
					ایجاد یک ماشین مجازی بسیار شبیه یک سیستم عادی است، با این تفاوت که ویژگی‌های ماشین مجازی به صورت گزینه‌های بی‌پایان در خط فرمان قرار می‌گیرند.
				</div><div
              class="para">
					در عمل، یعنی از یک نصب کننده دبیان استفاده خواهیم کرد، با راه‌اندازی ماشین مجازی روی یک درایو DVD-ROM که به یک تصویر از DVD دبیان ذخیره شده در سیستم میزبان نگاشت شده است. ماشین مجازی از طریق پروتکل VNC کنسول گرافیکی خود را آماده می‌کند (<a
                class="xref"
                href="sect.remote-login.html#sect.remote-desktops">
      قسمت 9.2.2, “استفاده از میزکارهای گرافیکی راه‌دور”
    </a> را مشاهده کنید) که اینکار به ما اجازه می‌دهد فرآیند نصب را کنترل کنیم.
				</div><div
              class="para">
					ابتدا باید به libvirtd بگوییم تصاویر دیسک را در کجا ذخیره کند، مگر مکان پیشفرض <code
                class="filename">/var/lib/libvirt/images/</code> مناسب باشد.
				</div><pre
              class="screen"><code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>mkdir /srv/kvm</code></strong>
<code
                class="computeroutput">root@mirwiz:~# </code><strong
                class="userinput"><code>virsh pool-create-as srv-kvm dir --target /srv/kvm</code></strong>
<code
                class="computeroutput">Pool srv-kvm created

root@mirwiz:~# </code></pre><div
              class="sidebar"><div
                class="titlepage"><div><div><p
                      class="title"><strong><span
                          class="emphasis"><em>نکته</em></span> افزودن کاربر خود به گروه libvirt</strong></p></div></div></div><div
                class="para">
					تمام نمونه‌های این قسمت فرض را بر این می‌گذارند که شما به عنوان root دستورات را اجرا می‌کنید. اگر قصد کنترل یک libvirt محلی را دارید، یا باید root باشید یا عضوی از گروه <code
                  class="literal">libvirt</code> (که به صورت پیشفرض فعال نیست). بنابراین به منظور جلوگیری از اجرای تمام دستورات به عنوان root می‌توانید با افزودن کاربر خود به گروه <code
                  class="literal">libvirt</code> تمام دستورات آن را تحت مجوز کاربری خود اجرا کنید.
				</div></div><div
              class="para">
					اکنون بیایید فرآیند نصب ماشین مجازی را آغاز کرده و نگاهی بر مهم‌ترین گزینه‌های <code
                class="command">virt-install</code> بیندازیم. این دستور، ماشین مجازی و پارامترهای آن را در libvirtd ثبت می‌کند سپس به اجرای آن پرداخته تا فرآیند نصب ادامه یابد.
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>virt-install --connect qemu:///system  <span
                    id="virtinst.connect"><img
                      class="callout"
                      src="Common_Content/images/1.png"
                      alt="1" /></span>
               --virt-type kvm           <span
                    id="virtinst.type"><img
                      class="callout"
                      src="Common_Content/images/2.png"
                      alt="2" /></span>
               --name testkvm            <span
                    id="virtinst.name"><img
                      class="callout"
                      src="Common_Content/images/3.png"
                      alt="3" /></span>
               --ram 1024                <span
                    id="virtinst.ram"><img
                      class="callout"
                      src="Common_Content/images/4.png"
                      alt="4" /></span>
               --disk /srv/kvm/testkvm.qcow,format=qcow2,size=10 <span
                    id="virtinst.disk"><img
                      class="callout"
                      src="Common_Content/images/5.png"
                      alt="5" /></span>
               --cdrom /srv/isos/debian-8.1.0-amd64-netinst.iso  <span
                    id="virtinst.cdrom"><img
                      class="callout"
                      src="Common_Content/images/6.png"
                      alt="6" /></span>
               --network bridge=br0      <span
                    id="virtinst.network"><img
                      class="callout"
                      src="Common_Content/images/7.png"
                      alt="7" /></span>
               --vnc                     <span
                    id="virtinst.vnc"><img
                      class="callout"
                      src="Common_Content/images/8.png"
                      alt="8" /></span>
               --os-type linux           <span
                    id="virtinst.os"><img
                      class="callout"
                      src="Common_Content/images/9.png"
                      alt="9" /></span>
               --os-variant debianwheezy
</code></strong><code
                class="computeroutput">
Starting install...
Allocating 'testkvm.qcow'             |  10 GB     00:00
Creating domain...                    |    0 B     00:00
Guest installation complete... restarting guest.
</code></pre><div
              class="calloutlist"><table
                border="0"
                summary="Callout list"><tr><td
                    width="5%"
                    valign="top"
                    align="right"><p><a
                        href="#virtinst.connect"><img
                          class="callout"
                          src="Common_Content/images/1.png"
                          alt="1" /></a> </p></td><td
                    valign="top"
                    align="right"><div
                      class="para">
							گزینه <code
                        class="literal">--connect</code> مشخص می‌کند از کدام “hypervisor” استفاده شود. فرم استفاده از آن شامل یک URL همراه با سیستم مجازی‌سازی مرتبط (<code
                        class="literal">xen://</code>، <code
                        class="literal">qemu://</code>، <code
                        class="literal">lxc://</code>، <code
                        class="literal">openvz://</code>، <code
                        class="literal">vbox://</code>) و ماشینی که باید از آن میزبانی کند می‌باشد (در صورت استفاده از localhost می‌تواند خالی باشد). علاوه بر این و در مورد QEMU/KVM، هر کاربر می‌تواند با استفاده از مجوزهای محدودشده ماشین‌های مجازی را مدیریت کند و مسیر URL امکان تفاوت قائل شدن بین ماشین‌های “سیستم” (<code
                        class="literal">/system</code>) را از دیگر (<code
                        class="literal">/session</code>) فراهم می‌کند.
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="right"><p><a
                        href="#virtinst.type"><img
                          class="callout"
                          src="Common_Content/images/2.png"
                          alt="2" /></a> </p></td><td
                    valign="top"
                    align="right"><div
                      class="para">
							از آنجا که KVM به شیوه مشابه QEMU مدیریت می‌شود، <code
                        class="literal">--virt-type kvm</code> امکان مشخص کردن استفاده از KVM با وجود تشابه با URL QEMU را فراهم می‌کند.
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="right"><p><a
                        href="#virtinst.name"><img
                          class="callout"
                          src="Common_Content/images/3.png"
                          alt="3" /></a> </p></td><td
                    valign="top"
                    align="right"><div
                      class="para">
							گزینه <code
                        class="literal">--name</code> یک نام (منحصربفرد) برای ماشین مجازی تعریف می‌کند.
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="right"><p><a
                        href="#virtinst.ram"><img
                          class="callout"
                          src="Common_Content/images/4.png"
                          alt="4" /></a> </p></td><td
                    valign="top"
                    align="right"><div
                      class="para">
							گزینه <code
                        class="literal">--ram</code> میزان RAM (به مگابایت) اختصاص یافته به ماشین مجازی را تعریف می‌کند.
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="right"><p><a
                        href="#virtinst.disk"><img
                          class="callout"
                          src="Common_Content/images/5.png"
                          alt="5" /></a> </p></td><td
                    valign="top"
                    align="right"><div
                      class="para">
							گزینه <code
                        class="literal">--disk</code> مکان فایل تصویری که قرار است هارد دیسک ماشین مجازی در آن قرار گیرد را تعریف می‌کند؛ این فایل با استفاده از پارامتر <code
                        class="literal">size</code> (به گیگابایت) در صورت موجود نبودن، ایجاد می‌گردد. پارامتر <code
                        class="literal">format</code> امکان ذخیره‌سازی فایل تصویر را در قالب‌های گوناگون بوجود می‌آورد. قالب پیشفرض (<code
                        class="literal">raw</code>) یک فایل تکی است که با محتوا و اندازه دیسک سازگاری داشته باشد. در اینجا از یک قالب پیشرفته‌تر استفاده کرده‌ایم، که مختص به QEMU می‌باشد و امکان شروع با یک فایل کوچک را می‌دهد که به مرور زمان و نیاز ماشین مجازی به فضای بیشتر، بزرگ‌تر می‌شود.
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="right"><p><a
                        href="#virtinst.cdrom"><img
                          class="callout"
                          src="Common_Content/images/6.png"
                          alt="6" /></a> </p></td><td
                    valign="top"
                    align="right"><div
                      class="para">
							گزینه <code
                        class="literal">--cdrom</code> به منظور یافتن دیسک نوری برای فرآیند نصب استفاده می‌شود. مسیر می‌تواند شامل یک مسیر محلی برای فایل ISO، یک URL که فایل می‌تواند از آنجا دریافت شود یا فایل دستگاه مربوط به یک درایو فیزیکی CD-ROM باشد (<code
                        class="literal">/dev/cdrom</code>).
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="right"><p><a
                        href="#virtinst.network"><img
                          class="callout"
                          src="Common_Content/images/7.png"
                          alt="7" /></a> </p></td><td
                    valign="top"
                    align="right"><div
                      class="para">
							گزینه <code
                        class="literal">--network</code> مشخص می‌کند کارت مجازی شبکه چطور با پیکربندی سیستم میزبان ادغام شود. عملکرد پیشفرض آن (که در این نمونه به صورت صریح بیان کرده‌ایم) ادغام آن با شبکه bridge از قبل موجود در سیستم است. اگر چنین bridge موجود نباشد، ماشین مجازی تنها با استفاده از NAT می‌تواند به شبکه فیزیکی دسترسی یابد، بنابراین یک نشانی در محدوده زیرشبکه 192.168.122.0/24 دریافت می‌کند.
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="right"><p><a
                        href="#virtinst.vnc"><img
                          class="callout"
                          src="Common_Content/images/8.png"
                          alt="8" /></a> </p></td><td
                    valign="top"
                    align="right"><div
                      class="para">
							گزینه <code
                        class="literal">--vnc</code> بیان می‌کند که کنسول گرافیکی باید توسط VNC قابل ارائه باشد. عملکرد پیشفرض سرور VNC این است که تنها به رابط local گوش دهد؛ اگر برنامه VNC در یک میزبان دیگر قرار داشته باشد، برقراری ارتباط نیازمند برپایی تونل SSH می‌باشد (<a
                        class="xref"
                        href="sect.remote-login.html#sect.ssh-port-forwarding">
      قسمت 9.2.1.3, “ایجاد تونل‌های رمزگذاری شده با پورت فورواردینگ”
    </a> را مشاهده کنید). به همین ترتیب، از <code
                        class="literal">--vnclisten=0.0.0.0</code> می‌توان برای دسترسی به سرور VNC از طریق تمام رابط‌های شبکه استفاده کرد؛ به یاد داشته باشید که در این صورت باید از یک طراحی firewall بهره‌مند شوید.
						</div></td></tr><tr><td
                    width="5%"
                    valign="top"
                    align="right"><p><a
                        href="#virtinst.os"><img
                          class="callout"
                          src="Common_Content/images/9.png"
                          alt="9" /></a> </p></td><td
                    valign="top"
                    align="right"><div
                      class="para">
							گزینه‌های <code
                        class="literal">--os-type</code> و <code
                        class="literal">--os-variant</code>، با توجه به برخی از ویژگ‌های سیستم عامل اشاره شده، امکان بهینه‌سازی چندین پارامتر ماشین مجازی را فراهم می‌کنند.
						</div></td></tr></table></div><div
              class="para">
					در این نقطه، ماشین مجازی در حال اجرا است و به منظور ادامه فرآیند نصب باید به کنسول گرافیکی متصل شویم. اگر عملیات قبل از طریق یک میزکار گرافیکی صورت گرفته باشد، این ارتباط به صورت مستقیم برقرار می‌شود. در غیر اینصورت، یا در حالتی که از راه دور اینکار را انجام می‌دهیم، <code
                class="command">virt-viewer</code> با استفاده از هر محیط گرافیکی برای باز کردن کنسول گرافیکی می‌تواند اجرا شود (به یاد داشته باشید که دو مرتبه گذرواژه root درخواست می‌شود چرا که ۲ ارتباط SSH مورد نیاز است):
				</div><pre
              class="screen"><code
                class="computeroutput">$ </code><strong
                class="userinput"><code>virt-viewer --connect qemu+ssh://root@<em
                    class="replaceable">server</em>/system testkvm
</code></strong><code
                class="computeroutput">root@server's password: 
root@server's password: </code></pre><div
              class="para">
					زمانی که فرآیند نصب به پایان برسد، ماشین مجازی راه‌اندازی مجدد می‌گردد تا قابل استفاده شود.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      id="id-1.15.5.14.9"></a>12.2.3.4. مدیریت ماشین‌های مجازی با <code
                      class="command">virsh</code></h4></div></div></div><a
              id="id-1.15.5.14.9.2"
              class="indexterm"></a><div
              class="para">
					اکنون که نصب به پایان رسیده است، بیایید چگونگی مدیریت ماشین‌های مجازی را بررسی کنیم. اولین کاری که باید بکنیم پرسش از <code
                class="command">libvirtd</code> برای فهرستی از ماشین‌های مجازی موجود است:
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>virsh -c qemu:///system list --all
 Id Name                 State
----------------------------------
  - testkvm              shut off
</code></strong></pre><div
              class="para">
					بیایید ماشین مجازی آزمایشی خود را آغاز کنیم:
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>virsh -c qemu:///system start testkvm
</code></strong><code
                class="computeroutput">Domain testkvm started</code></pre><div
              class="para">
					اکنون می‌توانیم دستورالعمل‌های ارتباط به کنسول گرافیکی را دریافت کنیم (نمایش VNC بازگشتی می‌تواند به عنوان پارامتر <code
                class="command">vncviewer</code> استفاده شود):
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>virsh -c qemu:///system vncdisplay testkvm
</code></strong><code
                class="computeroutput">:0</code></pre><div
              class="para">
					سایر دستورات <code
                class="command">virsh</code> عبارتند از:
				</div><div
              xmlns:d="http://docbook.org/ns/docbook"
              class="itemizedlist"><ul><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">reboot</code> برای راه‌اندازی مجدد یک ماشین مجازی؛
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">shutdown</code> برای درخواست یک shutdown تمیز؛
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">destroy</code> برای توقف خشن آن؛
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">suspend</code> برای توقف عادی آن؛
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">resume</code> برای ادامه فعالیت آن؛
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">autostart</code> برای فعال کردن (یا غیر فعال کردن با گزینه <code
                      class="literal">--disable</code>) راه‌اندازی ماشین مجازی به صورت خودکار در زمان راه‌اندازی میزبان؛
						</div></li><li
                  class="listitem"><div
                    class="para">
							<code
                      class="literal">undefine</code> برای حذف تمام نشانه‌های ماشین مجازی از <code
                      class="command">libvirtd</code>.
						</div></li></ul></div><div
              class="para">
					تمام این دستورات شناسه ماشین مجازی را به عنوان یک پارامتر دریافت می‌کنند.
				</div></div><div
            class="section"><div
              class="titlepage"><div><div><h4
                    class="title"><a
                      id="id-1.15.5.14.10"></a>12.2.3.5. نصب یک سیستم مبتنی بر RPM در دبیان با استفاده از yum</h4></div></div></div><div
              class="para">
					اگر قرار باشد ماشین مجازی به منظور اجرای دبیان (یا یکی از توزیع‌های آن) راه‌اندازی گردد، سیستم می‌تواند با استفاده از <code
                class="command">debootstrap</code> همانطور که توضیح داده شد راه‌اندازی شود. اما اگر قرار باشد ماشین مجازی به منظور اجرای یک سیستم مبتنی بر RPM (مانند Fedora، CentOS یا Scientific Linux) راه‌اندازی گردد، اینکار باید با استفاده از ابزار <code
                class="command">yum</code> صورت گیرد (که در بسته‌ای با همین نام قرار دارد).
				</div><div
              class="para">
					این فرآیند شامل استفاده از <code
                class="command">rpm</code> به منظور استخراج مجموعه‌ای از فایل‌ها، شامل فایل‌های پیکربندی <code
                class="command">yum</code>، سپس فراخوانی <code
                class="command">yum</code> برای استخراج سایر بسته‌های باقیمانده می‌باشد. اما از آنجا که فراخوانی <code
                class="command">yum</code> خارج از chroot صورت می‌گیرد، باید برخی تغییرات موقتی را ایجاد کنیم. در نمونه زیر، chroot هدف عبارت است از <code
                class="filename">/srv/centos</code>.
				</div><pre
              class="screen"><code
                class="computeroutput"># </code><strong
                class="userinput"><code>rootdir="/srv/centos"
</code></strong><code
                class="computeroutput"># </code><strong
                class="userinput"><code>mkdir -p "$rootdir" /etc/rpm
</code></strong><code
                class="computeroutput"># </code><strong
                class="userinput"><code>echo "%_dbpath /var/lib/rpm" &gt; /etc/rpm/macros.dbpath
</code></strong><code
                class="computeroutput"># </code><strong
                class="userinput"><code>wget http://mirror.centos.org/centos/7/os/x86_64/Packages/centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm
</code></strong><code
                class="computeroutput"># </code><strong
                class="userinput"><code>rpm --nodeps --root "$rootdir" -i centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm
</code></strong><code
                class="computeroutput">rpm: RPM should not be used directly install RPM packages, use Alien instead!
rpm: However assuming you know what you are doing...
warning: centos-release-7-1.1503.el7.centos.2.8.x86_64.rpm: Header V3 RSA/SHA256 Signature, key ID f4a80eb5: NOKEY
# </code><strong
                class="userinput"><code>sed -i -e "s,gpgkey=file:///etc/,gpgkey=file://${rootdir}/etc/,g" $rootdir/etc/yum.repos.d/*.repo
</code></strong><code
                class="computeroutput"># </code><strong
                class="userinput"><code>yum --assumeyes --installroot $rootdir groupinstall core
</code></strong><code
                class="computeroutput">[...]
# </code><strong
                class="userinput"><code>sed -i -e "s,gpgkey=file://${rootdir}/etc/,gpgkey=file:///etc/,g" $rootdir/etc/yum.repos.d/*.repo
</code></strong></pre></div></div></div><ul
        class="docnav"><li
          class="previous"><a
            accesskey="p"
            href="advanced-administration.html"><strong>قبلی</strong>
      فصل 12. مدیریت پیشرفته</a></li><li
          class="up"><a
            accesskey="u"
            href="#"><strong>بالا</strong></a></li><li
          class="home"><a
            accesskey="h"
            href="index.html"><strong>خانه</strong></a></li><li
          class="next"><a
            accesskey="n"
            href="sect.automated-installation.html"><strong>بعدی</strong>12.3. نصب خودکار</a></li></ul><div
        id="translated_pages"><ul><li><a
              href="../ar-MA/sect.virtualization.html">ar-MA</a></li><li><a
              href="../da-DK/sect.virtualization.html">da-DK</a></li><li><a
              href="../de-DE/sect.virtualization.html">de-DE</a></li><li><a
              href="../el-GR/sect.virtualization.html">el-GR</a></li><li><a
              href="../en-US/sect.virtualization.html">en-US</a></li><li><a
              href="../es-ES/sect.virtualization.html">es-ES</a></li><li><a
              href="../fa-IR/sect.virtualization.html">fa-IR</a></li><li><a
              href="../fr-FR/sect.virtualization.html">fr-FR</a></li><li><a
              href="../hr-HR/sect.virtualization.html">hr-HR</a></li><li><a
              href="../id-ID/sect.virtualization.html">id-ID</a></li><li><a
              href="../it-IT/sect.virtualization.html">it-IT</a></li><li><a
              href="../ja-JP/sect.virtualization.html">ja-JP</a></li><li><a
              href="../ko-KR/sect.virtualization.html">ko-KR</a></li><li><a
              href="../nb-NO/sect.virtualization.html">nb-NO</a></li><li><a
              href="../pl-PL/sect.virtualization.html">pl-PL</a></li><li><a
              href="../pt-BR/sect.virtualization.html">pt-BR</a></li><li><a
              href="../ro-RO/sect.virtualization.html">ro-RO</a></li><li><a
              href="../ru-RU/sect.virtualization.html">ru-RU</a></li><li><a
              href="../tr-TR/sect.virtualization.html">tr-TR</a></li><li><a
              href="../vi-VN/sect.virtualization.html">vi-VN</a></li><li><a
              href="../zh-CN/sect.virtualization.html">zh-CN</a></li><li><a
              href="../zh-TW/sect.virtualization.html">zh-TW</a></li></ul></div></body></html>
